{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dd1270-2131-435d-ab40-bd65c17b278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.special as sp\n",
    "import os as os\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from time import time as ti\n",
    "from skimage.restoration import denoise_wavelet\n",
    "import pickle\n",
    "#import CoreFunctions as cf\n",
    "import sys\n",
    "import random\n",
    "import psutil\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484d05e0-1007-4ac7-9b37-86cf182686ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFolder = '/sciclone/scr10/dchendrickson01/Recordings2/'\n",
    "DataFolder = '/scratch/Recordings2/'\n",
    "\n",
    "DateString = 'Good'\n",
    "LastGoodModel = 40\n",
    "\n",
    "TIME_STEPS = 1200\n",
    "Skips = 600\n",
    "RollSize = 50\n",
    "\n",
    "LastSuccesfull = 40\n",
    "\n",
    "tic = ti()\n",
    "start = tic\n",
    "\n",
    "MemoryProtection = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad91f14-71e3-426b-a755-5348f4c98159",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e414fea-661c-40ed-b695-c3ed174b8d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "FilesPerRun = 5\n",
    "ConcurrentFiles = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ac60be-1f53-4a38-8bf8-600b7d52a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RollingStdDevFaster(RawData, SmoothData, RollSize = 25):\n",
    "\n",
    "    Diffs = RawData - SmoothData\n",
    "    del RawData, SmoothData\n",
    "    \n",
    "    Sqs = Diffs * Diffs\n",
    "    del Diffs\n",
    "    \n",
    "    Sqs = Sqs.tolist() \n",
    "    Sqs.extend(np.zeros(RollSize))\n",
    "    mSqs = np.matrix(Sqs)\n",
    "    \n",
    "    for i in range(RollSize):\n",
    "        Sqs.insert(0, Sqs.pop())\n",
    "        mSqs = np.concatenate((np.matrix(Sqs),mSqs))\n",
    "    \n",
    "    sVect = mSqs.sum(axis=0)\n",
    "    eVect = (mSqs!=0).sum(axis=0)\n",
    "    del mSqs, Sqs\n",
    "    \n",
    "    VarVect = sVect / eVect\n",
    "    StdDevs = np.sqrt(VarVect)\n",
    "    return np.asarray(StdDevs[:-RollSize].T)\n",
    "\n",
    "def SquelchPattern(DataSet, StallRange = 5000, SquelchLevel = 0.02, verbose = False):\n",
    "    \n",
    "    SquelchSignal = np.ones(len(DataSet))\n",
    "    if verbose:\n",
    "        print(len(SquelchSignal))\n",
    "        \n",
    "    for i in range(len(DataSet)-2*StallRange):\n",
    "        if np.average(DataSet[i:i+StallRange]) < SquelchLevel:\n",
    "            SquelchSignal[i+StallRange]=0\n",
    "\n",
    "    return SquelchSignal\n",
    "\n",
    "def split_MorS(original_list, ones_list,MorS,SegIndexes = False):\n",
    "    # modified split_list_by_ones function to instead split by the zeros.\n",
    "    #\n",
    "    #\n",
    "    # Created with Bing AI support\n",
    "    #  1st request: \"python split list into chunks based on value\"\n",
    "    #  2nd request: \"I want to split the list based on the values in a second list.  Second list is all 1s and 0s.  I want all 0s removed, and each set of consequtive ones as its own item\"\n",
    "    #  3rd request: \"That is close.  Here is an example of the two lists, and what I would want returned: original_list = [1, 2, 3, 8, 7, 4, 5, 6, 4, 7, 8, 9]\n",
    "    #                ones_list =     [1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1]\n",
    "    #                return: [[1, 2, 3, 8], [4, 5, 6], [8,9]]\"\n",
    "    #\n",
    "    #This is the function that was created and seems to work on the short lists, going to use for long lists\n",
    "    \n",
    "    result_sublists = []\n",
    "    SegsEnds = []\n",
    "    sublist = []\n",
    "    ii = 0\n",
    "    \n",
    "    if MorS == 0:\n",
    "        for val, is_one in zip(original_list, ones_list):\n",
    "            if not is_one:\n",
    "                sublist.append(val)\n",
    "            elif sublist:\n",
    "                result_sublists.append(sublist)\n",
    "                SegsEnds.append(ii)\n",
    "                sublist = []\n",
    "            ii+=1\n",
    "    elif MorS == 1:\n",
    "        for val, is_one in zip(original_list, ones_list):\n",
    "            if is_one:\n",
    "                sublist.append(val)\n",
    "            elif sublist:\n",
    "                result_sublists.append(sublist)\n",
    "                SegsEnds.append(ii)\n",
    "                sublist = []\n",
    "            ii+=1\n",
    "    \n",
    "    # Add the last sublist (if any)\n",
    "    if sublist:\n",
    "        result_sublists.append(sublist)\n",
    "        SegsEnds.append(ii)\n",
    "\n",
    "    if SegIndexes:\n",
    "        return result_sublists, SegsEnds\n",
    "    else:\n",
    "        return result_sublists\n",
    "\n",
    "def split_list_by_ones(original_list, ones_list):\n",
    "    # modified split_list_by_ones function to instead split by the zeros.\n",
    "    #\n",
    "    #\n",
    "    # Created with Bing AI support\n",
    "    #  1st request: \"python split list into chunks based on value\"\n",
    "    #  2nd request: \"I want to split the list based on the values in a second list.  Second list is all 1s and 0s.  I want all 0s removed, and each set of consequtive ones as its own item\"\n",
    "    #  3rd request: \"That is close.  Here is an example of the two lists, and what I would want returned: original_list = [1, 2, 3, 8, 7, 4, 5, 6, 4, 7, 8, 9]\n",
    "    #                ones_list =     [1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1]\n",
    "    #                return: [[1, 2, 3, 8], [4, 5, 6], [8,9]]\"\n",
    "    #\n",
    "    #This is the function that was created and seems to work on the short lists, going to use for long lists\n",
    "    \n",
    "    result_sublists = []\n",
    "    sublist = []\n",
    "\n",
    "    for val, is_one in zip(original_list, ones_list):\n",
    "        if is_one:\n",
    "            sublist.append(val)\n",
    "        elif sublist:\n",
    "            result_sublists.append(sublist)\n",
    "            sublist = []\n",
    "\n",
    "    # Add the last sublist (if any)\n",
    "    if sublist:\n",
    "        result_sublists.append(sublist)\n",
    "\n",
    "    return result_sublists\n",
    "\n",
    "# Generated training sequences for use in the model.\n",
    "def create_sequences(values, time_steps=TIME_STEPS, skips = Skips, IndexEnd = 0):\n",
    "    output = []\n",
    "    ends = []\n",
    "    for i in range(int((len(values) - time_steps + skips)/skips)):\n",
    "        output.append(values[i*skips : (i*skips + time_steps)])\n",
    "        ends.append(IndexEnd - len(values) + i*skips + time_steps)\n",
    "    if IndexEnd ==0:\n",
    "        return np.stack(output)\n",
    "    else:\n",
    "        return np.stack(output), ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8657fe64-9de2-4a19-ab39-cea97a5524a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVelocity(Acceleration, Timestamps = 0.003, Squelch = [], corrected = 0):\n",
    "    velocity = np.zeros(len(Acceleration))\n",
    "    \n",
    "    Acceleration -= np.average(Acceleration)\n",
    "    \n",
    "    if len(Timestamps) == 1:\n",
    "        dTime = np.ones(len(Acceleration),dtype=float) * Timestamps\n",
    "    elif len(Timestamps) == len(Acceleration):\n",
    "        dTime = np.zeros(len(Timestamps), dtype=float)\n",
    "        dTime[0]=1\n",
    "        for i in range(len(Timestamps)-1):\n",
    "            j = i+1\n",
    "            if Timestamps[j] > Timestamps[i]:\n",
    "                dTime[j]=Timestamps[j]-Timestamps[i]\n",
    "            else:\n",
    "                dTime[j]=Timestamps[j]-Timestamps[i]+10000.0\n",
    "        dTime /= 10000.0\n",
    "\n",
    "    velocity[0] = Acceleration[0] * (dTime[0])\n",
    "\n",
    "    for i in range(len(Acceleration)-1):\n",
    "        j = i + 1\n",
    "        if corrected ==2:\n",
    "            if Squelch[j]==0:\n",
    "                velocity[j]=0\n",
    "            else:\n",
    "                velocity[j] = velocity[i] + Acceleration[j] * dTime[j]                \n",
    "        else:\n",
    "            velocity[j] = velocity[i] + Acceleration[j] * dTime[j]\n",
    "\n",
    "    if corrected == 1:\n",
    "        PointVairance = velocity[-1:] / len(velocity)\n",
    "        for i in range(len(velocity)):\n",
    "            velocity[i] -=  PointVairance * i\n",
    "    \n",
    "    velocity *= 9.81\n",
    "\n",
    "    return velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bca307-8ab5-4410-a6a4-4e7e6d3087f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def runFile(file, verbose = False, small = False, index=0, start=ti()):\n",
    "    noise = verbose\n",
    "    dataset = pd.read_csv(DataFolder+file, delimiter =\",\", header=None, engine='python',on_bad_lines='skip')\n",
    "    \n",
    "    \n",
    "    dataset = dataset.rename(columns={0:\"Day\"})\n",
    "    dataset = dataset.rename(columns={1:\"Second\"})\n",
    "    dataset = dataset.rename(columns={2:\"FracSec\"})\n",
    "    dataset = dataset.rename(columns={3:\"p\"})\n",
    "    dataset = dataset.rename(columns={4:\"h\"})\n",
    "    dataset = dataset.rename(columns={5:\"v\"})\n",
    "    dataset = dataset.rename(columns={6:\"Sensor\"})\n",
    "    \n",
    "    #dataset['Second'].replace('',0)\n",
    "    #dataset['FracSec'].replace('',0)\n",
    "    #dataset.replace([np.nan, np.inf, -np.inf],0,inplace=True)\n",
    "    \n",
    "    #dataset[['Day','Second']] = dataset[['Day','Second']].apply(lambda x: x.astype(int).astype(str).str.zfill(6))\n",
    "    #dataset[['FracSec']] = dataset[['FracSec']].apply(lambda x: x.astype(int).astype(str).str.zfill(4))\n",
    "    \n",
    "    #dataset[\"timestamp\"] = pd.to_datetime(dataset.Day+dataset.Second+dataset.FracSec,format='%y%m%d%H%M%S%f')\n",
    "    #dataset[\"timestamps\"] = dataset[\"timestamp\"]\n",
    "    \n",
    "    dataset[\"p\"] = dataset.p - np.average(dataset.p)\n",
    "    dataset[\"h\"] = dataset.h - np.average(dataset.h)\n",
    "    dataset[\"v\"] = dataset.v - np.average(dataset.v)\n",
    "    #dataset[\"r\"] = np.sqrt(dataset.p**2 + dataset.h**2 + dataset.v**2)\n",
    "    \n",
    "    #dataset.index = dataset.timestamp\n",
    "    \n",
    "    dataset[\"SmoothP\"] = denoise_wavelet(dataset.p, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "    dataset[\"SmoothH\"] = denoise_wavelet(dataset.h, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "    dataset[\"SmoothV\"] = denoise_wavelet(dataset.v, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "    #dataset[\"SmoothR\"] = denoise_wavelet(dataset.r, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "    \n",
    "    \n",
    "    RawData = dataset.v\n",
    "    SmoothData = dataset.SmoothV\n",
    "    RollSize = 25\n",
    "    \n",
    "    Diffs = RawData - SmoothData\n",
    "    \n",
    "    Sqs = Diffs * Diffs\n",
    "    \n",
    "    Sqs = Sqs.tolist() \n",
    "    \n",
    "    Sqs.extend(np.zeros(RollSize))\n",
    "    \n",
    "    mSqs = np.matrix(Sqs)\n",
    "    \n",
    "    for i in range(RollSize):\n",
    "        Sqs.insert(0, Sqs.pop())\n",
    "        mSqs = np.concatenate((np.matrix(Sqs),mSqs))\n",
    "    \n",
    "    sVect = mSqs.sum(axis=0)\n",
    "    eVect = (mSqs!=0).sum(axis=0)\n",
    "    \n",
    "    VarVect = sVect / eVect\n",
    "    \n",
    "    StdDevs = np.sqrt(VarVect)\n",
    "    \n",
    "    StdDevsZ = np.asarray(StdDevs)\n",
    "    \n",
    "    StdDevsZ=np.append(StdDevsZ,[0])\n",
    "    \n",
    "    StdDevsZ = np.asarray(StdDevsZ.T[:len(dataset.p)])\n",
    "    \n",
    "    print(\"Size StdDevsZ\", ti()-start, np.shape(StdDevsZ))\n",
    "    \n",
    "    StdDevsZ = np.nan_to_num(StdDevsZ)\n",
    "    \n",
    "    StdDevsZ[StdDevsZ == np.inf] = 0\n",
    "    StdDevsZ[StdDevsZ == -np.inf] = 0\n",
    "    \n",
    "    SmoothDevZ = denoise_wavelet(StdDevsZ, method='VisuShrink', mode='soft', wavelet='sym2', rescale_sigma='True')\n",
    "    \n",
    "    print(\"denoise 1\", ti()-start, np.shape(StdDevsZ))\n",
    "    \n",
    "    SmoothDevZ[np.isnan(SmoothDevZ)]=0\n",
    "    \n",
    "    Max = np.max(SmoothDevZ)\n",
    "    \n",
    "    buckets = int(Max / 0.005) + 1\n",
    "    bins = np.linspace(0,buckets*0.005,buckets+1)\n",
    "    counts, bins = np.histogram(SmoothDevZ,bins=bins)\n",
    "    \n",
    "    CummCount = 0\n",
    "    HalfWay = 0\n",
    "    for i in range(len(counts)):\n",
    "        CummCount += counts[i]\n",
    "        if CummCount / len(SmoothDevZ) >= 0.5:\n",
    "            if HalfWay == 0:\n",
    "                HalfWay = i\n",
    "    \n",
    "    SquelchLevel = bins[HalfWay] \n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(SmoothDevZ)\n",
    "    plt.show()    \n",
    "\n",
    "    if SquelchLevel > .015:\n",
    "        SL = 0.04\n",
    "    else:\n",
    "        SL = 0.02\n",
    "    \n",
    "    \n",
    "    dataset[\"IsMoving\"] = SquelchPattern(SmoothDevZ, 2000, SL, False)\n",
    "    \n",
    "    \n",
    "    velocity = getVelocity(dataset.p, dataset.FracSec, dataset.IsMoving, 2)\n",
    "    \n",
    "    df_pr, SegEndr = split_MorS(dataset.p, dataset.IsMoving,1,SegIndexes = True)\n",
    "    df_hr = split_MorS(dataset.h, dataset.IsMoving,1)\n",
    "    df_vr = split_MorS(dataset.v, dataset.IsMoving,1)\n",
    "    \n",
    "    df_ps, SegEnds = split_MorS(dataset.SmoothP, dataset.IsMoving,0,SegIndexes = True)\n",
    "    df_hs = split_MorS(dataset.SmoothH, dataset.IsMoving,0)\n",
    "    df_vs = split_MorS(dataset.SmoothV, dataset.IsMoving,0)\n",
    "    \n",
    "    SmoothMove = []\n",
    "    for i in range(len(df_pr)):\n",
    "        if len(df_pr[i])>TIME_STEPS:\n",
    "            tempP,tempE=create_sequences(df_pr[i],IndexEnd=SegEndr[i])\n",
    "            tempH=create_sequences(df_hr[i])\n",
    "            tempV=create_sequences(df_vr[i])\n",
    "            for j in range(len(tempP)):\n",
    "                SmoothMove.append([np.vstack((tempP[j],tempH[j],tempV[j])),tempE[j],file]) #,df_rs[i])))\n",
    "    \n",
    "    SmoothStop = []\n",
    "    for i in range(len(df_ps)):\n",
    "        if len(df_ps[i])>TIME_STEPS:\n",
    "            tempP,tempE=create_sequences(df_ps[i],IndexEnd=SegEnds[i])\n",
    "            tempH=create_sequences(df_hs[i])\n",
    "            tempV=create_sequences(df_vs[i])\n",
    "            for j in range(len(tempP)):\n",
    "                SmoothStop.append([np.vstack((tempP[j],tempH[j],tempV[j])),tempE[j],file]) #,df_rs[i])))\n",
    "    \n",
    "    return SmoothMove, SmoothStop, velocity, SmoothDevZ\n",
    "    #else:\n",
    "    #    return ['fail','fail']\n",
    "        \n",
    "        #if verbose:\n",
    "        #    print('Data normalized', ti()-start)\n",
    "\n",
    "        #return df_p, df_h, df_v, df_r, df_rp, df_rh, df_rv, df_rr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933adb44-58cf-4121-a00a-ad13a23ca315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import CoreFunctions as cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902f00b1-e96a-4c7e-b844-d4bd52adc131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runWrapper(file_path, verbose=True, small=False, index=0, start=ti()):\n",
    "    #try:\n",
    "    rtrn = runFile(file_path, verbose, small, index, start)\n",
    "    return rtrn\n",
    "    #except Exception as e:\n",
    "    #    with open('BadInputs.text', 'a') as bad_file:\n",
    "    #        bad_file.write(file_path + '\\n')\n",
    "    #    return np.zeros((10, 10, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dbaf6f-2f31-416a-8428-5bca469416db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintWrap(Mat):\n",
    "    localPrints = []\n",
    "    lenm = np.shape(Mat)[1]\n",
    "    slices = int(lenm/TIME_STEPS)\n",
    "    for i in range(slices):\n",
    "        temp = (cf.makeMPFast(Mat[:3,i*TIME_STEPS:(i+1)*TIME_STEPS], wvt = 'sym4', scales = 32, spacer = 2, title = ''))\n",
    "        localPrints.append(temp.astype(np.float32)/255.0)\n",
    "    return localPrints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b347773d-6e6d-4b90-a416-614484995b0e",
   "metadata": {},
   "source": [
    "# Start Machine Learning\n",
    "## Using Autoencoder with Kears and Tensorflow\n",
    "cite: https://pyimagesearch.com/2020/02/17/autoencoders-with-keras-tensorflow-and-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf9e779-ae36-4be4-9de3-436cddfdb599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73075a69-dac3-4e26-8f4f-4a26c13da05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder:\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, filters=(32, 64), latentDim=24):\n",
    "        inputShape = (height, width, depth)\n",
    "        chanDim = -1\n",
    "        inputs = Input(shape=inputShape)\n",
    "        x = inputs\n",
    "\n",
    "        for f in filters:\n",
    "            x = Conv2D(f, (3, 3), strides=2, padding=\"same\")(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            x = BatchNormalization(axis=chanDim)(x)\n",
    "\n",
    "        volumeSize = K.int_shape(x)\n",
    "        print(\"Volume Size:\", volumeSize)\n",
    "        x = Flatten()(x)\n",
    "        latent = Dense(latentDim)(x)\n",
    "\n",
    "        encoder = Model(inputs, latent, name=\"encoder\")\n",
    "\n",
    "        latentInputs = Input(shape=(latentDim,))\n",
    "        flattenedVolumeSize = int(np.prod(volumeSize[1:]))\n",
    "        print(\"Flattened Volume Size:\", flattenedVolumeSize)\n",
    "        x = Dense(flattenedVolumeSize)(latentInputs)\n",
    "        x = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(x)\n",
    "\n",
    "        for f in filters[::-1]:\n",
    "            x = Conv2DTranspose(f, (3, 3), strides=2, padding=\"same\")(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            x = BatchNormalization(axis=chanDim)(x)\n",
    "\n",
    "        x = Conv2DTranspose(depth, (3, 3), padding=\"same\")(x)\n",
    "        outputs = Activation(\"sigmoid\")(x)\n",
    "\n",
    "        decoder = Model(latentInputs, outputs, name=\"decoder\")\n",
    "        autoencoder = Model(inputs, decoder(encoder(inputs)), name=\"autoencoder\")\n",
    "\n",
    "        return (encoder, decoder, autoencoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a2ee2a-8f18-4995-b1a1-707a2a0017bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "#import matplotlib\n",
    "#matplotlib.use(\"Agg\")\n",
    "# import the necessary packages\n",
    "#from pyimagesearch.convautoencoder import ConvAutoencoder\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "# construct the argument parse and parse the arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a660d2-332a-42e5-8682-b872dedbb4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8299fc99-d18e-4b17-915b-41f113171b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4144ff94-e398-4448-873a-11920fda2e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c1be56-981b-4c06-9874-11421be7c25d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "directory = '/scratch/models/'\n",
    "target_file = f'3DFP_{DateString}_{str(LastGoodModel).zfill(3)}_good2_ae.keras'\n",
    "\n",
    "ImageShape=[5,32,600,3]\n",
    "\n",
    "encoder = load_model(directory+target_file[:-8]+'en.keras')\n",
    "decoder = load_model(directory+target_file[:-8]+'de.keras')\n",
    "\n",
    "autoencoder_input = Input(shape=(ImageShape[1], ImageShape[2], ImageShape[3]))\n",
    "\n",
    "# Pass the input through the encoder and decoder\n",
    "encoded_repr = encoder(autoencoder_input)\n",
    "reconstructed = decoder(encoded_repr)\n",
    "\n",
    "# Create the reassembled autoencoder model\n",
    "autoencoder = Model(autoencoder_input, reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8116afff-0622-40b5-943e-34f2dab5ca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder.summary())\n",
    "print(decoder.summary())\n",
    "print(autoencoder.summary())\n",
    "#if LastSuccesfull != 0:\n",
    "#    print(reautoencoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22c488b-85a7-4a14-a917-6fa18ad805c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTensors(file_list):\n",
    "\n",
    "    AllDatas = Parallel(n_jobs=ConcurrentFiles)(delayed(runFile)(file_list[i], False, False, 0, ti()) for i in range(min(len(file_list),FilesPerRun)))\n",
    "\n",
    "    Mats=[]\n",
    "    StopIndex=[]\n",
    "    Runs=[]\n",
    "    MoveIndex=[]\n",
    "    StopFiles=[]\n",
    "    MoveFiles=[]\n",
    "\n",
    "    for fileResponse in AllDatas:\n",
    "        for Mat in fileResponse[0]:\n",
    "            Mats.append(Mat[0])\n",
    "            StopIndex.append(Mat[1])\n",
    "            StopFiles.append(Mat[2])\n",
    "        for Mat in fileResponse[1]:\n",
    "            Runs.append(Mat[0])\n",
    "            MoveIndex.append(Mat[1])\n",
    "            MoveFiles.append(Mat[2])\n",
    "        fig = plt.figure()\n",
    "        plt.plot(fileResponse[3])\n",
    "        plt.show()\n",
    "        fig = plt.figure()\n",
    "        plt.plot(fileResponse[2])\n",
    "        plt.show()\n",
    "        \n",
    "            \n",
    "    if MemoryProtection:\n",
    "        del AllDatas\n",
    "        print('RAM after AllData:', psutil.virtual_memory()[2], len(Mats))        \n",
    "\n",
    "    AllPrints = Parallel(n_jobs=8)(delayed(PrintWrap)(Mat) for Mat in Mats)\n",
    "    AllRuns = Parallel(n_jobs=8)(delayed(PrintWrap)(Mat) for Mat in Runs)\n",
    "    \n",
    "    if MemoryProtection:\n",
    "        del Mats, Runs\n",
    "        print('RAM after Keeps:', psutil.virtual_memory()[2])\n",
    "    \n",
    "    Prints = []\n",
    "    Rprint = []    \n",
    "    for group in AllPrints:\n",
    "        for fprint in group:\n",
    "            Prints.append(fprint[:, ::2, :])\n",
    "    for group in AllRuns:\n",
    "        for fprint in group:\n",
    "            Rprint.append(fprint[:, ::2, :])\n",
    "    \n",
    "    if MemoryProtection:\n",
    "        del AllPrints, AllRuns\n",
    "    \n",
    "    for i, image in enumerate(Prints):\n",
    "        if not isinstance(image, np.ndarray):\n",
    "            Prints[i] = np.array(image, dtype=np.float32)\n",
    "        elif image.dtype != np.float32:\n",
    "            Prints[i] = image.astype(np.float32)\n",
    "    for i, image in enumerate(Rprint):\n",
    "        if not isinstance(image, np.ndarray):\n",
    "            Rprint[i] = np.array(image, dtype=np.float32)\n",
    "        elif image.dtype != np.float32:\n",
    "            Rprint[i] = image.astype(np.float32)\n",
    "    \n",
    "    # Stack the images into a single NumPy array\n",
    "    prints_array = np.stack(Prints, axis=0)\n",
    "    rprint_array = np.stack(Rprint, axis=0)\n",
    "    \n",
    "    if MemoryProtection:\n",
    "        del Prints, Rprint\n",
    "        print('RAM after Prints:', psutil.virtual_memory()[2])\n",
    "    # Convert the NumPy array to a TensorFlow tensor\n",
    "    trX = tf.convert_to_tensor(prints_array)\n",
    "    trR = tf.convert_to_tensor(rprint_array)\n",
    "    \n",
    "    if MemoryProtection:\n",
    "        del prints_array, rprint_array\n",
    "    return trX, trR, fileResponse[3], fileResponse[2], MoveIndex, StopIndex, MoveFiles, StopFiles, Runs, Mats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e142882-824a-4a50-a11b-11def38083e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "files=['230801 recording1.csv',\n",
    "       '230802 recording1.csv',\n",
    "       '230424 recording3.csv',\n",
    "       '230426 recording3.csv',\n",
    "       '230801 recording1.csv',\n",
    "       '230802 recording1.csv',\n",
    "       '230424 recording3.csv',\n",
    "       '230426 recording3.csv',\n",
    "       '230119 recording1.csv'\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1786dcf3-19c9-40da-9177-659a521729ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Moves, Stops, StdDev, Velocity, MoveLoc, StopLoc, MoveFiles, StopFiles, MoveData, StopData = getTensors(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305280b2-70ef-494a-ad95-2e3e32aee31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fab36a-3f08-4dc7-afc4-356d75749f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e515ce-41c0-472e-8d12-31b4ed71363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MovePred = autoencoder.predict(Moves)\n",
    "StopPred = autoencoder.predict(Stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25e862b-b2b9-40dc-bd32-5b4a2d74b1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Diff_Move = Moves-MovePred\n",
    "Diff_Stop = Stops-StopPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb89bb2b-4632-4c9b-bf31-462f2596da48",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_Move_Pixel = np.mean(Diff_Move**2,axis=3)\n",
    "MSE_Stop_Pixel = np.mean(Diff_Stop**2,axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56beb3e5-29fd-4560-b342-c36012a81a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_Move_Time = np.mean(MSE_Move_Pixel,axis=1)\n",
    "MSE_Stop_Time = np.mean(MSE_Stop_Pixel,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd14377d-87d8-4f9c-8d08-4418aa4a251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_Move_Frame = np.mean(MSE_Move_Time,axis=1)\n",
    "MSE_Stop_Frame = np.mean(MSE_Stop_Time,axis=1)\n",
    "print('RAM after Predictions and Deltas:', psutil.virtual_memory()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc33a928-4271-43fd-8ea7-6dda873b4ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Diff_Move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d446d4-ad52-4467-baa3-dd363c9f0205",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(MSE_Move_Pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1404c574-104e-4152-936e-d851031146ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(MSE_Move_Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6608db7-7661-4c11-b393-80c3c3881534",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(MSE_Move_Frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7db7b2c-2c91-4eeb-8901-4784295d5996",
   "metadata": {},
   "outputs": [],
   "source": [
    "MoveResult = np.mean(MSE_Move_Frame)\n",
    "StopResult = np.mean(MSE_Stop_Frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54b9b44-8c95-433b-a0b1-8b7dbb1f104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MoveResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4686274f-978d-4345-89a6-58d25fea5fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "StopResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc83790-9e75-4637-b345-a347705999f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RResultsS = []\n",
    "RResultsA = []\n",
    "RResultsM = []\n",
    "RResultsAM = []\n",
    "RResultsMS = []\n",
    "for result in Diff_Move:\n",
    "    RResultsS.append(np.sum(result))\n",
    "    RResultsA.append(np.sum(np.abs(result)))\n",
    "    RResultsM.append(np.mean(result))\n",
    "    RResultsAM.append(np.mean(np.abs(result)))\n",
    "    RResultsMS.append(np.mean(result**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34aad87-787b-40cd-9f94-c638184af22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SResultsS = []\n",
    "SResultsA = []\n",
    "SResultsM = []\n",
    "SResultsAM = []\n",
    "SResultsMS = []\n",
    "for result in Diff_Stop:\n",
    "    SResultsS.append(np.sum(result))\n",
    "    SResultsA.append(np.sum(np.abs(result)))\n",
    "    SResultsM.append(np.mean(result))\n",
    "    SResultsAM.append(np.mean(np.abs(result)))\n",
    "    SResultsMS.append(np.mean(result**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a0fe48-1be4-4c90-9cca-c3c64e2efac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(data, min = -np.inf, max = np.inf):\n",
    "    data = np.array(data)\n",
    "    data[np.isnan(data)] = 0\n",
    "    data[np.isinf(data)] = 0\n",
    "    mean = np.mean(data) \n",
    "    std_dev = np.std(data) \n",
    "    filtered_data = data[np.abs(data - mean) / std_dev < 2]\n",
    "    filtered_data = filtered_data[filtered_data > min]\n",
    "    filtered_data = filtered_data[filtered_data < max]\n",
    "\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a815099-a6d2-43b0-93a4-b80eb3be9543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(filter_data(RResultsS), bins=20)\n",
    "plt.xlabel(\"Sum Difs\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()\n",
    "plt.hist((RResultsS), bins=20)\n",
    "plt.xlabel(\"Sum Difs\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c288f110-304c-4d9d-8af8-f5524f5094b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(filter_data(RResultsA,0,50000), bins=20)\n",
    "plt.xlabel(\"Sum Abs Value Diffs\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()\n",
    "plt.hist(RResultsA, bins=20)\n",
    "plt.xlabel(\"Sum Abs Value Diffs\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eeb012-8ccf-4831-a2d0-54ffd7b90339",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(filter_data(RResultsAM,0,2), bins=20)\n",
    "plt.xlabel(\"Mean Abs Val Difs\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()\n",
    "plt.hist(RResultsAM, bins=20)\n",
    "plt.xlabel(\"Sum Difs\")\n",
    "plt.ylabel(\"Mean Abs Val Difs\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd793681-5709-4e7e-b567-d84f0d5e052e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist((RResultsS), bins=20)\n",
    "plt.xlabel(\"Sum Difs\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03204bf-5804-47f7-a1d3-9470b80af80f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(1,2,figsize=(8,3), dpi=300 )\n",
    "ax0.hist((RResultsS), bins=20)\n",
    "ax1.hist((SResultsS), bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ad344f-f384-43ae-a80e-001c9c8fb8f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(1,2,figsize=(8,3), dpi=300 )\n",
    "ax0.hist((RResultsA), bins=20)\n",
    "ax1.hist((SResultsA), bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e12416-1c5c-4f2f-bb0f-5ab448a89e9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(1,2,figsize=(8,3), dpi=300 )\n",
    "ax0.hist((RResultsMS), bins=20)\n",
    "ax1.hist((SResultsMS), bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb70da8-081c-49db-8fe1-8a76f5fa4d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "Diff_Move = Moves-MovePred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5215c43d-d11f-446f-b800-49d36f5926e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    fig, (ax0, ax1, ax2) = plt.subplots(1, 3,figsize=(10,3), dpi=300 )\n",
    "    ax0.imshow(Moves[i], origin='lower',aspect='auto')\n",
    "    ax0.axis(\"off\")\n",
    "    ax1.imshow(MovePred[i], origin='lower',aspect='auto')\n",
    "    ax1.axis(\"off\")\n",
    "    ax2.imshow(np.abs(Diff_Move[i]), origin='lower',aspect='auto')\n",
    "    ax2.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ff4338-39df-4a47-b57c-9acebfe1fc5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    fig, (ax0, ax1, ax2) = plt.subplots(1, 3,figsize=(10,3), dpi=300 )\n",
    "    ax0.imshow(Stops[i], origin='lower',aspect='auto')\n",
    "    ax0.axis(\"off\")\n",
    "    ax1.imshow(StopPred[i], origin='lower',aspect='auto')\n",
    "    ax1.axis(\"off\")\n",
    "    ax2.imshow(np.abs(Diff_Stop[i]), origin='lower',aspect='auto')\n",
    "    ax2.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704ff9f0-ba82-48a8-87b1-fb0041e927c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BigMoveError = np.argmax(RResultsMS)\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(1, 3,figsize=(10,3), dpi=300 )\n",
    "ax0.imshow(Moves[BigMoveError], origin='lower',aspect='auto')\n",
    "ax0.axis(\"off\")\n",
    "ax1.imshow(MovePred[BigMoveError], origin='lower',aspect='auto')\n",
    "ax1.axis(\"off\")\n",
    "ax2.imshow(np.abs(Diff_Move[BigMoveError]), origin='lower',aspect='auto')\n",
    "ax2.axis(\"off\")\n",
    "plt.show()\n",
    "BigStopError = np.argmax(SResultsMS)\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(1, 3,figsize=(10,3), dpi=300 )\n",
    "ax0.imshow(Stops[BigStopError], origin='lower',aspect='auto')\n",
    "ax0.axis(\"off\")\n",
    "ax1.imshow(StopPred[BigStopError], origin='lower',aspect='auto')\n",
    "ax1.axis(\"off\")\n",
    "ax2.imshow(np.abs(Diff_Stop[BigStopError]), origin='lower',aspect='auto')\n",
    "ax2.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ce760b-a91f-4aab-b0ac-5585f5eae97e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BigMoveError = np.argmin(RResultsMS)\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(1, 3,figsize=(10,3), dpi=300 )\n",
    "ax0.imshow(Moves[BigMoveError], origin='lower',aspect='auto')\n",
    "ax0.axis(\"off\")\n",
    "ax1.imshow(MovePred[BigMoveError], origin='lower',aspect='auto')\n",
    "ax1.axis(\"off\")\n",
    "ax2.imshow(np.abs(Diff_Move[BigMoveError]), origin='lower',aspect='auto')\n",
    "ax2.axis(\"off\")\n",
    "plt.show()\n",
    "BigStopError = np.argmin(SResultsMS)\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(1, 3,figsize=(10,3), dpi=300 )\n",
    "ax0.imshow(Stops[BigStopError], origin='lower',aspect='auto')\n",
    "ax0.axis(\"off\")\n",
    "ax1.imshow(StopPred[BigStopError], origin='lower',aspect='auto')\n",
    "ax1.axis(\"off\")\n",
    "ax2.imshow(np.abs(Diff_Stop[BigStopError]), origin='lower',aspect='auto')\n",
    "ax2.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04150138-3715-43f6-8f7c-be2b85f8c78f",
   "metadata": {},
   "source": [
    "MoveLoc, StopLoc, MoveFiles, StopFiles, MoveData, StopData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8fdb39-e49a-4c62-97da-35162704ce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MoveLoc[np.argmin(RResultsMS)],MoveFiles[np.argmin(RResultsMS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a952e4-955f-4e02-990a-c2b837262bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(StopData[np.argmin(RResultsMS)][0,:])\n",
    "plt.plot(StopData[np.argmin(RResultsMS)][1,:])\n",
    "plt.plot(StopData[np.argmin(RResultsMS)][2,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8580644c-117e-4169-9c60-27185f5673c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(MoveData[np.argmin(SResultsMS)][0,:])\n",
    "plt.plot(MoveData[np.argmin(SResultsMS)][1,:])\n",
    "plt.plot(MoveData[np.argmin(SResultsMS)][2,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df0d049-3f75-47b2-8bb2-8131d04a8651",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(MoveData[np.argmax(SResultsMS)][0,:])\n",
    "plt.plot(MoveData[np.argmax(SResultsMS)][1,:])\n",
    "plt.plot(MoveData[np.argmax(SResultsMS)][2,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67cbbf8-2f86-4437-b20d-4cd928fef837",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(StopData[np.argmax(RResultsMS)][0,:])\n",
    "plt.plot(StopData[np.argmax(RResultsMS)][1,:])\n",
    "plt.plot(StopData[np.argmax(RResultsMS)][2,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ac9a7a-9814-4c13-9797-65fe39d3d77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "StopData = np.array(StopData)\n",
    "StopSortIndexes=np.argsort(RResultsMS)\n",
    "StopSorted=StopData[StopSortIndexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d449419-515b-4fbd-a7f2-793243ed52da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    fig = plt.figure(figsize=(8,3),dpi=300)\n",
    "    plt.plot(StopSorted[i][0,:],linewidth=0.5)\n",
    "    plt.plot(StopSorted[i][1,:],linewidth=0.5)\n",
    "    plt.plot(StopSorted[i][2,:],linewidth=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c29afa2-2423-40fa-8cc4-ecbfe244b52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = len(StopSorted)-1\n",
    "for i in range(15):\n",
    "    fig = plt.figure(figsize=(8,3),dpi=300)\n",
    "    plt.plot(StopSorted[j-i][0,:],linewidth=0.5)\n",
    "    plt.plot(StopSorted[j-i][1,:],linewidth=0.5)\n",
    "    plt.plot(StopSorted[j-i][2,:],linewidth=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ad002f-e519-4716-9bbc-251b4904758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MoveData = np.array(MoveData)\n",
    "MoveSortIndexes=np.argsort(SResultsMS)\n",
    "MoveSorted=MoveData[MoveSortIndexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58476b86-8b55-49d8-ba3f-33d3258637e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    fig = plt.figure(figsize=(8,3),dpi=300)\n",
    "    plt.plot(MoveSorted[i][0,:],linewidth=0.5)\n",
    "    plt.plot(MoveSorted[i][1,:],linewidth=0.5)\n",
    "    plt.plot(MoveSorted[i][2,:],linewidth=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f8059b-21ea-481c-841d-94d096e72f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = len(MoveSorted)-1\n",
    "for i in range(15):\n",
    "    fig = plt.figure(figsize=(8,3),dpi=300)\n",
    "    plt.plot(MoveSorted[j-i][0,:],linewidth=0.5)\n",
    "    plt.plot(MoveSorted[j-i][1,:],linewidth=0.5)\n",
    "    plt.plot(MoveSorted[j-i][2,:],linewidth=0.5)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
