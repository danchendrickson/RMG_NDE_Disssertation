{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24dd1270-2131-435d-ab40-bd65c17b278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.special as sp\n",
    "import os as os\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from time import time as ti\n",
    "from skimage.restoration import denoise_wavelet\n",
    "import pickle\n",
    "#import CoreFunctions as cf\n",
    "import sys\n",
    "import random\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "484d05e0-1007-4ac7-9b37-86cf182686ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFolder = '/sciclone/scr10/dchendrickson01/Recordings2/'\n",
    "DataFolder = '/scratch/Recordings2/'\n",
    "model_directory = '/scratch/models/moving/'\n",
    "\n",
    "DateString = '1009'\n",
    "\n",
    "TIME_STEPS = 1200\n",
    "Skips = 125\n",
    "RollSize = 50\n",
    "\n",
    "LastSuccesfull = 0\n",
    "\n",
    "tic = ti()\n",
    "start = tic\n",
    "\n",
    "MemoryProtection = True\n",
    "\n",
    "LR_Starting = 3e-2\n",
    "LR_PeriodGrow = 6\n",
    "LR_Decay = .75\n",
    "LR_Expand = 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fad91f14-71e3-426b-a755-5348f4c98159",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "869b723b-501a-461d-a00c-5b12306d9814",
   "metadata": {},
   "outputs": [],
   "source": [
    "RunTwice = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e414fea-661c-40ed-b695-c3ed174b8d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "FilesPerRun = 8\n",
    "ConcurrentFiles = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6ac60be-1f53-4a38-8bf8-600b7d52a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RollingStdDevFaster(RawData, SmoothData, RollSize = 25):\n",
    "\n",
    "    Diffs = RawData - SmoothData\n",
    "    del RawData, SmoothData\n",
    "    \n",
    "    Sqs = Diffs * Diffs\n",
    "    del Diffs\n",
    "    \n",
    "    Sqs = Sqs.tolist() \n",
    "    Sqs.extend(np.zeros(RollSize))\n",
    "    mSqs = np.matrix(Sqs)\n",
    "    \n",
    "    for i in range(RollSize):\n",
    "        Sqs.insert(0, Sqs.pop())\n",
    "        mSqs = np.concatenate((np.matrix(Sqs),mSqs))\n",
    "    \n",
    "    sVect = mSqs.sum(axis=0)\n",
    "    eVect = (mSqs!=0).sum(axis=0)\n",
    "    del mSqs, Sqs\n",
    "    \n",
    "    VarVect = sVect / eVect\n",
    "    StdDevs = np.sqrt(VarVect)\n",
    "    return np.asarray(StdDevs[:-RollSize].T)\n",
    "\n",
    "def SquelchPattern(DataSet, StallRange = 5000, SquelchLevel = 0.02, verbose = False):\n",
    "    \n",
    "    SquelchSignal = np.ones(len(DataSet))\n",
    "    if verbose:\n",
    "        print(len(SquelchSignal))\n",
    "        \n",
    "    for i in range(len(DataSet)-2*StallRange):\n",
    "        if np.average(DataSet[i:i+StallRange]) < SquelchLevel:\n",
    "            SquelchSignal[i+StallRange]=0\n",
    "\n",
    "    return SquelchSignal\n",
    "\n",
    "def split_list_by_ones(original_list, ones_list):\n",
    "    # Created with Bing AI support\n",
    "    #  1st request: \"python split list into chunks based on value\"\n",
    "    #  2nd request: \"I want to split the list based on the values in a second list.  Second list is all 1s and 0s.  I want all 0s removed, and each set of consequtive ones as its own item\"\n",
    "    #  3rd request: \"That is close.  Here is an example of the two lists, and what I would want returned: original_list = [1, 2, 3, 8, 7, 4, 5, 6, 4, 7, 8, 9]\n",
    "    #                ones_list =     [1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1]\n",
    "    #                return: [[1, 2, 3, 8], [4, 5, 6], [8,9]]\"\n",
    "    #\n",
    "    #This is the function that was created and seems to work on the short lists, goin to use fo rlong lists\n",
    "    \n",
    "    result_sublists = []\n",
    "    sublist = []\n",
    "\n",
    "    for val, is_one in zip(original_list, ones_list):\n",
    "        if is_one:\n",
    "            sublist.append(val)\n",
    "        elif sublist:\n",
    "            result_sublists.append(sublist)\n",
    "            sublist = []\n",
    "\n",
    "    # Add the last sublist (if any)\n",
    "    if sublist:\n",
    "        result_sublists.append(sublist)\n",
    "\n",
    "    return result_sublists\n",
    "\n",
    "def split_list_by_zeros(original_list, ones_list):\n",
    "    # modified split_list_by_ones function to instead split by the zeros.\n",
    "    #\n",
    "    #\n",
    "    # Created with Bing AI support\n",
    "    #  1st request: \"python split list into chunks based on value\"\n",
    "    #  2nd request: \"I want to split the list based on the values in a second list.  Second list is all 1s and 0s.  I want all 0s removed, and each set of consequtive ones as its own item\"\n",
    "    #  3rd request: \"That is close.  Here is an example of the two lists, and what I would want returned: original_list = [1, 2, 3, 8, 7, 4, 5, 6, 4, 7, 8, 9]\n",
    "    #                ones_list =     [1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1]\n",
    "    #                return: [[1, 2, 3, 8], [4, 5, 6], [8,9]]\"\n",
    "    #\n",
    "    #This is the function that was created and seems to work on the short lists, going to use for long lists\n",
    "    \n",
    "    result_sublists = []\n",
    "    sublist = []\n",
    "\n",
    "    for val, is_one in zip(original_list, ones_list):\n",
    "        if not is_one:\n",
    "            sublist.append(val)\n",
    "        elif sublist:\n",
    "            result_sublists.append(sublist)\n",
    "            sublist = []\n",
    "\n",
    "    # Add the last sublist (if any)\n",
    "    if sublist:\n",
    "        result_sublists.append(sublist)\n",
    "\n",
    "    return result_sublists\n",
    "\n",
    "# Generated training sequences for use in the model.\n",
    "def create_sequences(values, time_steps=TIME_STEPS, skips = Skips):\n",
    "    output = []\n",
    "    for i in range(int((len(values) - time_steps + skips)/skips)):\n",
    "        output.append(values[i*skips : (i*skips + time_steps)])\n",
    "    return np.stack(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46bca307-8ab5-4410-a6a4-4e7e6d3087f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runFile(file, verbose = False, small = False, index=0, start=ti()):\n",
    "    noise = verbose\n",
    "    if file[-4:] == '.csv':    \n",
    "        dataset = pd.read_csv(DataFolder+file, delimiter =\",\", header=None, engine='python',on_bad_lines='skip')\n",
    "        if noise:\n",
    "            print(\"File Read\", ti()-start)\n",
    "        dataset = dataset.rename(columns={0:\"Day\"})\n",
    "        dataset = dataset.rename(columns={1:\"Second\"})\n",
    "        dataset = dataset.rename(columns={2:\"FracSec\"})\n",
    "        dataset = dataset.rename(columns={3:\"p\"})\n",
    "        dataset = dataset.rename(columns={4:\"h\"})\n",
    "        dataset = dataset.rename(columns={5:\"v\"})\n",
    "        dataset = dataset.rename(columns={6:\"Sensor\"})\n",
    "\n",
    "        dataset['Second'].replace('',0)\n",
    "        dataset['FracSec'].replace('',0)\n",
    "        dataset.replace([np.nan, np.inf, -np.inf],0,inplace=True)\n",
    "        \n",
    "        dataset[['Day','Second']] = dataset[['Day','Second']].apply(lambda x: x.astype(int).astype(str).str.zfill(6))\n",
    "        dataset[['FracSec']] = dataset[['FracSec']].apply(lambda x: x.astype(int).astype(str).str.zfill(4))\n",
    "\n",
    "        dataset[\"timestamp\"] = pd.to_datetime(dataset.Day+dataset.Second+dataset.FracSec,format='%y%m%d%H%M%S%f')\n",
    "        dataset[\"timestamps\"] = dataset[\"timestamp\"]\n",
    "\n",
    "        dataset[\"p\"] = dataset.p - np.average(dataset.p)\n",
    "        dataset[\"h\"] = dataset.h - np.average(dataset.h)\n",
    "        dataset[\"v\"] = dataset.v - np.average(dataset.v)\n",
    "        dataset[\"r\"] = np.sqrt(dataset.p**2 + dataset.h**2 + dataset.v**2)\n",
    "\n",
    "        dataset.index = dataset.timestamp\n",
    "\n",
    "        dataset[\"SmoothP\"] = denoise_wavelet(dataset.p, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        dataset[\"SmoothH\"] = denoise_wavelet(dataset.h, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        dataset[\"SmoothV\"] = denoise_wavelet(dataset.v, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        dataset[\"SmoothR\"] = denoise_wavelet(dataset.r, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "\n",
    "        if noise:\n",
    "            print(\"Data Cleaned\", ti()-start, len(dataset.p))\n",
    "\n",
    "        RawData = dataset.v\n",
    "        SmoothData = dataset.SmoothV\n",
    "        RollSize = 25\n",
    "\n",
    "        Diffs = RawData - SmoothData\n",
    "\n",
    "        Sqs = Diffs * Diffs\n",
    "\n",
    "        Sqs = Sqs.tolist() \n",
    "\n",
    "        Sqs.extend(np.zeros(RollSize))\n",
    "\n",
    "        mSqs = np.matrix(Sqs)\n",
    "\n",
    "        for i in range(RollSize):\n",
    "            Sqs.insert(0, Sqs.pop())\n",
    "            mSqs = np.concatenate((np.matrix(Sqs),mSqs))\n",
    "\n",
    "        sVect = mSqs.sum(axis=0)\n",
    "        eVect = (mSqs!=0).sum(axis=0)\n",
    "\n",
    "        VarVect = sVect / eVect\n",
    "\n",
    "        StdDevs = np.sqrt(VarVect)\n",
    "\n",
    "        StdDevsZ = np.asarray(StdDevs)\n",
    "\n",
    "        StdDevsZ=np.append(StdDevsZ,[0])\n",
    "\n",
    "        StdDevsZ = np.asarray(StdDevsZ.T[:len(dataset.p)])\n",
    "\n",
    "        if noise:\n",
    "            print(\"Size StdDevsZ\", ti()-start, np.shape(StdDevsZ))\n",
    "\n",
    "        #StdDevsZ = np.nan_to_num(StdDevsZ)\n",
    "\n",
    "        #StdDevsZ[StdDevsZ == np.inf] = 0\n",
    "        #StdDevsZ[StdDevsZ == -np.inf] = 0\n",
    "\n",
    "        if noise:\n",
    "            print(\"cleaned\", ti()-start, np.shape(StdDevsZ))\n",
    "\n",
    "        SmoothDevZ = denoise_wavelet(StdDevsZ, method='VisuShrink', mode='soft', wavelet='sym2', rescale_sigma='True')\n",
    "\n",
    "        if noise:\n",
    "            print(\"denoise 1\", ti()-start, np.shape(StdDevsZ))\n",
    "\n",
    "        #SmoothDevZa = cf.Smoothing(StdDevsZ, 3, wvt='sym2', dets_to_remove=2, levels=3)\n",
    "        #SmoothDevZ = np.ravel(SmoothDevZ[0,:])\n",
    "\n",
    "        #SmoothDevZ = SmoothDevZ.tolist()\n",
    "\n",
    "        if noise:\n",
    "            print(\"denoise 2\", ti()-start, np.shape(SmoothDevZ))\n",
    "\n",
    "        #ataset[\"SmoothDevZ\"] = SmoothDevZ\n",
    "\n",
    "        SmoothDevZ[np.isnan(SmoothDevZ)]=0\n",
    "        \n",
    "        Max = np.max(SmoothDevZ)\n",
    "\n",
    "        \n",
    "        \n",
    "        if noise:\n",
    "            print(\"Max\", ti()-start, np.shape(Max), Max)\n",
    "\n",
    "        buckets = int(Max / 0.005) + 1\n",
    "        bins = np.linspace(0,buckets*0.005,buckets+1)\n",
    "        counts, bins = np.histogram(SmoothDevZ,bins=bins)\n",
    "\n",
    "        CummCount = 0\n",
    "        HalfWay = 0\n",
    "        for i in range(len(counts)):\n",
    "            CummCount += counts[i]\n",
    "            if CummCount / len(SmoothDevZ) >= 0.5:\n",
    "                if HalfWay == 0:\n",
    "                    HalfWay = i\n",
    "\n",
    "        SquelchLevel = bins[HalfWay] \n",
    "        if noise:\n",
    "            print(\"SmoothDevz size\", np.shape(SmoothDevZ))\n",
    "\n",
    "        dataset[\"IsMoving\"] = SquelchPattern(SmoothDevZ, 4000, SquelchLevel, verbose=noise)\n",
    "\n",
    "        if noise:\n",
    "            print(\"Squelch Made\", ti()-start)\n",
    "        #dataset[\"velocity\"] = getVelocity(dataset.p, dataset.FracSec, dataset.IsMoving, 2)\n",
    "        #if noise:\n",
    "        #    print(\"Velocity Calculated.  File done: \",file)\n",
    "\n",
    "        #df_pr = split_list_by_zeros(dataset.p, dataset.IsMoving)\n",
    "        #df_hr = split_list_by_ones(dataset.h, dataset.IsMoving)\n",
    "        #df_vr = split_list_by_ones(dataset.v, dataset.IsMoving)\n",
    "        #df_rrr = split_list_by_ones(dataset.r, dataset.IsMoving)\n",
    "        df_ps = split_list_by_ones(dataset.SmoothP, dataset.IsMoving)\n",
    "        df_hs = split_list_by_ones(dataset.SmoothH, dataset.IsMoving)\n",
    "        df_vs = split_list_by_ones(dataset.SmoothV, dataset.IsMoving)\n",
    "        df_rs = split_list_by_ones(dataset.SmoothR, dataset.IsMoving)\n",
    "\n",
    "        del dataset\n",
    "        \n",
    "        MatsSmooth = []\n",
    "        for i in range(len(df_ps)):\n",
    "            MatsSmooth.append(np.vstack((df_ps[i],df_hs[i],df_vs[i],df_rs[i])))\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Split by ones\", ti()-start)\n",
    "\n",
    "\n",
    "        '''df_p=[0]\n",
    "        df_h=[0]\n",
    "        df_v=[0]\n",
    "        df_r=[0]\n",
    "        df_rp=[0]\n",
    "        df_rh=[0]\n",
    "        df_rv=[0]\n",
    "        df_rr=[0]\n",
    "        for i in range(len(df_ps)):\n",
    "            df_p += df_ps[i]\n",
    "            df_h += df_hs[i]\n",
    "            df_v += df_vs[i]\n",
    "            df_r += df_rs[i]\n",
    "            df_rp += df_pr[i]\n",
    "            df_rh += df_hr[i]\n",
    "            df_rv += df_vr[i]\n",
    "            df_rr += df_rrr[i]\n",
    "        '''\n",
    "        if verbose:\n",
    "            print('format changed', ti()-start, len(MatsSmooth))\n",
    "\n",
    "        return MatsSmooth\n",
    "    else:\n",
    "        return ['fail','fail']\n",
    "        \n",
    "        #if verbose:\n",
    "        #    print('Data normalized', ti()-start)\n",
    "\n",
    "        #return df_p, df_h, df_v, df_r, df_rp, df_rh, df_rv, df_rr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "953c6863-42b9-4908-bff9-cdb6078141e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LastSuccesfull ==0:\n",
    "    files= os.listdir(DataFolder) \n",
    "    random.shuffle(files)\n",
    "    with open('CurrentFileList.text','w') as file:\n",
    "        for item in files:\n",
    "            file.write(f\"{item}\\n\")\n",
    "else:\n",
    "    with open('CurrentFileList.text','r') as file:\n",
    "        files = file.readlines()\n",
    "    files=[item.strip() for item in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c496d295-af9d-4ead-b6be-d305e86cad81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Read 124.15165543556213\n",
      "Data Cleaned 210.2705945968628 26150162\n",
      "Size StdDevsZ 241.95979642868042 (26150162,)\n",
      "cleaned 241.9599530696869 (26150162,)\n",
      "denoise 1 242.59239387512207 (26150162,)\n",
      "denoise 2 242.59249687194824 (26150162,)\n",
      "Max 242.61205792427063 () 0.06201649233611547\n",
      "SmoothDevz size (26150162,)\n",
      "26150162\n",
      "Squelch Made 362.27342891693115\n",
      "Split by ones 379.1518611907959\n",
      "format changed 379.1520268917084 1175\n",
      "CPU times: user 5min 52s, sys: 24.8 s, total: 6min 17s\n",
      "Wall time: 6min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#df_p, df_h, df_v, df_r, \n",
    "Mats = runFile(files[0], verbose = True, small = False, index=0, start=ti())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "902f00b1-e96a-4c7e-b844-d4bd52adc131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runWrapper(file_path, verbose=True, small=False, index=0, start=ti()):\n",
    "    try:\n",
    "        rtrn = runFile(file_path, verbose, small, index, start)\n",
    "        return rtrn\n",
    "    except Exception as e:\n",
    "        with open('BadInputs.text', 'a') as bad_file:\n",
    "            bad_file.write(file_path + '\\n')\n",
    "        return np.zeros((10, 10, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37d8a1f-241a-45c2-835f-b28902ee2c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "AllDatas = Parallel(n_jobs=ConcurrentFiles)(delayed(runWrapper)(files[(i+1+LastSuccesfull*FilesPerRun)], False, False, 0, ti()) for i in range(FilesPerRun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83fb42d-bf61-453f-9963-a9beeafb7d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fileResponse in AllDatas:\n",
    "    for Mat in fileResponse:\n",
    "        Mats.append(Mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20cc4b4-8373-45c9-864f-6951894366b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Mats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933adb44-58cf-4121-a00a-ad13a23ca315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import CoreFunctions as cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbd7992-59d1-43e2-99a8-2540ae4cf4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "rejects = []\n",
    "Keeps = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad25b878-c896-490e-97c3-9a77efd4943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for Mat in Mats:\n",
    "    spm = np.shape(Mat)\n",
    "    if len(spm) > 1:\n",
    "        lenM = spm[1]\n",
    "    else:\n",
    "        lenM = 1\n",
    "    if (lenM > 1250):\n",
    "        lengths.append(lenM)\n",
    "        Keeps.append(Mat)\n",
    "    else:\n",
    "        rejects.append(lenM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77ee1ab-999e-49d2-a870-41282dbfc9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MemoryProtection:\n",
    "    print('RAM memory % used:', psutil.virtual_memory()[2])\n",
    "    del Mats, AllDatas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d47fa2-fcdb-408a-a9d0-1dec4f15289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lengths) / (len(lengths)+len(rejects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a7263e-636a-4ce2-8c82-1cc24523c7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(lengths, bins=20)\n",
    "plt.xlabel(\"lengths\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292553a7-1478-4660-920e-b685072d5868",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prints = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca65672f-8f38-440f-9968-f225d71c10d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Keeps[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dbaf6f-2f31-416a-8428-5bca469416db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintWrap(Mat):\n",
    "    localPrints = []\n",
    "    lenm = np.shape(Mat)[1]\n",
    "    slices = int(lenm/TIME_STEPS)\n",
    "    for i in range(slices):\n",
    "        temp = (cf.makeMPFast(Mat[:3,i*TIME_STEPS:(i+1)*TIME_STEPS], wvt = 'sym4', scales = 32, spacer = 2, title = ''))\n",
    "        localPrints.append(temp.astype(np.float32)/255.0)\n",
    "    return localPrints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d56003-bb85-47fc-ae46-60b74ad8a90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "AllPrints = Parallel(n_jobs=ConcurrentFiles)(delayed(PrintWrap)(Mat) for Mat in Keeps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8629a168-bda0-4d2c-ba66-efdba5a60fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MemoryProtection:\n",
    "    print('RAM memory % used:', psutil.virtual_memory()[2])\n",
    "    del Keeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0babd0-af97-480b-932e-f815c5499146",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in AllPrints:\n",
    "    for fprint in group:\n",
    "        Prints.append(fprint[:, ::2, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53af29a-2ba6-48fb-8d17-83cafb384c26",
   "metadata": {},
   "source": [
    "for Mat in Keeps:\n",
    "    lenm = np.shape(Mat)[1]\n",
    "    slices = int(lenm/TIME_STEPS)\n",
    "    for i in range(slices):\n",
    "        temp = (cf.makeMPFast(Mat[:3,i*TIME_STEPS:(i+1)*TIME_STEPS], wvt = 'sym4', scales = 60, spacer = 1, title = ''))\n",
    "        Prints.append(temp.astype(np.float32)/255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb089c9-479b-4141-9b12-cf36869a472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MemoryProtection:\n",
    "    print('RAM memory % used:', psutil.virtual_memory()[2])\n",
    "    del AllPrints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0a1e56-c95d-4d7c-ae0d-50e0d61d2ff0",
   "metadata": {},
   "source": [
    "np.shape(Prints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6687524-416d-4865-a8a1-248fc617fda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Prints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccafceaf-dcd6-4144-804f-0a4977ffabd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(Prints[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dac7d8-2b60-4bca-a235-c55703bf498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    print(np.shape(Prints[i*13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3b997d-a788-4dd3-8974-1f28b916705f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    fig = plt.figure(figsize=(6,2), dpi=100)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(Prints[i*13], origin='lower',aspect='auto')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b347773d-6e6d-4b90-a416-614484995b0e",
   "metadata": {},
   "source": [
    "# Start Machine Learning\n",
    "## Using Autoencoder with Kears and Tensorflow\n",
    "cite: https://pyimagesearch.com/2020/02/17/autoencoders-with-keras-tensorflow-and-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf9e779-ae36-4be4-9de3-436cddfdb599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73075a69-dac3-4e26-8f4f-4a26c13da05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder:\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, filters=(32, 64), latentDim=24):\n",
    "        inputShape = (height, width, depth)\n",
    "        chanDim = -1\n",
    "        inputs = Input(shape=inputShape)\n",
    "        x = inputs\n",
    "\n",
    "        for f in filters:\n",
    "            x = Conv2D(f, (3, 3), strides=2, padding=\"same\")(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            x = BatchNormalization(axis=chanDim)(x)\n",
    "\n",
    "        volumeSize = K.int_shape(x)\n",
    "        print(\"Volume Size:\", volumeSize)\n",
    "        x = Flatten()(x)\n",
    "        latent = Dense(latentDim)(x)\n",
    "\n",
    "        encoder = Model(inputs, latent, name=\"encoder\")\n",
    "\n",
    "        latentInputs = Input(shape=(latentDim,))\n",
    "        flattenedVolumeSize = int(np.prod(volumeSize[1:]))\n",
    "        print(\"Flattened Volume Size:\", flattenedVolumeSize)\n",
    "        x = Dense(flattenedVolumeSize)(latentInputs)\n",
    "        x = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(x)\n",
    "\n",
    "        for f in filters[::-1]:\n",
    "            x = Conv2DTranspose(f, (3, 3), strides=2, padding=\"same\")(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            x = BatchNormalization(axis=chanDim)(x)\n",
    "\n",
    "        x = Conv2DTranspose(depth, (3, 3), padding=\"same\")(x)\n",
    "        outputs = Activation(\"sigmoid\")(x)\n",
    "\n",
    "        decoder = Model(latentInputs, outputs, name=\"decoder\")\n",
    "        autoencoder = Model(inputs, decoder(encoder(inputs)), name=\"autoencoder\")\n",
    "\n",
    "        return (encoder, decoder, autoencoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a2ee2a-8f18-4995-b1a1-707a2a0017bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "#import matplotlib\n",
    "#matplotlib.use(\"Agg\")\n",
    "# import the necessary packages\n",
    "#from pyimagesearch.convautoencoder import ConvAutoencoder\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "# construct the argument parse and parse the arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a660d2-332a-42e5-8682-b872dedbb4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8299fc99-d18e-4b17-915b-41f113171b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4144ff94-e398-4448-873a-11920fda2e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d9f0cb-f9c9-4c2a-832c-12fd3edb9936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the number of epochs to train for and batch size\n",
    "EPOCHS = 25\n",
    "BS = 128\n",
    "TestSplit = 10 # 1/this many\n",
    "\n",
    "# load the MNIST dataset\n",
    "\n",
    "random.shuffle(Prints)\n",
    "\n",
    "TestLength = int(len(Prints)/TestSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47dfb49-b2e5-47bb-ace4-0ac6cb9eaa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Prints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5698f80d-5316-46f1-b2f2-b91242987e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(Prints))\n",
    "print(Prints[0].dtype)\n",
    "print(np.shape(Prints))\n",
    "print('RAM memory % used:', psutil.virtual_memory()[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb369f0-e871-446c-93b1-e494907eb3a2",
   "metadata": {},
   "source": [
    "\n",
    "# Assuming 'Prints' is your list\n",
    "\n",
    "# Convert the list to a NumPy array with a consistent data type\n",
    "try:\n",
    "    prints_array = np.array(Prints, dtype=np.float32)\n",
    "except Exception as e:\n",
    "    print(f\"Error converting to NumPy array: {e}\")\n",
    "\n",
    "# Check the shape and type of the NumPy array\n",
    "print(f\"Shape of prints_array: {prints_array.shape}\")\n",
    "print(f\"Data type of prints_array: {prints_array.dtype}\")\n",
    "\n",
    "# Convert the NumPy array to a TensorFlow tensor\n",
    "try:\n",
    "    tensor_prints = tf.convert_to_tensor(prints_array)\n",
    "    print(tensor_prints)\n",
    "except Exception as e:\n",
    "    print(f\"Error converting to TensorFlow tensor: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc00923-0616-4193-a7f4-ba3cbf756495",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, image in enumerate(Prints):\n",
    "    if not isinstance(image, np.ndarray):\n",
    "        Prints[i] = np.array(image, dtype=np.float32)\n",
    "    elif image.dtype != np.float32:\n",
    "        Prints[i] = image.astype(np.float32)\n",
    "\n",
    "# Stack the images into a single NumPy array\n",
    "prints_array = np.stack(Prints, axis=0)\n",
    "\n",
    "if MemoryProtection:\n",
    "    del Prints\n",
    "    print('RAM memory % used:', psutil.virtual_memory()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704126ab-74f2-46df-92df-b9cd17af5ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if psutil.virtual_memory()[2] > 49:\n",
    "    temp = (float(psutil.virtual_memory()[2]) - 40.0) / float(psutil.virtual_memory()[2])\n",
    "    temp2 = int(np.shape(prints_array)[0]*temp)\n",
    "    prints_array = prints_array[:temp2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05cb2d8-fda1-4aee-9cd6-fa38cea233e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(prints_array)\n",
    "print('RAM memory % used:', psutil.virtual_memory()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28dc419-1add-4528-bae0-f1c568b50bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy array to a TensorFlow tensor\n",
    "tensor_prints = tf.convert_to_tensor(prints_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e407fa8b-7ee7-4f6e-9d3e-85c6bb4785a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ImageShape = np.shape(tensor_prints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba33f6-5d6a-4460-8c47-bd1f26fec50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MemoryProtection:\n",
    "    del prints_array\n",
    "    print('RAM memory % used:', psutil.virtual_memory()[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8def752-c899-42c6-977f-cdd88928dab7",
   "metadata": {},
   "source": [
    "NP = []\n",
    "for prt in Prints:\n",
    "    temp = tf.convert_to_tensor(prt)\n",
    "    NP.append(temp)\n",
    "NP = tf.onvert_to_tensor(NP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8ab82c-7efd-4819-bb56-0f3f5b473e3e",
   "metadata": {},
   "source": [
    "trainX = np.asarray(Prints[:TestLength]).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2dc914-6259-422b-9494-df4553f7be65",
   "metadata": {},
   "source": [
    "testX = Prints[TestLength:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1b4cfb-59d9-4e5a-9a4d-30f4e660ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trX = tensor_prints[:TestLength]  #tf.convert_to_tensor(trainX, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2596940c-eff2-4105-9dc5-209c71af2ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "teX = tensor_prints[TestLength:]  #tf.convert_to_tensor(testX, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647727cc-a87a-4814-9d47-ab64f797f74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MemoryProtection:\n",
    "    del tensor_prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7789e504-52cd-4336-a1ca-31a5a5c1431b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# construct our convolutional autoencoder\n",
    "print(\"[INFO] building autoencoder...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c1be56-981b-4c06-9874-11421be7c25d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if LastSuccesfull == 0:\n",
    "    (encoder, decoder, autoencoder) = ConvAutoencoder.build(ImageShape[2], ImageShape[1], ImageShape[3],(64,32),32)\n",
    "else:\n",
    "\n",
    "    \n",
    "\n",
    "    # Regular expression to match the filenames\n",
    "    pattern = re.compile(r'3DFP_(\\d{4})_(\\d{3})autoencoder\\.keras')\n",
    "    \n",
    "    # Initialize variables to track the highest numbers\n",
    "    max_main_number = -1\n",
    "    max_sub_number = -1\n",
    "    target_file = None\n",
    "    \n",
    "    # Iterate over the files in the directory\n",
    "    file_list = os.listdir(model_directory)\n",
    "    for filename in file_list:\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            main_number = int(match.group(1))\n",
    "            sub_number = int(match.group(2))\n",
    "            \n",
    "            # Check if this file has the highest main number\n",
    "            if main_number > max_main_number: # or (main_number == max_main_number and sub_number > max_sub_number):\n",
    "                max_main_number = main_number\n",
    "                \n",
    "    for filename in file_list:\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            main_number = int(match.group(1))\n",
    "            sub_number = int(match.group(2))\n",
    "        \n",
    "            if main_number==max_main_number and sub_number > max_sub_number:\n",
    "                max_sub_number = sub_number\n",
    "                target_file = filename\n",
    "    \n",
    "    reautoencoder = load_model(directory+target_file)\n",
    "    encoder = load_model(directory+target_file[:-18]+'_encoder.keras')\n",
    "    decoder = load_model(directory+target_file[:-18]+'_decoder.keras')\n",
    "\n",
    "    autoencoder_input = Input(shape=(ImageShape[1], ImageShape[2], ImageShape[3]))\n",
    "\n",
    "    # Pass the input through the encoder and decoder\n",
    "    encoded_repr = encoder(autoencoder_input)\n",
    "    reconstructed = decoder(encoded_repr)\n",
    "\n",
    "    # Create the reassembled autoencoder model\n",
    "    autoencoder = Model(autoencoder_input, reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8116afff-0622-40b5-943e-34f2dab5ca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder.summary())\n",
    "print(decoder.summary())\n",
    "print(autoencoder.summary())\n",
    "if LastSuccesfull != 0:\n",
    "    print(reautoencoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6e29b1-df0e-4f5a-a0c0-7201c21158a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=LR_Starting)\n",
    "autoencoder.compile(loss=\"mse\", optimizer=opt)\n",
    "# train the convolutional autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec853a24-296f-41f9-bafd-680202f9c2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomModelCheckpoint(Callback):\n",
    "    def __init__(self, filepath, save_freq):\n",
    "        super(CustomModelCheckpoint, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.save_freq = save_freq\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.save_freq == 0:\n",
    "            self.model.save(self.filepath.format(epoch=epoch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade6ac3-ca48-4bec-87f7-b59bade36226",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = CustomModelCheckpoint(\n",
    "    filepath=model_directory+'3DFP_'+DateString+'_00_{epoch:02d}.keras',\n",
    "    save_freq=3  \n",
    ")\n",
    "\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir='/scratch/models/profiles/'+DateString,\n",
    "                                            profile_batch='01, 125')\n",
    "\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor=\"loss\", patience=6, mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7928d43-1d2c-4935-bf9f-1d4d9809ef7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "H = autoencoder.fit(\n",
    "\ttrX, trX,\n",
    "\tvalidation_data=(teX, teX),\n",
    "\tepochs=25,\n",
    "    callbacks=[checkpoint_callback, es_callback],     \n",
    "\tbatch_size=BS*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65e18ba-f46d-4500-b733-c2f8b559ee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(H.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(H.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a690d40d-17a2-4652-9f2f-42fca9e12226",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pred = autoencoder.predict(teX[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ede93e8-539d-4203-889a-ce7b3c2a5a61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(7):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(10,2), dpi=100 )\n",
    "    ax1.imshow(teX[i*13], origin='lower',aspect='auto')\n",
    "    ax1.axis(\"off\")\n",
    "    ax2.imshow(x_train_pred[i*13], origin='lower',aspect='auto')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c85747-877f-4ffb-8125-5ed65187706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del trX, teX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bb7483-fc22-42d7-af1d-151faefbf1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414ed04a-b8ec-475d-bffc-4ef24cbfcef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "toc = ti()\n",
    "LR_Current = LR_Starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32593d5-7232-43f4-bcaa-d140e2385300",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RunTwice:\n",
    "    LoopsToGetAll = int(len(files)/FilesPerRun)-LastSuccesfull-1\n",
    "    print(f'Loops Needed: {LoopsToGetAll}')\n",
    "    for j in range(LoopsToGetAll):\n",
    "        j+=1+LastSuccesfull\n",
    "        Mats=[]\n",
    "        AllDatas = Parallel(n_jobs=ConcurrentFiles)(delayed(runWrapper)(files[(j*FilesPerRun+i)], False, False, 0, ti()) for i in range(FilesPerRun))\n",
    "        for fileResponse in AllDatas:\n",
    "            for Mat in fileResponse:\n",
    "                Mats.append(Mat)\n",
    "        \n",
    "        if MemoryProtection:\n",
    "            del AllDatas\n",
    "            print('RAM after AllData:', psutil.virtual_memory()[2])        \n",
    "        lengths = []\n",
    "        rejects = []\n",
    "        Keeps = []\n",
    "        \n",
    "        for Mat in Mats:\n",
    "            spm = np.shape(Mat)\n",
    "            if len(spm) > 1:\n",
    "                lenM = spm[1]\n",
    "            else:\n",
    "                lenM = 1\n",
    "            if (lenM > 1250):\n",
    "                lengths.append(lenM)\n",
    "                Keeps.append(Mat)\n",
    "            else:\n",
    "                rejects.append(lenM)\n",
    "        \n",
    "        if MemoryProtection:\n",
    "            del Mats\n",
    "        \n",
    "        Prints = []\n",
    "        \n",
    "        \n",
    "        AllPrints = Parallel(n_jobs=ConcurrentFiles)(delayed(PrintWrap)(Mat) for Mat in Keeps)\n",
    "        \n",
    "        if MemoryProtection:\n",
    "            del Keeps\n",
    "            print('RAM after Keeps:', psutil.virtual_memory()[2])\n",
    "        for group in AllPrints:\n",
    "            for fprint in group:\n",
    "                Prints.append(fprint[:, ::2, :])\n",
    "        \n",
    "        if MemoryProtection:\n",
    "            del AllPrints\n",
    "        \n",
    "        \n",
    "        random.shuffle(Prints)\n",
    "        \n",
    "        for i, image in enumerate(Prints):\n",
    "            if not isinstance(image, np.ndarray):\n",
    "                Prints[i] = np.array(image, dtype=np.float32)\n",
    "            elif image.dtype != np.float32:\n",
    "                Prints[i] = image.astype(np.float32)\n",
    "        \n",
    "        # Stack the images into a single NumPy array\n",
    "        prints_array = np.stack(Prints, axis=0)\n",
    "        \n",
    "        if MemoryProtection:\n",
    "            del Prints\n",
    "            print('RAM after Prints:', psutil.virtual_memory()[2])\n",
    "        # Convert the NumPy array to a TensorFlow tensor\n",
    "\n",
    "        if psutil.virtual_memory()[2] > 50:\n",
    "            trX = tf.convert_to_tensor(prints_array[:int((psutil.virtual_memory()[2]-50)/50)])\n",
    "        else:\n",
    "            trX = tf.convert_to_tensor(prints_array)\n",
    "        if MemoryProtection:\n",
    "            del prints_array\n",
    "        \n",
    "        ImageShape = np.shape(trX)\n",
    "        \n",
    "        #trX = tensor_prints[:TestLength]  #tf.convert_to_tensor(trainX, dtype=tf.float32)\n",
    "        \n",
    "        #teX = tensor_prints[TestLength:]  #tf.convert_to_tensor(testX, dtype=tf.float32)\n",
    "        \n",
    "        #if MemoryProtection:\n",
    "        #    del tensor_prints\n",
    "        \n",
    "        #checkpoint_callback = CustomModelCheckpoint(\n",
    "        #    filepath=model_directory+'3DFP_'+DateString+'_'+str(j)+'_{epoch:02d}.keras',\n",
    "        #    save_freq=3  \n",
    "        #)\n",
    "        \n",
    "        H = autoencoder.fit(\n",
    "        \ttrX, trX,\n",
    "        \tvalidation_split=0.1,\n",
    "        \tepochs=20,\n",
    "            #callbacks=[checkpoint_callback, es_callback],     \n",
    "        \tbatch_size=BS)\n",
    "        \n",
    "        plt.plot(H.history[\"loss\"], label=\"Training Loss\")\n",
    "        plt.plot(H.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        #random.shuffle(trX)\n",
    "        autoencoder.save(model_directory+'3DFP_'+DateString+'_'+str(j).zfill(3)+'autoencoder.keras')\n",
    "        encoder.save(model_directory+'3DFP_'+DateString+'_'+str(j).zfill(3)+'encoder.keras')\n",
    "        decoder.save(model_directory+'3DFP_'+DateString+'_'+str(j).zfill(3)+'decoder.keras')\n",
    "\n",
    "        \n",
    "        x_train_pred = autoencoder.predict(trX[:7])\n",
    "        \n",
    "        for i in range(7):\n",
    "            fig, (ax1, ax2, ax3) = plt.subplots(1, 3,figsize=(12,2), dpi=200 )\n",
    "            ax1.imshow(trX[i], origin='lower',aspect='auto')\n",
    "            ax1.axis(\"off\")\n",
    "            ax2.imshow(x_train_pred[i], origin='lower',aspect='auto')\n",
    "            ax2.axis(\"off\")\n",
    "            ax3.imshow(np.abs(trX[i]-x_train_pred[i]), origin='lower',aspect='auto')\n",
    "            ax3.axis(\"off\")\n",
    "            plt.show()\n",
    "\n",
    "        del trX, x_train_pred, fig, ax1,ax2,ax3\n",
    "\n",
    "        if j%LR_PeriodGrow == 0:\n",
    "            LR_Current *= LR_Expand\n",
    "        else:\n",
    "            LR_Current *= LR_Decay\n",
    "            \n",
    "        autoencoder.optimizer.learning_rate = LR_Current\n",
    "    \n",
    "        print(f'{j} of {LoopsToGetAll} in {int((ti()-toc)/.6)/100} minutes. Using { psutil.virtual_memory()[2]} of RAM')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
