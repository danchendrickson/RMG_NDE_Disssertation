{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Header used on the projects\n",
    "\n",
    "#first the major packages used for math and graphing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "import scipy.special as sp\n",
    "\n",
    "#Custome graph format style sheet\n",
    "plt.style.use('Prospectus.mplstyle')\n",
    "\n",
    "#If being run by a seperate file, use the seperate file's graph format and saving paramaeters\n",
    "#otherwise set what is needed\n",
    "if not 'Saving' in locals():\n",
    "    Saving = False\n",
    "if not 'Titles' in locals():\n",
    "    Titles = True\n",
    "if not 'Ledgends' in locals():\n",
    "    Ledgends = True\n",
    "if not 'FFormat' in locals():\n",
    "    FFormat = '.eps'\n",
    "if not 'location' in locals():\n",
    "    #save location.  First one is for running on home PC, second for running on the work laptop.  May need to make a global change\n",
    "    location = 'E:\\\\Documents\\\\Dan\\\\Code\\\\FigsAndPlots\\\\FigsAndPlotsDocument\\\\Figures\\\\'\n",
    "    #location = 'C:\\\\Users\\\\dhendrickson\\\\Documents\\\\github\\\\FigsAndPlots\\\\FigsAndPlotsDocument\\\\Figures\\\\'\n",
    "\n",
    "my_cmap = plt.get_cmap('gray')\n",
    "#Standard cycle for collors and line styles\n",
    "default_cycler = (cycler('color', ['0.00', '0.40', '0.60', '0.70']) + cycler(linestyle=['-', '--', ':', '-.']))\n",
    "plt.rc('axes', prop_cycle=default_cycler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#special Header for package imports:\n",
    "import os\n",
    "from random import random\n",
    "import datetime\n",
    "\n",
    "import keras\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    " \n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras_metrics as km\n",
    " \n",
    " \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SensorPositonFile = 'C:\\\\Users\\\\dhendrickson\\\\Documents\\\\Code\\\\RMG_NDE_Disssertation\\\\SensorLocations.csv'\n",
    "#SensorPositonFile = 'C:\\\\Users\\\\danhe\\\\Code\\\\RMG_NDE_Disssertation\\\\SensorLocations.csv'\n",
    "#SensorPositonFile = 'E:\\\\Documents\\\\Dan\\\\Code\\\\RMG_NDE_Disssertation\\\\SensorLocations.csv'\n",
    "SensorPositonFile = 'D:\\\\SensorStatsSmall.csv'\n",
    "\n",
    "OutputVectors = np.genfromtxt(open(SensorPositonFile,'r'), delimiter=',',skip_header=1,dtype=int, missing_values=0)\n",
    "\n",
    "def truthVector(Filename):\n",
    "    # Parses the filename, and compares it against the record of sensor position on cranes\n",
    "    # inputs: filename\n",
    "    # outputs: truth vector\n",
    "\n",
    "    try:\n",
    "        #Parsing the file name.  Assuming it is in the standard format\n",
    "\n",
    "        sSensor = Filename[23]\n",
    "        sDate = datetime.datetime.strptime('20'+Filename[10:21],\"%Y%m%d-%H%M\")\n",
    "\n",
    "        mask = []\n",
    "\n",
    "        i=0\n",
    "        #loops through the known sensor movements, and creates a filter mask\n",
    "        for spf in OutputVectors:\n",
    "            \n",
    "            startDate = datetime.datetime.strptime(str(spf[0])+str(spf[1]).zfill(2)+str(spf[2]).zfill(2)\n",
    "                +str(spf[3]).zfill(2)+str(spf[4]).zfill(2),\"%Y%m%d%H%M\")\n",
    "            #datetime.date(int(spf[0]), int(spf[1]), int(spf[2])) + datetime.timedelta(hours=spf[3]) + datetime.timedelta(minutes=spf[4])\n",
    "            endDate = datetime.datetime.strptime(str(spf[5])+str(spf[6]).zfill(2)+str(spf[7]).zfill(2)\n",
    "                +str(spf[8]).zfill(2)+str(spf[9]).zfill(2),\"%Y%m%d%H%M\")\n",
    "            #datetime.date(int(spf[5]), int(spf[6]), int(spf[7])) + datetime.timedelta(hours=spf[8]) + datetime.timedelta(minutes=spf[9])\n",
    "            \n",
    "            if sDate >= startDate and sDate <= endDate and int(spf[10]) == int(sSensor):\n",
    "                mask.append(True)\n",
    "                i+=1\n",
    "            else:\n",
    "                mask.append(False)\n",
    "            \n",
    "        if i != 1: print('error ', i, Filename)\n",
    "\n",
    "        results = OutputVectors[mask,11:]\n",
    "\n",
    "        if i > 1: \n",
    "            print('Found Two ', Filename)\n",
    "            results = results[0,:]\n",
    "        #np.array(results)\n",
    "    except:\n",
    "        print(Filename)\n",
    "        results = [0,0,0,0]\n",
    "    return results\n",
    "\n",
    "def makeFrames(input,sequ,frameLength):\n",
    "    frames=[] #np.array([],dtype=object,)\n",
    "    segmentGap = int((np.shape(input)[0]-frameLength)/sequ)\n",
    "    #print(segmentGap,sequ, frameLength)\n",
    "    for i in range(sequ):\n",
    "        start = i * segmentGap\n",
    "        imageMatrix = input[start:start+frameLength,:]\n",
    "        np.matrix(imageMatrix)\n",
    "        imageMatrix = imageMatrix.T\n",
    "        frames.append(imageMatrix)\n",
    "    \n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2021-11-12 15:18:00\n"
     ]
    }
   ],
   "source": [
    "Filename= 'Accel60k-211112-1518-s1.csv'\n",
    "\n",
    "Filename = '60kPoints'+Filename[8:]\n",
    "\n",
    "sSensor = Filename[23]\n",
    "sDate = datetime.datetime.strptime('20'+Filename[10:21],\"%Y%m%d-%H%M\")\n",
    "\n",
    "\n",
    "\n",
    "print(sSensor, sDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sample data set with labels\n",
    "\n",
    "#Number of files to get for test\n",
    "n = 10\n",
    "img_height , img_width = 3, 100\n",
    "numberFrames = 600\n",
    "\n",
    "#folder = 'E:\\\\Documents\\\\Dan\\\\PhD\\\\Data Backup\\\\ASC Accel Pi\\\\Excel Versions\\\\'\n",
    "#folder = 'C:\\\\Users\\\\danhe\\\\Code\\\\TestData\\\\'\n",
    "#folder = 'C:\\\\Users\\\\dhendrickson\\\\Desktop\\\\AccelData\\\\AccelData\\\\'\n",
    "folder = \"D:\\\\SmallCopy\\\\\"\n",
    "\n",
    "files = os.listdir(folder)\n",
    "#files = random.sample(files,n)\n",
    "\n",
    "DataSet = [] #np.array([],dtype=object,)\n",
    "\n",
    "ResultsSet = np.zeros((len(files),np.shape(OutputVectors[:,11:])[1])) #np.array([],dtype=object,)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(ResultsSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "\n",
    "for filename in files:\n",
    "    if filename[-3:] == 'csv':\n",
    "        ResultsSet[i,:] = truthVector(filename)\n",
    "        fileData = np.genfromtxt(open(folder+filename,'r'), delimiter=',',skip_header=0,missing_values=0).T[2:5,:]\n",
    "        frames = makeFrames(fileData.T,numberFrames,img_width)\n",
    "        #print(np.shape(DataSet),np.shape(frames),1)\n",
    "        frames = np.asarray(frames)\n",
    "        DataSet.append(frames)\n",
    "        i+=1\n",
    "    else: print(filename[-3:])\n",
    "\n",
    "#ResultsSet = np.asarray(ResultsSet)\n",
    "DataSet = np.asarray(DataSet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(DataSet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResultsSet = ResultsSet[0:np.shape(DataSet)[0],:]\n",
    "np.shape(ResultsSet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(DataSet, ResultsSet, test_size=0.20, shuffle=True, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(y_train))\n",
    "model = Sequential()\n",
    "model.add(ConvLSTM2D(filters = 64, \n",
    "            kernel_size = (3, 3), \n",
    "            return_sequences = False, \n",
    "            data_format = \"channels_last\", \n",
    "            input_shape = (numberFrames, img_height, img_width, 1)\n",
    "            )\n",
    "        )\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(np.shape(y_train)[1], activation = \"softmax\"))\n",
    " \n",
    "model.summary()\n",
    " \n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])\n",
    " \n",
    "earlystop = EarlyStopping(patience=7)   \n",
    "callbacks = [earlystop]\n",
    "\n",
    "history = model.fit(x = X_train, y = y_train, epochs=40, batch_size = 8 , shuffle=False, validation_split=0.2, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n",
      "55000\n",
      "60000\n",
      "65000\n",
      "70000\n",
      "75000\n",
      "80000\n",
      "85000\n",
      "90000\n",
      "95000\n",
      "100000\n",
      "105000\n",
      "110000\n",
      "115000\n",
      "120000\n",
      "125000\n",
      "130000\n",
      "135000\n",
      "140000\n",
      "145000\n",
      "150000\n",
      "155000\n",
      "160000\n",
      "165000\n",
      "170000\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "72674e6928aeb149b17451b3afa2eccc8eb91630257127848cb7dabc56ce48f2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
