{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "245ecb0f-9092-41d4-89f8-913e4a37dffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.special as sp\n",
    "import os as os\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from time import time as ti\n",
    "from time import ctime as ct\n",
    "from skimage.restoration import denoise_wavelet\n",
    "import pickle\n",
    "import CoreFunctions as cf\n",
    "import sys\n",
    "import random\n",
    "import psutil\n",
    "import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff382d22-33ae-467a-a43d-a272f44eb0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 12:22:50.696712: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-13 12:22:50.708882: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-13 12:22:50.718670: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-13 12:22:50.721586: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-13 12:22:50.729933: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-13 12:22:52.535448: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: UNKNOWN ERROR (34)\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "484d05e0-1007-4ac7-9b37-86cf182686ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFolder = '/sciclone/scr10/dchendrickson01/Recordings2/'\n",
    "DataFolder = '/scratch/Recordings2/'\n",
    "model_directory = '/scratch/models/stopped/'\n",
    "\n",
    "\n",
    "TIME_STEPS = 1200\n",
    "Skips = 125\n",
    "RollSize = 50\n",
    "\n",
    "LastSuccesfull = 1\n",
    "DateString = 'Bad'\n",
    "MakeOnesOrZeros = 1\n",
    "RunParallel = 1\n",
    "FilesPerRun = 12\n",
    "ConcurrentFiles = 6\n",
    "\n",
    "\n",
    "tic = ti()\n",
    "start = tic\n",
    "\n",
    "MemoryProtection = True\n",
    "noisy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fad91f14-71e3-426b-a755-5348f4c98159",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "869b723b-501a-461d-a00c-5b12306d9814",
   "metadata": {},
   "outputs": [],
   "source": [
    "RunTwice = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6ac60be-1f53-4a38-8bf8-600b7d52a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RollingStdDevFaster(RawData, SmoothData, RollSize = 25):\n",
    "\n",
    "    Diffs = RawData - SmoothData\n",
    "    del RawData, SmoothData\n",
    "    \n",
    "    Sqs = Diffs * Diffs\n",
    "    del Diffs\n",
    "    \n",
    "    Sqs = Sqs.tolist() \n",
    "    Sqs.extend(np.zeros(RollSize))\n",
    "    mSqs = np.matrix(Sqs)\n",
    "    \n",
    "    for i in range(RollSize):\n",
    "        Sqs.insert(0, Sqs.pop())\n",
    "        mSqs = np.concatenate((np.matrix(Sqs),mSqs))\n",
    "    \n",
    "    sVect = mSqs.sum(axis=0)\n",
    "    eVect = (mSqs!=0).sum(axis=0)\n",
    "    del mSqs, Sqs\n",
    "    \n",
    "    VarVect = sVect / eVect\n",
    "    StdDevs = np.sqrt(VarVect)\n",
    "    return np.asarray(StdDevs[:-RollSize].T)\n",
    "\n",
    "def SquelchPattern(DataSet, StallRange = 5000, SquelchLevel = 0.02, verbose = noisy):\n",
    "    \n",
    "    SquelchSignal = np.ones(len(DataSet))\n",
    "    if verbose:\n",
    "        print(len(SquelchSignal))\n",
    "        \n",
    "    for i in range(len(DataSet)-2*StallRange):\n",
    "        if np.average(DataSet[i:i+StallRange]) < SquelchLevel:\n",
    "            SquelchSignal[i+StallRange]=0\n",
    "\n",
    "    return SquelchSignal\n",
    "\n",
    "def split_list_by_ones(original_list, ones_list):\n",
    "    # Created with Bing AI support\n",
    "    #  1st request: \"python split list into chunks based on value\"\n",
    "    #  2nd request: \"I want to split the list based on the values in a second list.  Second list is all 1s and 0s.  I want all 0s removed, and each set of consequtive ones as its own item\"\n",
    "    #  3rd request: \"That is close.  Here is an example of the two lists, and what I would want returned: original_list = [1, 2, 3, 8, 7, 4, 5, 6, 4, 7, 8, 9]\n",
    "    #                ones_list =     [1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1]\n",
    "    #                return: [[1, 2, 3, 8], [4, 5, 6], [8,9]]\"\n",
    "    #\n",
    "    #This is the function that was created and seems to work on the short lists, goin to use fo rlong lists\n",
    "    \n",
    "    result_sublists = []\n",
    "    sublist = []\n",
    "\n",
    "    for val, is_one in zip(original_list, ones_list):\n",
    "        if is_one:\n",
    "            sublist.append(val)\n",
    "        elif sublist:\n",
    "            result_sublists.append(sublist)\n",
    "            sublist = []\n",
    "\n",
    "    # Add the last sublist (if any)\n",
    "    if sublist:\n",
    "        result_sublists.append(sublist)\n",
    "\n",
    "    return result_sublists\n",
    "\n",
    "def split_list_by_zeros(original_list, ones_list):\n",
    "    # modified split_list_by_ones function to instead split by the zeros.\n",
    "    #\n",
    "    #\n",
    "    # Created with Bing AI support\n",
    "    #  1st request: \"python split list into chunks based on value\"\n",
    "    #  2nd request: \"I want to split the list based on the values in a second list.  Second list is all 1s and 0s.  I want all 0s removed, and each set of consequtive ones as its own item\"\n",
    "    #  3rd request: \"That is close.  Here is an example of the two lists, and what I would want returned: original_list = [1, 2, 3, 8, 7, 4, 5, 6, 4, 7, 8, 9]\n",
    "    #                ones_list =     [1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1]\n",
    "    #                return: [[1, 2, 3, 8], [4, 5, 6], [8,9]]\"\n",
    "    #\n",
    "    #This is the function that was created and seems to work on the short lists, going to use for long lists\n",
    "    \n",
    "    result_sublists = []\n",
    "    sublist = []\n",
    "\n",
    "    for val, is_one in zip(original_list, ones_list):\n",
    "        if not is_one:\n",
    "            sublist.append(val)\n",
    "        elif sublist:\n",
    "            result_sublists.append(sublist)\n",
    "            sublist = []\n",
    "\n",
    "    # Add the last sublist (if any)\n",
    "    if sublist:\n",
    "        result_sublists.append(sublist)\n",
    "\n",
    "    return result_sublists\n",
    "\n",
    "# Generated training sequences for use in the model.\n",
    "def create_sequences(values, time_steps=TIME_STEPS, skips = Skips):\n",
    "    output = []\n",
    "    for i in range(int((len(values) - time_steps + skips)/skips)):\n",
    "        output.append(values[i*skips : (i*skips + time_steps)])\n",
    "    return np.stack(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46bca307-8ab5-4410-a6a4-4e7e6d3087f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runFile(file, verbose = noisy, small = False, index=0, start=ti()):\n",
    "    noise = verbose\n",
    "    if file[-4:] == '.csv':    \n",
    "        dataset = pd.read_csv(DataFolder+file, delimiter =\",\", header=None, engine='python',on_bad_lines='skip')\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"File Read\", ti()-start)\n",
    "            \n",
    "        dataset = dataset.rename(columns={0:\"Day\"})\n",
    "        dataset = dataset.rename(columns={1:\"Second\"})\n",
    "        dataset = dataset.rename(columns={2:\"FracSec\"})\n",
    "        dataset = dataset.rename(columns={3:\"p\"})\n",
    "        dataset = dataset.rename(columns={4:\"h\"})\n",
    "        dataset = dataset.rename(columns={5:\"v\"})\n",
    "        dataset = dataset.rename(columns={6:\"Sensor\"})\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Columns renamed\", ti()-start)\n",
    "\n",
    "        #dataset['Second'].replace('',0)\n",
    "        #dataset['FracSec'].replace('',0)\n",
    "        #dataset.replace([np.nan, np.inf, -np.inf],0,inplace=True)\n",
    "        #if noise:\n",
    "        #    print(\"Seconds and Frax Second clean\", ti()-start)\n",
    "        \n",
    "        #dataset[['Day','Second']] = dataset[['Day','Second']].apply(lambda x: x.astype(int).astype(str).str.zfill(6))\n",
    "        #dataset[['FracSec']] = dataset[['FracSec']].apply(lambda x: x.astype(int).astype(str).str.zfill(4))\n",
    "\n",
    "        #dataset[\"timestamp\"] = pd.to_datetime(dataset.Day+dataset.Second+dataset.FracSec,format='%y%m%d%H%M%S%f')\n",
    "        #dataset[\"timestamps\"] = dataset[\"timestamp\"]\n",
    "        #if noise:\n",
    "        #    print(\"time stamps made\", ti()-start)\n",
    "\n",
    "        dataset[\"p\"] = dataset.p - np.average(dataset.p)\n",
    "        dataset[\"h\"] = dataset.h - np.average(dataset.h)\n",
    "        dataset[\"v\"] = dataset.v - np.average(dataset.v)\n",
    "        #dataset[\"r\"] = np.sqrt(dataset.p**2 + dataset.h**2 + dataset.v**2)\n",
    "        if verbose:\n",
    "            print(\"signals normalized\", ti()-start)\n",
    "\n",
    "        #dataset.index = dataset.timestamp\n",
    " \n",
    "        if verbose:\n",
    "            print(\"Data Loaded\", ti()-start, len(dataset.p))\n",
    "\n",
    "\n",
    "        dataset[\"SmoothP\"] = denoise_wavelet(dataset.p, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        dataset[\"SmoothH\"] = denoise_wavelet(dataset.h, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        dataset[\"SmoothV\"] = denoise_wavelet(dataset.v, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        #dataset[\"SmoothR\"] = denoise_wavelet(dataset.r, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Data Cleaned\", ti()-start, len(dataset.p))\n",
    "\n",
    "        RawData = dataset.v\n",
    "        SmoothData = dataset.SmoothV\n",
    "        RollSize = 25\n",
    "\n",
    "        Diffs = RawData - SmoothData\n",
    "\n",
    "        Sqs = Diffs * Diffs\n",
    "\n",
    "        Sqs = Sqs.tolist() \n",
    "\n",
    "        Sqs.extend(np.zeros(RollSize))\n",
    "\n",
    "        mSqs = np.matrix(Sqs)\n",
    "\n",
    "        for i in range(RollSize):\n",
    "            Sqs.insert(0, Sqs.pop())\n",
    "            mSqs = np.concatenate((np.matrix(Sqs),mSqs))\n",
    "\n",
    "        sVect = mSqs.sum(axis=0)\n",
    "        eVect = (mSqs!=0).sum(axis=0)\n",
    "\n",
    "        VarVect = sVect / eVect\n",
    "\n",
    "        StdDevs = np.sqrt(VarVect)\n",
    "\n",
    "        StdDevsZ = np.asarray(StdDevs)\n",
    "\n",
    "        StdDevsZ=np.append(StdDevsZ,[0])\n",
    "\n",
    "        StdDevsZ = np.asarray(StdDevsZ.T[:len(dataset.p)])\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Size StdDevsZ\", ti()-start, np.shape(StdDevsZ))\n",
    "\n",
    "        #StdDevsZ = np.nan_to_num(StdDevsZ)\n",
    "\n",
    "        #StdDevsZ[StdDevsZ == np.inf] = 0\n",
    "        #StdDevsZ[StdDevsZ == -np.inf] = 0\n",
    "\n",
    "        if verbose:\n",
    "            print(\"cleaned\", ti()-start, np.shape(StdDevsZ))\n",
    "\n",
    "        SmoothDevZ = denoise_wavelet(StdDevsZ, method='VisuShrink', mode='soft', wavelet='sym2', rescale_sigma='True')\n",
    "\n",
    "        if verbose:\n",
    "            print(\"denoise 1\", ti()-start, np.shape(StdDevsZ))\n",
    "\n",
    "        #SmoothDevZa = cf.Smoothing(StdDevsZ, 3, wvt='sym2', dets_to_remove=2, levels=3)\n",
    "        #SmoothDevZ = np.ravel(SmoothDevZ[0,:])\n",
    "\n",
    "        #SmoothDevZ = SmoothDevZ.tolist()\n",
    "\n",
    "        if verbose:\n",
    "            print(\"denoise 2\", ti()-start, np.shape(SmoothDevZ))\n",
    "\n",
    "        #ataset[\"SmoothDevZ\"] = SmoothDevZ\n",
    "\n",
    "        SmoothDevZ[np.isnan(SmoothDevZ)]=0\n",
    "        \n",
    "        Max = np.max(SmoothDevZ)\n",
    "\n",
    "        \n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Max\", ti()-start, np.shape(Max), Max)\n",
    "\n",
    "        buckets = int(Max / 0.005) + 1\n",
    "        bins = np.linspace(0,buckets*0.005,buckets+1)\n",
    "        counts, bins = np.histogram(SmoothDevZ,bins=bins)\n",
    "\n",
    "        CummCount = 0\n",
    "        HalfWay = 0\n",
    "        for i in range(len(counts)):\n",
    "            CummCount += counts[i]\n",
    "            if CummCount / len(SmoothDevZ) >= 0.5:\n",
    "                if HalfWay == 0:\n",
    "                    HalfWay = i\n",
    "\n",
    "        SquelchLevel = bins[HalfWay] \n",
    "        if verbose:\n",
    "            print(\"SmoothDevz size\", np.shape(SmoothDevZ))\n",
    "\n",
    "        dataset[\"IsMoving\"] = SquelchPattern(SmoothDevZ, 4000, SquelchLevel, verbose=noise)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Squelch Made\", ti()-start)\n",
    "        #dataset[\"velocity\"] = getVelocity(dataset.p, dataset.FracSec, dataset.IsMoving, 2)\n",
    "        #if noise:\n",
    "        #    print(\"Velocity Calculated.  File done: \",file)\n",
    "\n",
    "        #df_pr = split_list_by_zeros(dataset.p, dataset.IsMoving)\n",
    "        #df_hr = split_list_by_ones(dataset.h, dataset.IsMoving)\n",
    "        #df_vr = split_list_by_ones(dataset.v, dataset.IsMoving)\n",
    "        #df_rrr = split_list_by_ones(dataset.r, dataset.IsMoving)\n",
    "        if MakeOnesOrZeros == 1:\n",
    "            df_ps = split_list_by_ones(dataset.SmoothP, dataset.IsMoving)\n",
    "            df_hs = split_list_by_ones(dataset.SmoothH, dataset.IsMoving)\n",
    "            df_vs = split_list_by_ones(dataset.SmoothV, dataset.IsMoving)\n",
    "            #df_rs = split_list_by_ones(dataset.SmoothR, dataset.IsMoving)\n",
    "        else:\n",
    "            df_ps = split_list_by_zeros(dataset.SmoothP, dataset.IsMoving)\n",
    "            df_hs = split_list_by_zeros(dataset.SmoothH, dataset.IsMoving)\n",
    "            df_vs = split_list_by_zeros(dataset.SmoothV, dataset.IsMoving)\n",
    "            #df_rs = split_list_by_zeros(dataset.SmoothR, dataset.IsMoving)\n",
    "            \n",
    "\n",
    "        del dataset\n",
    "        \n",
    "        MatsSmooth = []\n",
    "        for i in range(len(df_ps)):\n",
    "            MatsSmooth.append(np.vstack((df_ps[i],df_hs[i],df_vs[i]))) #,df_rs[i])))\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Split by ones\", ti()-start)\n",
    "\n",
    "        if verbose:\n",
    "            print('format changed', ti()-start, len(MatsSmooth))\n",
    "\n",
    "        return MatsSmooth\n",
    "    else:\n",
    "        return ['fail','fail']\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7604206-e1c2-4532-a1e4-e2c4efcf42dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runWrapper(file_path, verbose=noisy, small=False, index=0, start=ti()):\n",
    "    try:\n",
    "        rtrn = runFile(file_path, verbose, small, index, start)\n",
    "        return rtrn\n",
    "    except Exception as e:\n",
    "        with open('BadInputs.text', 'a') as bad_file:\n",
    "            bad_file.write(file_path + '\\n')\n",
    "        return np.zeros((10, 10, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9428968c-6ce1-4cb6-9bb0-6801437167c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runWrapper3(file_path, verbose=noisy, small=False, index=0, start=ti(), timeout=1800):\n",
    "    pool = multiprocessing.Pool(1) \n",
    "    result = pool.apply_async(runFile, (file_path, verbose, small, index, start,)) \n",
    "    try: \n",
    "        return result.get(timeout) \n",
    "    except multiprocessing.TimeoutError: \n",
    "        with open('BadInputs.text', 'a') as bad_file:\n",
    "            bad_file.write(file_path + '\\n')\n",
    "        return np.zeros((10, 10, 3))\n",
    "    finally: pool.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fdfe775-9ce2-488d-a33b-8f0292ebc4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Function to be called when a timeout occurs \n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutError(\"Task timed out\")\n",
    "def runWrapper2(file_path, verbose=noisy, small=False, index=0, start=ti()):\n",
    "    try:\n",
    "        # Set the signal handler for the timeout \n",
    "        signal.signal(signal.SIGALRM, timeout_handler) \n",
    "        # Set the timeout duration (in seconds) \n",
    "        signal.alarm(1750)\n",
    "        \n",
    "        rtrn = runFile(file_path, verbose, small, index, start)\n",
    "        signal.alarm(0)\n",
    "        return rtrn\n",
    "    \n",
    "    except TimeoutError as e:\n",
    "        with open('BadInputs.text', 'a') as bad_file:\n",
    "            bad_file.write(file_path + '\\n')\n",
    "        return np.zeros((10, 10, 3))\n",
    "    except Exception as e:\n",
    "        with open('BadInputs.text', 'a') as bad_file:\n",
    "            bad_file.write(file_path + '\\n')\n",
    "        return np.zeros((10, 10, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3ff605f-b991-42ea-92b1-1a3a8f94c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanNanInf(data):\n",
    "    # Create a copy of the array to avoid in-place operation issues \n",
    "    data_copy = np.copy(data) \n",
    "\n",
    "    # Replace NaNs with 0 \n",
    "    data_copy = np.nan_to_num(data_copy, nan=0.0) \n",
    "\n",
    "    # Replace positive and negative infinities with the maximum finite value in the array \n",
    "    finite_values = data_copy[np.isfinite(data_copy)] \n",
    "    max_finite_value = np.max(finite_values) \n",
    "    data_copy[np.isinf(data_copy)] = max_finite_value \n",
    "\n",
    "    # Convert to integers \n",
    "    # data_copy = data_copy.astype(float32) \n",
    "    \n",
    "    return data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "515cc766-0630-4d3f-b367-80617ff3851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintWrap(data):\n",
    "    localPrints = []\n",
    "\n",
    "    Mat = CleanNanInf(data)\n",
    "\n",
    "    lenm = np.shape(Mat)[1]\n",
    "    slices = int(lenm/TIME_STEPS)\n",
    "\n",
    "    for i in range(slices):\n",
    "        temp = (cf.makeMPFast(Mat[:3,i*TIME_STEPS:(i+1)*TIME_STEPS], wvt = 'sym4', scales = 32, spacer = 2, title = ''))\n",
    "        localPrints.append(temp.astype(np.float32)/255.0)\n",
    "    return localPrints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "953c6863-42b9-4908-bff9-cdb6078141e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "if LastSuccesfull ==0:\n",
    "    files= os.listdir(DataFolder) \n",
    "    random.shuffle(files)\n",
    "    with open(f'CurrentFileList{DateString}.text','w') as file:\n",
    "        for item in files:\n",
    "            file.write(f\"{item}\\n\")\n",
    "else:\n",
    "'''\n",
    "with open(f'CurrentFileList{DateString}.text','r') as file:\n",
    "    files = file.readlines()\n",
    "files=[item.strip() for item in files][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "956f17f1-80ce-45f2-9163-c3a5d6ab8a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "toc=ti()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32593d5-7232-43f4-bcaa-d140e2385300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional Loops Needed: 11, at current time Wed Nov 13 12:22:52 2024\n",
      "RAM after AllData: 34.7\n",
      "keeps:  1442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sciclone/home/dchendrickson01/Code/RMG_NDE_Disssertation/FingerprintML/CoreFunctions.py:457: RuntimeWarning: invalid value encountered in divide\n",
      "  cfX /= highest\n",
      "/sciclone/home/dchendrickson01/miniconda3/lib/python3.12/site-packages/numpy/matrixlib/defmatrix.py:138: RuntimeWarning: invalid value encountered in cast\n",
      "  return new.astype(intype)\n",
      "/sciclone/home/dchendrickson01/miniconda3/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM after Keeps: 53.7\n",
      "Prints:  151326\n",
      "Prints:  151326\n",
      "RAM after Prints: 57.2\n",
      "Cant get all memory use too high\n",
      "trX:  (132276, 32, 600, 3)\n",
      "1 of 12 in 14.64 minutes. Using 56.2 of RAM, at current time Wed Nov 13 12:37:31 2024\n",
      "RAM after AllData: 58.7\n",
      "keeps:  4639\n",
      "RAM after Keeps: 70.6\n",
      "Prints:  212532\n",
      "Prints:  212532\n",
      "RAM after Prints: 77.5\n",
      "Cant get all memory use too high\n",
      "trX:  (137116, 32, 600, 3)\n",
      "2 of 12 in 33.53 minutes. Using 65.7 of RAM, at current time Wed Nov 13 12:56:24 2024\n"
     ]
    }
   ],
   "source": [
    "if RunTwice:\n",
    "    LoopsToGetAll = int(len(files)/FilesPerRun)-LastSuccesfull\n",
    "    print(f'Additional Loops Needed: {LoopsToGetAll}, at current time {ct(ti())}')\n",
    "    for j in range(LoopsToGetAll):\n",
    "        j+=LastSuccesfull\n",
    "        Mats=[]\n",
    "        if RunParallel ==1:\n",
    "            AllDatas = Parallel(n_jobs=ConcurrentFiles)(delayed(runWrapper)(files[(j*FilesPerRun+i)], False, False, 0, ti()) for i in range(FilesPerRun))\n",
    "            # other backends are loky (default), threading, dask ,timeout=2400, backend='multiprocessing'\n",
    "        else:\n",
    "            AllDatas = []\n",
    "            for i in range(FilesPerRun):\n",
    "                if i == 5:\n",
    "                    details = True\n",
    "                else:\n",
    "                    details = False\n",
    "                FileIndex = int(j*FilesPerRun+i)\n",
    "                AllDatas.append(runWrapper(files[FileIndex], details, False, 0, ti()))\n",
    "                print(f'Got data on {i} of {FilesPerRun} in {int((ti()-toc)/.6)/100} minutes, at current time {ct(ti())}.')\n",
    "        \n",
    "        for fileResponse in AllDatas:\n",
    "            for Mat in fileResponse:\n",
    "                Mats.append(Mat)\n",
    "        \n",
    "        if MemoryProtection:\n",
    "            del AllDatas\n",
    "            print('RAM after AllData:', psutil.virtual_memory()[2])        \n",
    "        lengths = []\n",
    "        rejects = []\n",
    "        Keeps = []\n",
    "        \n",
    "        for Mat in Mats:\n",
    "            spm = np.shape(Mat)\n",
    "            if len(spm) > 1:\n",
    "                lenM = spm[1]\n",
    "            else:\n",
    "                lenM = 1\n",
    "            if (lenM > 1250):\n",
    "                lengths.append(lenM)\n",
    "                Keeps.append(Mat)\n",
    "            else:\n",
    "                rejects.append(lenM)\n",
    "        \n",
    "        if MemoryProtection:\n",
    "            del Mats, rejects\n",
    "        \n",
    "        Prints = []\n",
    "\n",
    "        print('keeps: ',len(Keeps))\n",
    "        \n",
    "        if RunParallel == 1:\n",
    "            AllPrints = Parallel(n_jobs=ConcurrentFiles)(delayed(PrintWrap)(Mat) for Mat in Keeps)\n",
    "        else:\n",
    "            AllPrints = []\n",
    "            for i, Mat in enumerate(Keeps):\n",
    "                AllPrints.append(PrintWrap(Mat))\n",
    "                if i % 25 == 0:\n",
    "                    print(f'Through {i} of {len(Keeps)} moves. In {int((ti()-toc)/.6)/100} minutes, at current time {ct(ti())}.')\n",
    "        \n",
    "        if MemoryProtection:\n",
    "            del Keeps\n",
    "            print('RAM after Keeps:', psutil.virtual_memory()[2])\n",
    "        for group in AllPrints:\n",
    "            for fprint in group:\n",
    "                Prints.append(fprint[:, ::2, :])\n",
    "        \n",
    "        if MemoryProtection:\n",
    "            del AllPrints\n",
    "        \n",
    "        random.shuffle(Prints)\n",
    "        \n",
    "        for i, image in enumerate(Prints):\n",
    "            if not isinstance(image, np.ndarray):\n",
    "                Prints[i] = np.array(image, dtype=np.float32)\n",
    "            elif image.dtype != np.float32:\n",
    "                Prints[i] = image.astype(np.float32)\n",
    "        print('Prints: ',len(Prints))\n",
    "        \n",
    "        # Stack the images into a single NumPy array\n",
    "        prints_array = np.stack(Prints, axis=0)\n",
    "        \n",
    "        print('Prints: ',len(prints_array))\n",
    "        \n",
    "        if MemoryProtection:\n",
    "            del Prints\n",
    "            print('RAM after Prints:', psutil.virtual_memory()[2])\n",
    "        # Convert the NumPy array to a TensorFlow tensor\n",
    "\n",
    "        if psutil.virtual_memory()[2] > 50:\n",
    "            print('Cant get all memory use too high')\n",
    "            memInUse = psutil.virtual_memory()[2]\n",
    "            ExtraMem = memInUse - 50\n",
    "            ExtraPercent = ExtraMem / memInUse\n",
    "            cutPoint = len(prints_array) * (1-ExtraPercent)\n",
    "            Cut = int(cutPoint) - 1\n",
    "            trX = tf.convert_to_tensor(prints_array[:Cut])\n",
    "        else:\n",
    "            trX = tf.convert_to_tensor(prints_array)\n",
    "        print('trX: ', trX.shape)\n",
    "        \n",
    "        if MemoryProtection:\n",
    "            del prints_array\n",
    "\n",
    "        if MakeOnesOrZeros ==1:\n",
    "            MoveStation = 'Moving'\n",
    "        elif MakeOnesOrZeros == 0:\n",
    "            MoveStation = 'Stationary'\n",
    "            \n",
    "        with open(DataFolder + f'MLPickles/{DateString}/{MoveStation}Dataset_{str(j).zfill(4)}_{str(trX.shape[0]).zfill(6)}.p', 'wb') as handle:\n",
    "            pickle.dump(trX, handle)\n",
    "\n",
    "        if MemoryProtection:\n",
    "            del trX\n",
    "    \n",
    "        print(f'{j} of {LoopsToGetAll+LastSuccesfull} in {int((ti()-toc)/.6)/100} minutes. Using { psutil.virtual_memory()[2]} of RAM, at current time {ct(ti())}')\n",
    "        #%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9561fdd-0df0-450f-b882-e19083cd4db3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
