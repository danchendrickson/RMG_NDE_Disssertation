{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "245ecb0f-9092-41d4-89f8-913e4a37dffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.special as sp\n",
    "import os as os\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from time import time as ti\n",
    "from time import ctime as ct\n",
    "from skimage.restoration import denoise_wavelet\n",
    "import pickle\n",
    "import CoreFunctions as cf\n",
    "import sys\n",
    "import random\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff382d22-33ae-467a-a43d-a272f44eb0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 14:31:09.932117: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-09 14:31:09.964576: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-09 14:31:09.974467: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-09 14:31:09.998577: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "484d05e0-1007-4ac7-9b37-86cf182686ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFolder = '/sciclone/scr10/dchendrickson01/Recordings2/'\n",
    "DataFolder = '/scratch/Recordings2/'\n",
    "model_directory = '/scratch/models/stopped/'\n",
    "\n",
    "\n",
    "TIME_STEPS = 1200\n",
    "Skips = 125\n",
    "RollSize = 50\n",
    "\n",
    "LastSuccesfull = 50\n",
    "DateString = '1011'\n",
    "MakeOnesOrZeros = 1\n",
    "RunParallel = 0\n",
    "FilesPerRun = 15\n",
    "ConcurrentFiles = 5\n",
    "\n",
    "\n",
    "tic = ti()\n",
    "start = tic\n",
    "\n",
    "MemoryProtection = True\n",
    "noisy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fad91f14-71e3-426b-a755-5348f4c98159",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "869b723b-501a-461d-a00c-5b12306d9814",
   "metadata": {},
   "outputs": [],
   "source": [
    "RunTwice = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6ac60be-1f53-4a38-8bf8-600b7d52a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RollingStdDevFaster(RawData, SmoothData, RollSize = 25):\n",
    "\n",
    "    Diffs = RawData - SmoothData\n",
    "    del RawData, SmoothData\n",
    "    \n",
    "    Sqs = Diffs * Diffs\n",
    "    del Diffs\n",
    "    \n",
    "    Sqs = Sqs.tolist() \n",
    "    Sqs.extend(np.zeros(RollSize))\n",
    "    mSqs = np.matrix(Sqs)\n",
    "    \n",
    "    for i in range(RollSize):\n",
    "        Sqs.insert(0, Sqs.pop())\n",
    "        mSqs = np.concatenate((np.matrix(Sqs),mSqs))\n",
    "    \n",
    "    sVect = mSqs.sum(axis=0)\n",
    "    eVect = (mSqs!=0).sum(axis=0)\n",
    "    del mSqs, Sqs\n",
    "    \n",
    "    VarVect = sVect / eVect\n",
    "    StdDevs = np.sqrt(VarVect)\n",
    "    return np.asarray(StdDevs[:-RollSize].T)\n",
    "\n",
    "def SquelchPattern(DataSet, StallRange = 5000, SquelchLevel = 0.02, verbose = noisy):\n",
    "    \n",
    "    SquelchSignal = np.ones(len(DataSet))\n",
    "    if verbose:\n",
    "        print(len(SquelchSignal))\n",
    "        \n",
    "    for i in range(len(DataSet)-2*StallRange):\n",
    "        if np.average(DataSet[i:i+StallRange]) < SquelchLevel:\n",
    "            SquelchSignal[i+StallRange]=0\n",
    "\n",
    "    return SquelchSignal\n",
    "\n",
    "def split_list_by_ones(original_list, ones_list):\n",
    "    # Created with Bing AI support\n",
    "    #  1st request: \"python split list into chunks based on value\"\n",
    "    #  2nd request: \"I want to split the list based on the values in a second list.  Second list is all 1s and 0s.  I want all 0s removed, and each set of consequtive ones as its own item\"\n",
    "    #  3rd request: \"That is close.  Here is an example of the two lists, and what I would want returned: original_list = [1, 2, 3, 8, 7, 4, 5, 6, 4, 7, 8, 9]\n",
    "    #                ones_list =     [1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1]\n",
    "    #                return: [[1, 2, 3, 8], [4, 5, 6], [8,9]]\"\n",
    "    #\n",
    "    #This is the function that was created and seems to work on the short lists, goin to use fo rlong lists\n",
    "    \n",
    "    result_sublists = []\n",
    "    sublist = []\n",
    "\n",
    "    for val, is_one in zip(original_list, ones_list):\n",
    "        if is_one:\n",
    "            sublist.append(val)\n",
    "        elif sublist:\n",
    "            result_sublists.append(sublist)\n",
    "            sublist = []\n",
    "\n",
    "    # Add the last sublist (if any)\n",
    "    if sublist:\n",
    "        result_sublists.append(sublist)\n",
    "\n",
    "    return result_sublists\n",
    "\n",
    "def split_list_by_zeros(original_list, ones_list):\n",
    "    # modified split_list_by_ones function to instead split by the zeros.\n",
    "    #\n",
    "    #\n",
    "    # Created with Bing AI support\n",
    "    #  1st request: \"python split list into chunks based on value\"\n",
    "    #  2nd request: \"I want to split the list based on the values in a second list.  Second list is all 1s and 0s.  I want all 0s removed, and each set of consequtive ones as its own item\"\n",
    "    #  3rd request: \"That is close.  Here is an example of the two lists, and what I would want returned: original_list = [1, 2, 3, 8, 7, 4, 5, 6, 4, 7, 8, 9]\n",
    "    #                ones_list =     [1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1]\n",
    "    #                return: [[1, 2, 3, 8], [4, 5, 6], [8,9]]\"\n",
    "    #\n",
    "    #This is the function that was created and seems to work on the short lists, going to use for long lists\n",
    "    \n",
    "    result_sublists = []\n",
    "    sublist = []\n",
    "\n",
    "    for val, is_one in zip(original_list, ones_list):\n",
    "        if not is_one:\n",
    "            sublist.append(val)\n",
    "        elif sublist:\n",
    "            result_sublists.append(sublist)\n",
    "            sublist = []\n",
    "\n",
    "    # Add the last sublist (if any)\n",
    "    if sublist:\n",
    "        result_sublists.append(sublist)\n",
    "\n",
    "    return result_sublists\n",
    "\n",
    "# Generated training sequences for use in the model.\n",
    "def create_sequences(values, time_steps=TIME_STEPS, skips = Skips):\n",
    "    output = []\n",
    "    for i in range(int((len(values) - time_steps + skips)/skips)):\n",
    "        output.append(values[i*skips : (i*skips + time_steps)])\n",
    "    return np.stack(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46bca307-8ab5-4410-a6a4-4e7e6d3087f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runFile(file, verbose = noisy, small = False, index=0, start=ti()):\n",
    "    noise = verbose\n",
    "    if file[-4:] == '.csv':    \n",
    "        dataset = pd.read_csv(DataFolder+file, delimiter =\",\", header=None, engine='python',on_bad_lines='skip')\n",
    "        if noise:\n",
    "            print(\"File Read\", ti()-start)\n",
    "        dataset = dataset.rename(columns={0:\"Day\"})\n",
    "        dataset = dataset.rename(columns={1:\"Second\"})\n",
    "        dataset = dataset.rename(columns={2:\"FracSec\"})\n",
    "        dataset = dataset.rename(columns={3:\"p\"})\n",
    "        dataset = dataset.rename(columns={4:\"h\"})\n",
    "        dataset = dataset.rename(columns={5:\"v\"})\n",
    "        dataset = dataset.rename(columns={6:\"Sensor\"})\n",
    "\n",
    "        dataset['Second'].replace('',0)\n",
    "        dataset['FracSec'].replace('',0)\n",
    "        dataset.replace([np.nan, np.inf, -np.inf],0,inplace=True)\n",
    "        \n",
    "        dataset[['Day','Second']] = dataset[['Day','Second']].apply(lambda x: x.astype(int).astype(str).str.zfill(6))\n",
    "        dataset[['FracSec']] = dataset[['FracSec']].apply(lambda x: x.astype(int).astype(str).str.zfill(4))\n",
    "\n",
    "        dataset[\"timestamp\"] = pd.to_datetime(dataset.Day+dataset.Second+dataset.FracSec,format='%y%m%d%H%M%S%f')\n",
    "        dataset[\"timestamps\"] = dataset[\"timestamp\"]\n",
    "\n",
    "        dataset[\"p\"] = dataset.p - np.average(dataset.p)\n",
    "        dataset[\"h\"] = dataset.h - np.average(dataset.h)\n",
    "        dataset[\"v\"] = dataset.v - np.average(dataset.v)\n",
    "        dataset[\"r\"] = np.sqrt(dataset.p**2 + dataset.h**2 + dataset.v**2)\n",
    "\n",
    "        dataset.index = dataset.timestamp\n",
    "\n",
    "        dataset[\"SmoothP\"] = denoise_wavelet(dataset.p, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        dataset[\"SmoothH\"] = denoise_wavelet(dataset.h, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        dataset[\"SmoothV\"] = denoise_wavelet(dataset.v, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        dataset[\"SmoothR\"] = denoise_wavelet(dataset.r, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "\n",
    "        if noise:\n",
    "            print(\"Data Cleaned\", ti()-start, len(dataset.p))\n",
    "\n",
    "        RawData = dataset.v\n",
    "        SmoothData = dataset.SmoothV\n",
    "        RollSize = 25\n",
    "\n",
    "        Diffs = RawData - SmoothData\n",
    "\n",
    "        Sqs = Diffs * Diffs\n",
    "\n",
    "        Sqs = Sqs.tolist() \n",
    "\n",
    "        Sqs.extend(np.zeros(RollSize))\n",
    "\n",
    "        mSqs = np.matrix(Sqs)\n",
    "\n",
    "        for i in range(RollSize):\n",
    "            Sqs.insert(0, Sqs.pop())\n",
    "            mSqs = np.concatenate((np.matrix(Sqs),mSqs))\n",
    "\n",
    "        sVect = mSqs.sum(axis=0)\n",
    "        eVect = (mSqs!=0).sum(axis=0)\n",
    "\n",
    "        VarVect = sVect / eVect\n",
    "\n",
    "        StdDevs = np.sqrt(VarVect)\n",
    "\n",
    "        StdDevsZ = np.asarray(StdDevs)\n",
    "\n",
    "        StdDevsZ=np.append(StdDevsZ,[0])\n",
    "\n",
    "        StdDevsZ = np.asarray(StdDevsZ.T[:len(dataset.p)])\n",
    "\n",
    "        if noise:\n",
    "            print(\"Size StdDevsZ\", ti()-start, np.shape(StdDevsZ))\n",
    "\n",
    "        #StdDevsZ = np.nan_to_num(StdDevsZ)\n",
    "\n",
    "        #StdDevsZ[StdDevsZ == np.inf] = 0\n",
    "        #StdDevsZ[StdDevsZ == -np.inf] = 0\n",
    "\n",
    "        if noise:\n",
    "            print(\"cleaned\", ti()-start, np.shape(StdDevsZ))\n",
    "\n",
    "        SmoothDevZ = denoise_wavelet(StdDevsZ, method='VisuShrink', mode='soft', wavelet='sym2', rescale_sigma='True')\n",
    "\n",
    "        if noise:\n",
    "            print(\"denoise 1\", ti()-start, np.shape(StdDevsZ))\n",
    "\n",
    "        #SmoothDevZa = cf.Smoothing(StdDevsZ, 3, wvt='sym2', dets_to_remove=2, levels=3)\n",
    "        #SmoothDevZ = np.ravel(SmoothDevZ[0,:])\n",
    "\n",
    "        #SmoothDevZ = SmoothDevZ.tolist()\n",
    "\n",
    "        if noise:\n",
    "            print(\"denoise 2\", ti()-start, np.shape(SmoothDevZ))\n",
    "\n",
    "        #ataset[\"SmoothDevZ\"] = SmoothDevZ\n",
    "\n",
    "        SmoothDevZ[np.isnan(SmoothDevZ)]=0\n",
    "        \n",
    "        Max = np.max(SmoothDevZ)\n",
    "\n",
    "        \n",
    "        \n",
    "        if noise:\n",
    "            print(\"Max\", ti()-start, np.shape(Max), Max)\n",
    "\n",
    "        buckets = int(Max / 0.005) + 1\n",
    "        bins = np.linspace(0,buckets*0.005,buckets+1)\n",
    "        counts, bins = np.histogram(SmoothDevZ,bins=bins)\n",
    "\n",
    "        CummCount = 0\n",
    "        HalfWay = 0\n",
    "        for i in range(len(counts)):\n",
    "            CummCount += counts[i]\n",
    "            if CummCount / len(SmoothDevZ) >= 0.5:\n",
    "                if HalfWay == 0:\n",
    "                    HalfWay = i\n",
    "\n",
    "        SquelchLevel = bins[HalfWay] \n",
    "        if noise:\n",
    "            print(\"SmoothDevz size\", np.shape(SmoothDevZ))\n",
    "\n",
    "        dataset[\"IsMoving\"] = SquelchPattern(SmoothDevZ, 4000, SquelchLevel, verbose=noise)\n",
    "\n",
    "        if noise:\n",
    "            print(\"Squelch Made\", ti()-start)\n",
    "        #dataset[\"velocity\"] = getVelocity(dataset.p, dataset.FracSec, dataset.IsMoving, 2)\n",
    "        #if noise:\n",
    "        #    print(\"Velocity Calculated.  File done: \",file)\n",
    "\n",
    "        #df_pr = split_list_by_zeros(dataset.p, dataset.IsMoving)\n",
    "        #df_hr = split_list_by_ones(dataset.h, dataset.IsMoving)\n",
    "        #df_vr = split_list_by_ones(dataset.v, dataset.IsMoving)\n",
    "        #df_rrr = split_list_by_ones(dataset.r, dataset.IsMoving)\n",
    "        if MakeOnesOrZeros == 1:\n",
    "            df_ps = split_list_by_ones(dataset.SmoothP, dataset.IsMoving)\n",
    "            df_hs = split_list_by_ones(dataset.SmoothH, dataset.IsMoving)\n",
    "            df_vs = split_list_by_ones(dataset.SmoothV, dataset.IsMoving)\n",
    "            df_rs = split_list_by_ones(dataset.SmoothR, dataset.IsMoving)\n",
    "        else:\n",
    "            df_ps = split_list_by_zeros(dataset.SmoothP, dataset.IsMoving)\n",
    "            df_hs = split_list_by_zeros(dataset.SmoothH, dataset.IsMoving)\n",
    "            df_vs = split_list_by_zeros(dataset.SmoothV, dataset.IsMoving)\n",
    "            df_rs = split_list_by_zeros(dataset.SmoothR, dataset.IsMoving)\n",
    "            \n",
    "\n",
    "        del dataset\n",
    "        \n",
    "        MatsSmooth = []\n",
    "        for i in range(len(df_ps)):\n",
    "            MatsSmooth.append(np.vstack((df_ps[i],df_hs[i],df_vs[i],df_rs[i])))\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Split by ones\", ti()-start)\n",
    "\n",
    "        if verbose:\n",
    "            print('format changed', ti()-start, len(MatsSmooth))\n",
    "\n",
    "        return MatsSmooth\n",
    "    else:\n",
    "        return ['fail','fail']\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6cd595d-00a4-44b2-acad-a4db08eebb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runWrapper(file_path, verbose=noisy, small=False, index=0, start=ti()):\n",
    "    try:\n",
    "        rtrn = runFile(file_path, verbose, small, index, start)\n",
    "        return rtrn\n",
    "    except Exception as e:\n",
    "        with open('BadInputs.text', 'a') as bad_file:\n",
    "            bad_file.write(file_path + '\\n')\n",
    "        return np.zeros((10, 10, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "515cc766-0630-4d3f-b367-80617ff3851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintWrap(Mat):\n",
    "    localPrints = []\n",
    "    lenm = np.shape(Mat)[1]\n",
    "    slices = int(lenm/TIME_STEPS)\n",
    "    for i in range(slices):\n",
    "        temp = (cf.makeMPFast(Mat[:3,i*TIME_STEPS:(i+1)*TIME_STEPS], wvt = 'sym4', scales = 32, spacer = 2, title = ''))\n",
    "        localPrints.append(temp.astype(np.float32)/255.0)\n",
    "    return localPrints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "953c6863-42b9-4908-bff9-cdb6078141e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LastSuccesfull ==0:\n",
    "    files= os.listdir(DataFolder) \n",
    "    random.shuffle(files)\n",
    "    with open(f'CurrentFileList{DateString}.text','w') as file:\n",
    "        for item in files:\n",
    "            file.write(f\"{item}\\n\")\n",
    "else:\n",
    "    with open(f'CurrentFileList{DateString}.text','r') as file:\n",
    "        files = file.readlines()\n",
    "    files=[item.strip() for item in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "956f17f1-80ce-45f2-9163-c3a5d6ab8a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "toc=ti()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84273690-c5fa-4c84-9337-20ced4464d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('filelist.csv', files, '%s', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32593d5-7232-43f4-bcaa-d140e2385300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional Loops Needed: 21, at current time Sat Nov  9 14:31:16 2024\n",
      "Got data on 0 of 15 in 6.42 minutes, at current time Sat Nov  9 14:37:42 2024.\n",
      "Got data on 1 of 15 in 12.61 minutes, at current time Sat Nov  9 14:43:53 2024.\n",
      "Got data on 2 of 15 in 21.47 minutes, at current time Sat Nov  9 14:52:45 2024.\n",
      "Got data on 3 of 15 in 23.36 minutes, at current time Sat Nov  9 14:54:38 2024.\n",
      "Got data on 4 of 15 in 26.14 minutes, at current time Sat Nov  9 14:57:25 2024.\n",
      "Got data on 5 of 15 in 34.87 minutes, at current time Sat Nov  9 15:06:08 2024.\n",
      "Got data on 6 of 15 in 41.17 minutes, at current time Sat Nov  9 15:12:27 2024.\n",
      "Got data on 7 of 15 in 49.8 minutes, at current time Sat Nov  9 15:21:05 2024.\n",
      "Got data on 8 of 15 in 56.07 minutes, at current time Sat Nov  9 15:27:21 2024.\n",
      "Got data on 9 of 15 in 64.87 minutes, at current time Sat Nov  9 15:36:09 2024.\n",
      "Got data on 10 of 15 in 73.69 minutes, at current time Sat Nov  9 15:44:58 2024.\n",
      "Got data on 11 of 15 in 82.22 minutes, at current time Sat Nov  9 15:53:29 2024.\n",
      "Got data on 12 of 15 in 86.94 minutes, at current time Sat Nov  9 15:58:13 2024.\n",
      "Got data on 13 of 15 in 89.58 minutes, at current time Sat Nov  9 16:00:51 2024.\n",
      "Got data on 14 of 15 in 92.39 minutes, at current time Sat Nov  9 16:03:40 2024.\n",
      "RAM after AllData: 29.5\n",
      "Through 0 of 2387 moves. In 93.78 minutes, at current time Sat Nov  9 16:05:03 2024.\n",
      "Through 25 of 2387 moves. In 94.18 minutes, at current time Sat Nov  9 16:05:27 2024.\n",
      "Through 50 of 2387 moves. In 94.58 minutes, at current time Sat Nov  9 16:05:51 2024.\n",
      "Through 75 of 2387 moves. In 97.46 minutes, at current time Sat Nov  9 16:08:44 2024.\n",
      "Through 100 of 2387 moves. In 99.31 minutes, at current time Sat Nov  9 16:10:35 2024.\n",
      "Through 125 of 2387 moves. In 101.3 minutes, at current time Sat Nov  9 16:12:35 2024.\n",
      "Through 150 of 2387 moves. In 103.55 minutes, at current time Sat Nov  9 16:14:49 2024.\n",
      "Through 175 of 2387 moves. In 103.89 minutes, at current time Sat Nov  9 16:15:10 2024.\n",
      "Through 200 of 2387 moves. In 103.92 minutes, at current time Sat Nov  9 16:15:12 2024.\n",
      "Through 225 of 2387 moves. In 103.96 minutes, at current time Sat Nov  9 16:15:14 2024.\n",
      "Through 250 of 2387 moves. In 104.01 minutes, at current time Sat Nov  9 16:15:17 2024.\n",
      "Through 275 of 2387 moves. In 104.07 minutes, at current time Sat Nov  9 16:15:21 2024.\n",
      "Through 300 of 2387 moves. In 104.15 minutes, at current time Sat Nov  9 16:15:26 2024.\n",
      "Through 325 of 2387 moves. In 104.22 minutes, at current time Sat Nov  9 16:15:30 2024.\n",
      "Through 350 of 2387 moves. In 104.27 minutes, at current time Sat Nov  9 16:15:33 2024.\n",
      "Through 375 of 2387 moves. In 104.31 minutes, at current time Sat Nov  9 16:15:35 2024.\n",
      "Through 400 of 2387 moves. In 104.37 minutes, at current time Sat Nov  9 16:15:38 2024.\n",
      "Through 425 of 2387 moves. In 104.39 minutes, at current time Sat Nov  9 16:15:40 2024.\n",
      "Through 450 of 2387 moves. In 104.42 minutes, at current time Sat Nov  9 16:15:42 2024.\n",
      "Through 475 of 2387 moves. In 104.48 minutes, at current time Sat Nov  9 16:15:45 2024.\n",
      "Through 500 of 2387 moves. In 104.55 minutes, at current time Sat Nov  9 16:15:49 2024.\n",
      "Through 525 of 2387 moves. In 104.6 minutes, at current time Sat Nov  9 16:15:53 2024.\n",
      "Through 550 of 2387 moves. In 104.66 minutes, at current time Sat Nov  9 16:15:56 2024.\n",
      "Through 575 of 2387 moves. In 104.79 minutes, at current time Sat Nov  9 16:16:04 2024.\n",
      "Through 600 of 2387 moves. In 104.85 minutes, at current time Sat Nov  9 16:16:07 2024.\n",
      "Through 625 of 2387 moves. In 104.89 minutes, at current time Sat Nov  9 16:16:10 2024.\n",
      "Through 650 of 2387 moves. In 105.0 minutes, at current time Sat Nov  9 16:16:17 2024.\n",
      "Through 675 of 2387 moves. In 105.22 minutes, at current time Sat Nov  9 16:16:30 2024.\n",
      "Through 700 of 2387 moves. In 105.47 minutes, at current time Sat Nov  9 16:16:44 2024.\n",
      "Through 725 of 2387 moves. In 105.96 minutes, at current time Sat Nov  9 16:17:14 2024.\n",
      "Through 750 of 2387 moves. In 106.15 minutes, at current time Sat Nov  9 16:17:25 2024.\n",
      "Through 775 of 2387 moves. In 106.32 minutes, at current time Sat Nov  9 16:17:35 2024.\n",
      "Through 800 of 2387 moves. In 106.67 minutes, at current time Sat Nov  9 16:17:57 2024.\n",
      "Through 825 of 2387 moves. In 106.97 minutes, at current time Sat Nov  9 16:18:15 2024.\n",
      "Through 850 of 2387 moves. In 107.29 minutes, at current time Sat Nov  9 16:18:34 2024.\n",
      "Through 875 of 2387 moves. In 107.53 minutes, at current time Sat Nov  9 16:18:48 2024.\n",
      "Through 900 of 2387 moves. In 108.02 minutes, at current time Sat Nov  9 16:19:18 2024.\n",
      "Through 925 of 2387 moves. In 108.32 minutes, at current time Sat Nov  9 16:19:36 2024.\n",
      "Through 950 of 2387 moves. In 108.55 minutes, at current time Sat Nov  9 16:19:49 2024.\n",
      "Through 975 of 2387 moves. In 108.73 minutes, at current time Sat Nov  9 16:20:00 2024.\n",
      "Through 1000 of 2387 moves. In 109.95 minutes, at current time Sat Nov  9 16:21:13 2024.\n",
      "Through 1025 of 2387 moves. In 110.11 minutes, at current time Sat Nov  9 16:21:23 2024.\n",
      "Through 1050 of 2387 moves. In 110.15 minutes, at current time Sat Nov  9 16:21:25 2024.\n",
      "Through 1075 of 2387 moves. In 110.18 minutes, at current time Sat Nov  9 16:21:27 2024.\n",
      "Through 1100 of 2387 moves. In 110.2 minutes, at current time Sat Nov  9 16:21:28 2024.\n",
      "Through 1125 of 2387 moves. In 110.22 minutes, at current time Sat Nov  9 16:21:30 2024.\n",
      "Through 1150 of 2387 moves. In 110.26 minutes, at current time Sat Nov  9 16:21:32 2024.\n",
      "Through 1175 of 2387 moves. In 110.3 minutes, at current time Sat Nov  9 16:21:34 2024.\n",
      "Through 1200 of 2387 moves. In 110.33 minutes, at current time Sat Nov  9 16:21:36 2024.\n",
      "Through 1225 of 2387 moves. In 110.42 minutes, at current time Sat Nov  9 16:21:42 2024.\n",
      "Through 1250 of 2387 moves. In 110.46 minutes, at current time Sat Nov  9 16:21:44 2024.\n",
      "Through 1275 of 2387 moves. In 110.51 minutes, at current time Sat Nov  9 16:21:47 2024.\n",
      "Through 1300 of 2387 moves. In 110.58 minutes, at current time Sat Nov  9 16:21:51 2024.\n",
      "Through 1325 of 2387 moves. In 110.66 minutes, at current time Sat Nov  9 16:21:56 2024.\n",
      "Through 1350 of 2387 moves. In 110.71 minutes, at current time Sat Nov  9 16:21:59 2024.\n",
      "Through 1375 of 2387 moves. In 110.79 minutes, at current time Sat Nov  9 16:22:04 2024.\n",
      "Through 1400 of 2387 moves. In 110.94 minutes, at current time Sat Nov  9 16:22:13 2024.\n",
      "Through 1425 of 2387 moves. In 111.03 minutes, at current time Sat Nov  9 16:22:18 2024.\n",
      "Through 1450 of 2387 moves. In 111.13 minutes, at current time Sat Nov  9 16:22:24 2024.\n",
      "Through 1475 of 2387 moves. In 112.26 minutes, at current time Sat Nov  9 16:23:32 2024.\n",
      "Through 1500 of 2387 moves. In 112.82 minutes, at current time Sat Nov  9 16:24:06 2024.\n",
      "Through 1525 of 2387 moves. In 113.32 minutes, at current time Sat Nov  9 16:24:36 2024.\n",
      "Through 1550 of 2387 moves. In 113.49 minutes, at current time Sat Nov  9 16:24:46 2024.\n",
      "Through 1575 of 2387 moves. In 113.85 minutes, at current time Sat Nov  9 16:25:07 2024.\n",
      "Through 1600 of 2387 moves. In 114.02 minutes, at current time Sat Nov  9 16:25:18 2024.\n",
      "Through 1625 of 2387 moves. In 114.16 minutes, at current time Sat Nov  9 16:25:26 2024.\n",
      "Through 1650 of 2387 moves. In 114.35 minutes, at current time Sat Nov  9 16:25:38 2024.\n",
      "Through 1675 of 2387 moves. In 114.51 minutes, at current time Sat Nov  9 16:25:47 2024.\n",
      "Through 1700 of 2387 moves. In 114.69 minutes, at current time Sat Nov  9 16:25:58 2024.\n",
      "Through 1725 of 2387 moves. In 114.86 minutes, at current time Sat Nov  9 16:26:08 2024.\n",
      "Through 1750 of 2387 moves. In 115.29 minutes, at current time Sat Nov  9 16:26:34 2024.\n",
      "Through 1775 of 2387 moves. In 115.83 minutes, at current time Sat Nov  9 16:27:06 2024.\n",
      "Through 1800 of 2387 moves. In 115.97 minutes, at current time Sat Nov  9 16:27:15 2024.\n",
      "Through 1825 of 2387 moves. In 116.17 minutes, at current time Sat Nov  9 16:27:27 2024.\n",
      "Through 1850 of 2387 moves. In 116.59 minutes, at current time Sat Nov  9 16:27:52 2024.\n",
      "Through 1875 of 2387 moves. In 116.85 minutes, at current time Sat Nov  9 16:28:07 2024.\n",
      "Through 1900 of 2387 moves. In 117.06 minutes, at current time Sat Nov  9 16:28:20 2024.\n",
      "Through 1925 of 2387 moves. In 117.16 minutes, at current time Sat Nov  9 16:28:26 2024.\n",
      "Through 1950 of 2387 moves. In 117.27 minutes, at current time Sat Nov  9 16:28:33 2024.\n",
      "Through 1975 of 2387 moves. In 117.33 minutes, at current time Sat Nov  9 16:28:36 2024.\n",
      "Through 2000 of 2387 moves. In 117.38 minutes, at current time Sat Nov  9 16:28:39 2024.\n",
      "Through 2025 of 2387 moves. In 117.42 minutes, at current time Sat Nov  9 16:28:42 2024.\n",
      "Through 2050 of 2387 moves. In 117.47 minutes, at current time Sat Nov  9 16:28:45 2024.\n",
      "Through 2075 of 2387 moves. In 117.53 minutes, at current time Sat Nov  9 16:28:49 2024.\n",
      "Through 2100 of 2387 moves. In 117.58 minutes, at current time Sat Nov  9 16:28:51 2024.\n",
      "Through 2125 of 2387 moves. In 119.81 minutes, at current time Sat Nov  9 16:31:05 2024.\n",
      "Through 2150 of 2387 moves. In 120.05 minutes, at current time Sat Nov  9 16:31:20 2024.\n",
      "Through 2175 of 2387 moves. In 120.15 minutes, at current time Sat Nov  9 16:31:26 2024.\n",
      "Through 2200 of 2387 moves. In 120.22 minutes, at current time Sat Nov  9 16:31:30 2024.\n",
      "Through 2225 of 2387 moves. In 120.26 minutes, at current time Sat Nov  9 16:31:32 2024.\n",
      "Through 2250 of 2387 moves. In 123.79 minutes, at current time Sat Nov  9 16:35:04 2024.\n",
      "Through 2275 of 2387 moves. In 124.68 minutes, at current time Sat Nov  9 16:35:58 2024.\n"
     ]
    }
   ],
   "source": [
    "if RunTwice:\n",
    "    LoopsToGetAll = int(len(files)/FilesPerRun)-LastSuccesfull\n",
    "    print(f'Additional Loops Needed: {LoopsToGetAll}, at current time {ct(ti())}')\n",
    "    for j in range(LoopsToGetAll):\n",
    "        j+=LastSuccesfull\n",
    "        Mats=[]\n",
    "        if RunParallel ==1:\n",
    "            AllDatas = Parallel(n_jobs=ConcurrentFiles,timeout=1800)(delayed(runWrapper)(files[(j*FilesPerRun+i)], False, False, 0, ti()) for i in range(FilesPerRun))\n",
    "        else:\n",
    "            AllDatas = []\n",
    "            for i in range(FilesPerRun):\n",
    "                FileIndex = int(j*FilesPerRun+i)\n",
    "                AllDatas.append(runWrapper(files[FileIndex], False, False, 0, ti()))\n",
    "                print(f'Got data on {i} of {FilesPerRun} in {int((ti()-toc)/.6)/100} minutes, at current time {ct(ti())}.')\n",
    "        \n",
    "        for fileResponse in AllDatas:\n",
    "            for Mat in fileResponse:\n",
    "                Mats.append(Mat)\n",
    "        \n",
    "        if MemoryProtection:\n",
    "            del AllDatas\n",
    "            print('RAM after AllData:', psutil.virtual_memory()[2])        \n",
    "        lengths = []\n",
    "        rejects = []\n",
    "        Keeps = []\n",
    "        \n",
    "        for Mat in Mats:\n",
    "            spm = np.shape(Mat)\n",
    "            if len(spm) > 1:\n",
    "                lenM = spm[1]\n",
    "            else:\n",
    "                lenM = 1\n",
    "            if (lenM > 1250):\n",
    "                lengths.append(lenM)\n",
    "                Keeps.append(Mat)\n",
    "            else:\n",
    "                rejects.append(lenM)\n",
    "        \n",
    "        if MemoryProtection:\n",
    "            del Mats\n",
    "        \n",
    "        Prints = []\n",
    "        \n",
    "        \n",
    "        if RunParallel ==1:\n",
    "            AllPrints = Parallel(n_jobs=ConcurrentFiles)(delayed(PrintWrap)(Mat) for Mat in Keeps)\n",
    "        else:\n",
    "            AllPrints = []\n",
    "            for i, Mat in enumerate(Keeps):\n",
    "                AllPrints.append(PrintWrap(Mat))\n",
    "                if i % 25 == 0:\n",
    "                    print(f'Through {i} of {len(Keeps)} moves. In {int((ti()-toc)/.6)/100} minutes, at current time {ct(ti())}.')\n",
    "        \n",
    "        if MemoryProtection:\n",
    "            del Keeps\n",
    "            print('RAM after Keeps:', psutil.virtual_memory()[2])\n",
    "        for group in AllPrints:\n",
    "            for fprint in group:\n",
    "                Prints.append(fprint[:, ::2, :])\n",
    "        \n",
    "        if MemoryProtection:\n",
    "            del AllPrints\n",
    "        \n",
    "        random.shuffle(Prints)\n",
    "        \n",
    "        for i, image in enumerate(Prints):\n",
    "            if not isinstance(image, np.ndarray):\n",
    "                Prints[i] = np.array(image, dtype=np.float32)\n",
    "            elif image.dtype != np.float32:\n",
    "                Prints[i] = image.astype(np.float32)\n",
    "        \n",
    "        # Stack the images into a single NumPy array\n",
    "        prints_array = np.stack(Prints, axis=0)\n",
    "        \n",
    "        if MemoryProtection:\n",
    "            del Prints\n",
    "            print('RAM after Prints:', psutil.virtual_memory()[2])\n",
    "        # Convert the NumPy array to a TensorFlow tensor\n",
    "\n",
    "        if psutil.virtual_memory()[2] > 50:\n",
    "            trX = tf.convert_to_tensor(prints_array[:int((psutil.virtual_memory()[2]-50)/50)])\n",
    "        else:\n",
    "            trX = tf.convert_to_tensor(prints_array)\n",
    "        if MemoryProtection:\n",
    "            del prints_array\n",
    "\n",
    "        if MakeOnesOrZeros ==1:\n",
    "            MoveStation = 'Moving'\n",
    "        elif MakeOnesOrZeros == 0:\n",
    "            MoveStation = 'Stationary'\n",
    "            \n",
    "        with open(DataFolder + f'MLPickles/{MoveStation}Dataset_{str(j).zfill(4)}_{str(trX.shape[0]).zfill(6)}.p', 'wb') as handle:\n",
    "            pickle.dump(trX, handle)\n",
    "\n",
    "        if MemoryProtection:\n",
    "            del trX\n",
    "    \n",
    "        print(f'{j} of {LoopsToGetAll+LastSuccesfull} in {int((ti()-toc)/.6)/100} minutes. Using { psutil.virtual_memory()[2]} of RAM, at current time {ct(ti())}')\n",
    "        #%whos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdceb232-514c-43a1-b449-a0d15bb2f1f8",
   "metadata": {},
   "source": [
    "files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
