{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at accelerometer data \n",
    "\n",
    "Finding Zero velocity times by rail axis acceleration noise levels, making summary statistics for the noise levels across the whole day files.  Spot check graphs to see what works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Header used on the projects\n",
    "\n",
    "#first the major packages used for math and graphing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "import scipy.special as sp\n",
    "\n",
    "#Custome graph format style sheet\n",
    "#plt.style.use('Prospectus.mplstyle')\n",
    "\n",
    "#If being run by a seperate file, use the seperate file's graph format and saving paramaeters\n",
    "#otherwise set what is needed\n",
    "if not 'Saving' in locals():\n",
    "    Saving = False\n",
    "if not 'Titles' in locals():\n",
    "    Titles = True\n",
    "if not 'Ledgends' in locals():\n",
    "    Ledgends = True\n",
    "if not 'FFormat' in locals():\n",
    "    FFormat = '.png'\n",
    "\n",
    "#Standard cycle to make black and white images and dashed and line styles\n",
    "default_cycler = (cycler('color', ['0.00', '0.40', '0.60', '0.70']) + cycler(linestyle=['-', '-', '-', '-']))\n",
    "plt.rc('axes', prop_cycle=default_cycler)\n",
    "my_cmap = plt.get_cmap('gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra Headers:\n",
    "import os as os\n",
    "import pywt as py\n",
    "import statistics as st\n",
    "import os as os\n",
    "import random\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import platform\n",
    "\n",
    "from time import time as ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CoreFunctions as cf\n",
    "from skimage.restoration import denoise_wavelet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing Platform\n",
    "Working is beinging conducted on several computers, and author needs to be able to run code on all without rewriting..  This segment of determines which computer is being used, and sets the directories accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HostName = platform.node()\n",
    "\n",
    "if HostName == \"Server\":\n",
    "    Computer = \"Desktop\"   \n",
    "elif HostName[-6:] == 'wm.edu':\n",
    "    Computer = \"SciClone\"\n",
    "elif HostName == \"SchoolLaptop\":\n",
    "    Computer = \"LinLap\"\n",
    "elif HostName == \"WTC-TAB-512\":\n",
    "    Computer = \"PortLap\"\n",
    "else:\n",
    "    Computer = \"WinLap\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Computer == \"SciClone\":\n",
    "    location = '/sciclone/home20/dchendrickson01/image/'\n",
    "elif Computer == \"WinLap\":\n",
    "    location = 'C:\\\\Data\\\\'\n",
    "elif Computer == \"Desktop\":\n",
    "    location = \"E:\\\\Backups\\\\Dan\\\\CraneData\\\\\"\n",
    "elif Computer == \"LinLap\":\n",
    "    location = '/home/dan/Output/'\n",
    "elif Computer == 'PortLap':\n",
    "    location = 'C:\\\\users\\\\dhendrickson\\\\Desktop\\\\AccelData\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Computer ==  \"SciClone\":\n",
    "    rootfolder = '/sciclone/home/dchendrickson01/'\n",
    "    folder = '/sciclone/scr10/dchendrickson01/Recordings2/'\n",
    "    imageFolder = '/sciclone/scr10/dchendrickson01/Move3Dprint/'\n",
    "elif Computer == \"Desktop\":\n",
    "    rootfolder = location\n",
    "    folder = rootfolder + \"Recordings2\\\\\"\n",
    "elif Computer ==\"WinLap\":\n",
    "    rootfolder = location\n",
    "    folder = rootfolder + \"Recordings2\\\\\"   \n",
    "elif Computer == \"LinLap\":\n",
    "    rootfolder = '/home/dan/Data/'\n",
    "    folder = rootfolder + 'Recordings2/'\n",
    "elif Computer =='PortLap':\n",
    "    rootfolder = location \n",
    "    folder = rootfolder + 'Recordings2\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Saving = False\n",
    "location = folder\n",
    "Titles = True\n",
    "Ledgends = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#small set of files.  Files are 4 days of before and after tamping on 2 cranes\n",
    "files = ['221206 recording1.csv','221207 recording1.csv','221208 recording1.csv','221209 recording1.csv',\n",
    "         '221206 recording2.csv','221207 recording2.csv','221208 recording2.csv','221209 recording2.csv',\n",
    "         '230418 recording1.csv','230419 recording1.csv','230420 recording1.csv','230421 recording1.csv',\n",
    "         '230418 recording2.csv','230419 recording2.csv','230420 recording2.csv','230421 recording2.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Moves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions needed to make this work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RollingStdDev(RawData, SmoothData, RollSize = 25):\n",
    "    StdDevs = []\n",
    "    for i in range(RollSize):\n",
    "        Diffs = RawData[0:i+1]-SmoothData[0:i+1]\n",
    "        Sqs = Diffs * Diffs\n",
    "        Var = sum(Sqs) / (i+1)\n",
    "        StdDev = np.sqrt(Var)\n",
    "        StdDevs.append(StdDev)\n",
    "    for i in range(len(RawData)-RollSize-1):\n",
    "        j = i + RollSize\n",
    "        Diffs = RawData[i:j]-SmoothData[i:j]\n",
    "        Sqs = Diffs * Diffs\n",
    "        Var = sum(Sqs) / RollSize\n",
    "        StdDev = np.sqrt(Var)\n",
    "        StdDevs.append(StdDev)  \n",
    "    \n",
    "    return StdDevs\n",
    "\n",
    "def RollingSum(Data, Length = 100):\n",
    "    RollSumStdDev = []\n",
    "    for i in range(Length):\n",
    "        RollSumStdDev.append(sum(Data[0:i+1]))\n",
    "    for i in range(len(Data) - Length):\n",
    "        RollSumStdDev.append(sum(Data[i:i+Length]))\n",
    "    return RollSumStdDev\n",
    "\n",
    "def SquelchPattern(DataSet, StallRange = 5000, SquelchLevel = 0.0086):\n",
    "    SquelchSignal = []\n",
    "\n",
    "    for i in range(StallRange):\n",
    "        SquelchSignal.append(1)\n",
    "\n",
    "    for i in range(len(DataSet)-2*StallRange):\n",
    "        \n",
    "        if np.average(DataSet[i:i+2*StallRange]) < SquelchLevel:\n",
    "            SquelchSignal.append(0)\n",
    "        else:\n",
    "            SquelchSignal.append(1)\n",
    "\n",
    "    for i in range(StallRange+1):\n",
    "        SquelchSignal.append(1)    \n",
    "    \n",
    "    return SquelchSignal\n",
    "\n",
    "def getVelocity(Acceleration, Timestamps = 0.003, Squelch = [], corrected = 0):\n",
    "    velocity = np.zeros(len(Acceleration))\n",
    "    \n",
    "    if Squelch == []:\n",
    "        Squelch = np.ones(len(Acceleration))\n",
    "    \n",
    "    if len(Timestamps) == 1:\n",
    "        dTime = Timestamps[0]\n",
    "    elif len(Timestamps) == len(Acceleration):\n",
    "        totTime = 0\n",
    "        for i in range(len(Timestamps)-1):\n",
    "            if Timestamps[i]<Timestamps[i+1]:\n",
    "                totTime += (Timestamps[i+1] - Timestamps[i])\n",
    "            else:\n",
    "                totTime += (Timestamps[i+1] - Timestamps[i] + 10000)\n",
    "        dTime = totTime / len(Timestamps)\n",
    "    else:\n",
    "        print('error')\n",
    "\n",
    "    dTime = dTime / 10000.0\n",
    "\n",
    "    velocity[0] = Acceleration[0] * (dTime)\n",
    "    \n",
    "    MoveStart = 0\n",
    "    for i in range(len(Acceleration)-1):\n",
    "        j = i + 1\n",
    "        if corrected ==2:\n",
    "            if Squelch[j]==0:\n",
    "                velocity[j]=0\n",
    "                MoveStart = j\n",
    "            else:\n",
    "                velocity[j] = velocity[i] + Acceleration[j] * dTime \n",
    "                try:\n",
    "                    if Squelch[j+1]  == 0:\n",
    "                        PointVairance = velocity[j] / (j - MoveStart)\n",
    "                        for k in range(j-MoveStart):\n",
    "                            velocity[k+MoveStart] -=  PointVairance * k\n",
    "                except:\n",
    "                    pass\n",
    "        else:\n",
    "            velocity[j] = velocity[i] + Acceleration[j] * dTime\n",
    "\n",
    "    if corrected == 1:\n",
    "        PointVairance = velocity[-1:] / len(velocity)\n",
    "        for i in range(len(velocity)):\n",
    "            velocity[i] -=  PointVairance * i\n",
    "    \n",
    "    velocity *= 9.81\n",
    "\n",
    "    return velocity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smooth = cf.Smoothing(ODataSet[:,3],2) #,50)\n",
    "def DeviationVelocity(file):\n",
    "    if file[-3:] =='csv':\n",
    "        #try: \n",
    "        ODataSet = np.genfromtxt(open(folder+file,'r'), delimiter=',',skip_header=0,missing_values=0,invalid_raise=False)\n",
    "        SmoothX = denoise_wavelet(ODataSet[:,3], method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        SmoothY = denoise_wavelet(ODataSet[:,4], method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        SmoothZ = denoise_wavelet(ODataSet[:,5], method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        StdDevsX = RollingStdDev(ODataSet[:,3],SmoothX)\n",
    "        StdDevsY = RollingStdDev(ODataSet[:,4],SmoothY)\n",
    "        StdDevsZ = RollingStdDev(ODataSet[:,5],SmoothZ)\n",
    "        StdDevsX.append(0)\n",
    "        StdDevsY.append(0)\n",
    "        StdDevsZ.append(0)\n",
    "        StdDevsX = np.asarray(StdDevsX)\n",
    "        StdDevsY = np.asarray(StdDevsY)\n",
    "        StdDevsZ = np.asarray(StdDevsZ)\n",
    "        SmoothDevX = denoise_wavelet(StdDevsX, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        SmoothDevY = denoise_wavelet(StdDevsY, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        SmoothDevZ = denoise_wavelet(StdDevsZ, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        #RollSumStdDevX = RollingSum(SmoothDevX)\n",
    "        #RollSumStdDevX = np.asarray(RollSumStdDevX)\n",
    "        #RollSumStdDevY = RollingSum(SmoothDevY)\n",
    "        #RollSumStdDevY = np.asarray(RollSumStdDevY)\n",
    "        #RollSumStdDevZ = RollingSum(SmoothDevZ)\n",
    "        #RollSumStdDevZ = np.asarray(RollSumStdDevZ)\n",
    "        Max = np.max(SmoothDevX)\n",
    "        buckets = int(Max / 0.01) + 1\n",
    "        bins = np.linspace(0,buckets*0.01,buckets+1)\n",
    "        counts, bins = np.histogram(SmoothDevX,bins=bins)\n",
    "        SquelchLevel = bins[np.argmax(counts)+1] \n",
    "        SquelchSignal = SquelchPattern(SmoothDevX, 5000, SquelchLevel)\n",
    "        SmoothX -= np.average(SmoothX)\n",
    "        Velocity = getVelocity(SmoothX, ODataSet[:,2],SquelchSignal, 2)\n",
    "        Velocity = np.asarray(Velocity)\n",
    "\n",
    "        '''except:\n",
    "            Velocity = file\n",
    "            StdDevsX = 0\n",
    "            SmoothDevX = 0\n",
    "            StdDevsY = 0\n",
    "            SmoothDevY = 0\n",
    "            StdDevsZ = 0\n",
    "            SmoothDevZ = 0\n",
    "            SquelchSignal=0\n",
    "            SmoothX=0\n",
    "            ODataSet=np.zeros((5,5))\n",
    "            print(file)\n",
    "        '''\n",
    "        return [Velocity, [SmoothDevX, SmoothDevY, SmoothDevZ], [StdDevsX,StdDevsY,StdDevsZ], SquelchSignal,[SmoothX, SmoothY, SmoothZ],ODataSet[:,3],ODataSet[:,1]]\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeviationVelocity(file):\n",
    "    if file[-3:] =='csv':\n",
    "        ODataSet = np.genfromtxt(open(folder+file,'r'), delimiter=',',skip_header=0,missing_values=0,invalid_raise=False)\n",
    "        SmoothX = denoise_wavelet(ODataSet[:,3], method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        SmoothY = denoise_wavelet(ODataSet[:,4], method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        SmoothZ = denoise_wavelet(ODataSet[:,5], method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        SmoothX -= np.average(SmoothX)\n",
    "        SmoothY -= np.average(SmoothY)\n",
    "        SmoothZ -= np.average(SmoothZ)\n",
    "        StdDevsX = RollingStdDev(ODataSet[:,3],SmoothX)\n",
    "        StdDevsX.append(0)\n",
    "        StdDevsX = np.asarray(StdDevsX)\n",
    "        SmoothDevX = denoise_wavelet(StdDevsX, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        SquelchSignal = SquelchPattern(SmoothDevX, 2000, 0.03)\n",
    "        #Velocity = getVelocity(ODataSet[:,3], ODataSet[:,2],SquelchSignal, 2)\n",
    "        #Velocity = np.asarray(Velocity)\n",
    "        MoveMatrix = np.matrix([SmoothX, SmoothY, SmoothZ])\n",
    "        return [SquelchSignal,MoveMatrix,SmoothDevX,file[:-3]]\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SepreateMovements(SquelchSignal, RawData, FileName):\n",
    "    Moves= []\n",
    "    MoveNames = []\n",
    "    Move = np.zeros((1,3), dtype=float)\n",
    "    i = 0\n",
    "    for j in range(len(SquelchSignal)-1):\n",
    "        if SquelchSignal[j] == 1:\n",
    "            try:\n",
    "                Move = np.concatenate((Move, RawData[j,:]), axis=0)\n",
    "            except:\n",
    "                print(j)\n",
    "            if SquelchSignal[j+1] == 0:\n",
    "                #Move = np.matrix(Move)\n",
    "                Moves.append(Move)\n",
    "                MoveNames.append(FileName + str(i).zfill(3))\n",
    "                i+=1\n",
    "                Move = np.zeros((1,3), dtype=float)\n",
    "                #Move[0,2]=0\n",
    "    Moves.append(Move)\n",
    "    MoveNames.append(FileName + str(i).zfill(3))\n",
    "    return Moves, MoveNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitLong(Moves, maxLength = 4000, minLength = 1000, MoveNames = []):\n",
    "    if len(MoveNames) <=1:\n",
    "        MoveNames = ['null'  for x in range(len(Moves))]\n",
    "    Xmoves = []\n",
    "    Xnames = []\n",
    "    for i in range(len(Moves)):\n",
    "        if np.shape(move)[0] > maxLength: \n",
    "            Xmoves.append(Moves[i][:int(len(Moves[i])/2),:])\n",
    "            Xnames.append(MoveNames[i] + 'a')\n",
    "            Xmoves.append(Moves[i][int(len(Moves[i])/2):,:])\n",
    "            Xnames.append(MoveNames[i] + 'b')\n",
    "        else:\n",
    "            if np.shape(Moves[i])[0] < minLength:\n",
    "                pass\n",
    "            else:\n",
    "                Xmoves.append(Moves[i])\n",
    "                Xnames.append(MoveNames[i])\n",
    "    return Xmoves, Xnames\n",
    "\n",
    "def findMaxLength(Moves):\n",
    "    maxLength = 0\n",
    "    LongMove = 0\n",
    "    for i in range(len(Moves)):\n",
    "        if np.shape(Moves[i])[0] > maxLength: \n",
    "            maxLength =  np.shape(Moves[i])[0]\n",
    "            LongMove = i\n",
    "    return maxLength, LongMove\n",
    "\n",
    "def findMinLength(Moves):\n",
    "    minLength = 9999999\n",
    "    SmallMove = 0\n",
    "    for i in range(len(Moves)):\n",
    "        if np.shape(Moves[i])[0] < minLength: \n",
    "            minLength =  np.shape(Moves[i])[0]\n",
    "            SmallMove = i\n",
    "    return minLength, SmallMove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building up the list of movements with their names kept handy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LoopFiles = 3\n",
    "loops = int(len(files) / LoopFiles) \n",
    "if len(files)%LoopFiles != 0:\n",
    "    loops += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SquelchSignal = []\n",
    "RawData=[]\n",
    "OrderedFileNames=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = ti()\n",
    "\n",
    "for k in range(loops):\n",
    "    if k == loops -1:\n",
    "        tfiles = files[k*LoopFiles:]\n",
    "    else:\n",
    "        tfiles = files[k*LoopFiles:(k+1)*LoopFiles]\n",
    "    Results = Parallel(n_jobs=LoopFiles)(delayed(DeviationVelocity)(file) for file in tfiles)\n",
    "    \n",
    "    for i in range(len(Results)):       \n",
    "        SquelchSignal.append(Results[i][0])\n",
    "        RawData.append(np.matrix(Results[i][1]).T)\n",
    "        OrderedFileNames.append(Results[i][3])\n",
    "    print(k, np.shape(Results), (ti()-st)/60.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MoveData = Parallel(n_jobs=31)(delayed(SepreateMovements)(SquelchSignal[i], RawData[i], OrderedFileNames[i])\n",
    "                                       for i in range(len(RawData)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Movements = []\n",
    "GroupNames = []\n",
    "for move in MoveData:\n",
    "    Movements.append(move[0])\n",
    "    GroupNames.append(move[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Moves=[]\n",
    "for Groups in Movements:\n",
    "    for Move in Groups:\n",
    "        Moves.append(Move)\n",
    "\n",
    "MoveNames = []\n",
    "for Groups in GroupNames:\n",
    "    for name in Groups:\n",
    "        MoveNames.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "del SquelchSignal\n",
    "del RawData\n",
    "del Movements\n",
    "del GroupNames\n",
    "del MoveData\n",
    "del OrderedFileNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longMove, MoveNumb = findMaxLength(Moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Moves, MoveNames = splitLong(Moves, longMove+1, minLength, MoveNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Self Supervised\n",
    "#https://medium.com/@jetnew/anomaly-detection-of-time-series-data-e0cb6b382e33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_data, Test_data, Train_names, Test_Names = train_test_split(Moves, MoveNames, test_size=0.10, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Dense, RepeatVector, TimeDistributed\n",
    "from keras.models import Sequential\n",
    "\n",
    "class LSTM_Autoencoder:\n",
    "  def __init__(self, optimizer='adam', loss='mse'):\n",
    "    self.optimizer = optimizer\n",
    "    self.loss = loss\n",
    "    self.n_features = 3\n",
    "    \n",
    "  def build_model(self):\n",
    "    timesteps = self.timesteps\n",
    "    n_features = self.n_features\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Encoder\n",
    "    model.add(LSTM(timesteps, activation='relu', input_shape=(None, n_features), return_sequences=True))\n",
    "    model.add(LSTM(16, activation='relu', return_sequences=True))\n",
    "    model.add(LSTM(1, activation='relu'))\n",
    "    model.add(RepeatVector(timesteps))\n",
    "    \n",
    "    # Decoder\n",
    "    model.add(LSTM(timesteps, activation='relu', return_sequences=True))\n",
    "    model.add(LSTM(16, activation='relu', return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(n_features)))\n",
    "    \n",
    "    model.compile(optimizer=self.optimizer, loss=self.loss)\n",
    "    model.summary()\n",
    "    self.model = model\n",
    "    \n",
    "  def fit(self, X, epochs=3, batch_size=32):\n",
    "    self.timesteps = np.shape(X)[1]\n",
    "    self.build_model()\n",
    "    \n",
    "    input_X = np.expand_dims(X, axis=2)\n",
    "    self.model.fit(input_X, input_X, epochs=epochs, batch_size=batch_size)\n",
    "    \n",
    "  def predict(self, X):\n",
    "    input_X = np.expand_dims(X, axis=2)\n",
    "    output_X = self.model.predict(input_X)\n",
    "    reconstruction = np.squeeze(output_X)\n",
    "    return np.linalg.norm(X - reconstruction, axis=-1)\n",
    "  \n",
    "  def plot(self, scores, timeseries, threshold=0.95):\n",
    "    sorted_scores = sorted(scores)\n",
    "    threshold_score = sorted_scores[round(len(scores) * threshold)]\n",
    "    \n",
    "    plt.title(\"Reconstruction Error\")\n",
    "    plt.plot(scores)\n",
    "    plt.plot([threshold_score]*len(scores), c='r')\n",
    "    plt.show()\n",
    "    \n",
    "    anomalous = np.where(scores > threshold_score)\n",
    "    normal = np.where(scores <= threshold_score)\n",
    "    \n",
    "    plt.title(\"Anomalies\")\n",
    "    plt.scatter(normal, timeseries[normal][:,-1], s=3)\n",
    "    plt.scatter(anomalous, timeseries[anomalous][:,-1], s=5, c='r')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_autoencoder = LSTM_Autoencoder(optimizer='adam', loss='mse')\n",
    "lstm_autoencoder.fit(Train_data, epochs=3, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = lstm_autoencoder.predict(Test_data)\n",
    "lstm_autoencoder.plot(scores, Test_data, threshold=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lstm_autoencoder.model.save(\"LSTM_FP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give error and stop code on run all\n",
    "adsfasdfasdfasdfasdfasdfasdfasdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iForest \n",
    "Requires data in Pandas data frames\n",
    "\n",
    "https://towardsdatascience.com/unsupervised-anomaly-detection-in-python-f2e61be17c2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.anomaly import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Move_dict = dict(Xmoves)\n",
    "df_Move = pd.DataFrame.from_dict(Move_dict, oreint='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_Move[:-Split]\n",
    "df_unseen = df_Move[-Split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom = setup(data = df_train, silent = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_model = create_model(model = 'iforest', fraction = 0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = assign_model(anom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(anom_model, plot = 'tsne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(anom_model, plot = 'umap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_model.predict(df_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_model.predict_proba(df_unseen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_model.decision_function(df_unseen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heirarchal clustering\n",
    "https://medium.com/@jetnew/anomaly-detection-of-time-series-data-e0cb6b382e33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "clusters = 3\n",
    "y_pred = AgglomerativeClustering(n_clusters=clusters).fit_predict(Train_data)\n",
    "\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "\n",
    "clusters=5\n",
    "cls = linkage(Train_data, method='ward')\n",
    "y_pred = fcluster(cls, t=clusters, criterion='maxclust')\n",
    "\n",
    "dendrogram(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Techniques\n",
    "https://www.kaggle.com/code/victorambonati/unsupervised-anomaly-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "#%matplotlib notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import seaborn\n",
    "import matplotlib.dates as md\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "#from pyemma import msm # not available on Kaggle Kernel\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some function for later\n",
    "\n",
    "# return Series of distance between each point and his distance with the closest centroid\n",
    "def getDistanceByPoint(data, model):\n",
    "    distance = pd.Series()\n",
    "    for i in range(0,len(data)):\n",
    "        Xa = np.array(data.loc[i])\n",
    "        Xb = model.cluster_centers_[model.labels_[i]-1]\n",
    "        distance.set_value(i, np.linalg.norm(Xa-Xb))\n",
    "    return distance\n",
    "\n",
    "# train markov model to get transition matrix\n",
    "def getTransitionMatrix (df):\n",
    "    df = np.array(df)\n",
    "    model = msm.estimate_markov_model(df, 1)\n",
    "    return model.transition_matrix\n",
    "\n",
    "def markovAnomaly(df, windows_size, threshold):\n",
    "    transition_matrix = getTransitionMatrix(df)\n",
    "    real_threshold = threshold**windows_size\n",
    "    df_anomaly = []\n",
    "    for j in range(0, len(df)):\n",
    "        if (j < windows_size):\n",
    "            df_anomaly.append(0)\n",
    "        else:\n",
    "            sequence = df[j-windows_size:j]\n",
    "            sequence = sequence.reset_index(drop=True)\n",
    "            df_anomaly.append(anomalyElement(sequence, real_threshold, transition_matrix))\n",
    "    return df_anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In 13\n",
    "# calculate with different number of centroids to see the loss plot (elbow method)\n",
    "n_cluster = range(1, 20)\n",
    "kmeans = [KMeans(n_clusters=i).fit(Train_data) for i in n_cluster]\n",
    "scores = [kmeans[i].score(Train_data) for i in range(len(kmeans))]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(n_cluster, scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not clear for me, I choose 15 centroids arbitrarily and add these data to the central dataframe\n",
    "df['cluster'] = kmeans[14].predict(Train_data)\n",
    "df['principal_feature1'] = Train_data[0]\n",
    "df['principal_feature2'] = Train_data[1]\n",
    "df['cluster'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Simple",
   "language": "python",
   "name": "simple"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "178f6c3502586c94dc93af50f98dbd15c5205250cbf2345a6eb57380f8c77d96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
