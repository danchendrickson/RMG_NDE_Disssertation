{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/bin/python\n",
    "\n",
    "#\n",
    "#  Works in CPU Mode not GPU\n",
    "#\n",
    "\n",
    "#Standard Header used on the projects\n",
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "#first the major packages used for math and graphing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "\n",
    "import os as os\n",
    "import random\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import pywt\n",
    "from pywt._extensions._pywt import (DiscreteContinuousWavelet, ContinuousWavelet,\n",
    "                                Wavelet, _check_dtype)\n",
    "from pywt._functions import integrate_wavelet, scale2frequency\n",
    "from time import time as ti\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential, Model \n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import  EarlyStopping\n",
    " \n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import tensorflow.keras_metrics as km\n",
    "  \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "import platform\n",
    "\n",
    "HostName = platform.node()\n",
    "\n",
    "if HostName == \"Server\":\n",
    "    Computer = \"Desktop\"   \n",
    "elif HostName[-6:] == 'wm.edu':\n",
    "    Computer = \"SciClone\"\n",
    "elif HostName == \"SchoolLaptop\":\n",
    "    Computer = \"LinLap\"\n",
    "elif HostName == \"WTC-TAB-512\":\n",
    "    Computer = \"PortLap\"\n",
    "else:\n",
    "    Computer = \"WinLap\"\n",
    "\n",
    "if Computer == \"SciClone\":\n",
    "    location = '/sciclone/home20/dchendrickson01/'\n",
    "elif Computer == \"WinLap\":\n",
    "    location = 'C:\\\\Data\\\\'\n",
    "elif Computer == \"Desktop\":\n",
    "    location = \"E:\\\\Backups\\\\Dan\\\\CraneData\\\\\"\n",
    "elif Computer == \"LinLap\":\n",
    "    location = '/home/dan/Output/'\n",
    "    \n",
    "\n",
    "if Computer ==  \"SciClone\":\n",
    "    rootfolder = '/sciclone/home20/dchendrickson01/'\n",
    "    folder = '/sciclone/data10/dchendrickson01/SmallCopy/'\n",
    "elif Computer == \"Desktop\":\n",
    "    rootfolder = location\n",
    "    folder = rootfolder + \"SmallCopy\\\\\"\n",
    "elif Computer ==\"WinLap\":\n",
    "    rootfolder = location\n",
    "    folder = rootfolder + \"SmallCopy\\\\\"   \n",
    "elif Computer == \"LinLap\":\n",
    "    rootfolder = '/home/dan/Data/'\n",
    "    folder = rootfolder + 'SmallCopy/'\n",
    "    \n",
    "\n",
    "scales = 500\n",
    "#img_height , img_width = scales, 200\n",
    "DoSomeFiles = True\n",
    "NumberOfFiles = 15\n",
    "SmoothType = 3  # 0 = none, 1 = rolling average, 2 = rolling StdDev\n",
    "TrainEpochs = 3\n",
    "WaveletToUse = 'beta'\n",
    "\n",
    "num_cores = multiprocessing.cpu_count() -1\n",
    "SensorPositonFile = rootfolder + 'SensorStatsSmall.csv'\n",
    "\n",
    "if Computer == \"SciClone\" or Computer == \"LinLap\":\n",
    "    SaveModelFolder = rootfolder + 'SavedModel/'\n",
    "else:\n",
    "    SaveModelFolder = rootfolder + 'SavedModel\\\\'\n",
    "\n",
    "files = os.listdir(folder)\n",
    "if DoSomeFiles: files = random.sample(files,NumberOfFiles)\n",
    "\n",
    "OutputVectors = np.genfromtxt(open(SensorPositonFile,'r'), delimiter=',',skip_header=1,dtype=int, missing_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CoreFunctions as cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1970: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  result = asarray(a).shape\n"
     ]
    }
   ],
   "source": [
    "\n",
    "AllAccels = Parallel(n_jobs=num_cores)(delayed(cf.getAcceleration)(file) for file in files)\n",
    "Flattened = []\n",
    "for j in range(np.shape(AllAccels)[0]):\n",
    "    if AllAccels[j][0] == False:\n",
    "        print(j,AllAccels[j][1])\n",
    "    else: \n",
    "        Flattened.append(AllAccels[j])\n",
    "print('Have Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MetaData = []  #np.asarray([],dtype=object)\n",
    "DataOnlyMatrix = np.asarray([],dtype=object)\n",
    "for j in range(np.shape(AllAccels)[0]):\n",
    "    if AllAccels[j][0] == False :\n",
    "        if AllAccels[j][1][4:9] =='Accel':\n",
    "            print(j,AllAccels[j][1])\n",
    "    else: \n",
    "        for k in range(3):\n",
    "            MetaData.append([AllAccels[j][k][0], AllAccels[j][k][1], AllAccels[j][k][3], AllAccels[j][k][4]])\n",
    "            if np.size(DataOnlyMatrix) == 0:\n",
    "                    DataOnlyMatrix =np.matrix(AllAccels[j][k][2])\n",
    "            else:\n",
    "                    DataOnlyMatrix = np.concatenate((DataOnlyMatrix,np.matrix(AllAccels[j][k][2])),axis=0)\n",
    "\n",
    "MetaData = np.matrix(MetaData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resizeImage(FP):\n",
    "    res = cv2.resize(FP, dsize=(int(np.shape(FP)[0]/2), int(np.shape(FP)[1]/6)), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllAccels = cf.KalmanGroup(DataOnlyMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxes = np.amax(AllAccels[:,500:], axis = 1)\n",
    "mins = np.amin(AllAccels[:,500:], axis = 1)\n",
    "\n",
    "Keep = np.zeros(mins.size)\n",
    "for i in range(mins.size):\n",
    "    if i % 3 == 0:\n",
    "        if maxes[i] > 0.01 and mins[i] < -0.01:\n",
    "            Keep[i]=1\n",
    "            Keep[i+1]=1\n",
    "            Keep[i+2]=1\n",
    "            #print(i)\n",
    "            \n",
    "\n",
    "Keep = np.array(Keep, dtype='bool')\n",
    "            \n",
    "AllAccels = AllAccels[Keep,:]\n",
    "MetaData = MetaData[Keep,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MotionsLeft = int(np.shape(AllAccels)[0]/3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have fingerprints\n"
     ]
    }
   ],
   "source": [
    "AllFingers =  Parallel(n_jobs=num_cores)(delayed(cf.makeMatrixImages)([AllAccels[i*3],AllAccels[i*3+1],AllAccels[i*3+2]]) for i in range(MotionsLeft))\n",
    "\n",
    "print('Have fingerprints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataSet = Parallel(n_jobs=num_cores)(delayed(ParseData)(file[0]) for file in AllFingers)\n",
    "#DataSet = np.asarray(DataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error  2 60kPoints-220425-0211-s4.csv\n",
      "Found Two  60kPoints-220425-0211-s4.csv\n",
      "Data Parsed\n"
     ]
    }
   ],
   "source": [
    "ResultsSet = []\n",
    "for i in range(MotionsLeft):\n",
    "    ResultsSet.append(np.asarray(cf.truthVector(MetaData[i*3,3])).flatten())\n",
    "\n",
    "print('Data Parsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SmallFingers =  Parallel(n_jobs=num_cores)(delayed(resizeImage)(FP) for FP in AllFingers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(SmallFingers, ResultsSet, test_size=0.20, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width = np.shape(X_train)[1]\n",
    "img_height = np.shape(X_train)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 250\n"
     ]
    }
   ],
   "source": [
    "print(img_width, img_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_1 (Rescaling)     (None, 10000, 250, 3)     0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 10000, 250, 16)    448       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 5000, 125, 16)    0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 5000, 125, 32)     4640      \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 2500, 62, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 2500, 62, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 1250, 31, 64)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2480000)           0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               317440128 \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 317,464,228\n",
      "Trainable params: 317,464,228\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Rescaling(1./255, input_shape=(img_width, img_height, 3)),\n",
    "    layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(np.shape(y_train)[1])\n",
    "    ])\n",
    " \n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])\n",
    " \n",
    "earlystop = EarlyStopping(patience=7)   \n",
    "callbacks = [earlystop]\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.matrix(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 4) (7, 10000, 250, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(y_train), np.shape(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1/1 [==============================] - 5s 5s/step - loss: 7.1760 - accuracy: 0.2000 - val_loss: 1.4753 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6675 - accuracy: 0.4000 - val_loss: 8.2777 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.4217 - accuracy: 0.4000 - val_loss: 8.0590 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x = X_train, y = y_train, epochs=3, batch_size = 8 , shuffle=False, validation_split=0.25, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(SaveModelFolder)\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig(rootfolder + 'ModelAccuracy.png')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig(rootfolder + 'ModelLoss.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "11c16a051206f53cf7fe024f12cacb318023d916d0a5509b7bf3391ee4b4163a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
