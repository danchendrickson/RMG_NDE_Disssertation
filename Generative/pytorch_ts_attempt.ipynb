{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e878feb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Header used on the projects\n",
    "\n",
    "#first the major packages used for math and graphing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "import scipy.special as sp\n",
    "\n",
    "#Custome graph format style sheet\n",
    "#plt.style.use('Prospectus.mplstyle')\n",
    "\n",
    "#If being run by a seperate file, use the seperate file's graph format and saving paramaeters\n",
    "#otherwise set what is needed\n",
    "if not 'Saving' in locals():\n",
    "    Saving = False\n",
    "if not 'Titles' in locals():\n",
    "    Titles = True\n",
    "if not 'Ledgends' in locals():\n",
    "    Ledgends = True\n",
    "if not 'FFormat' in locals():\n",
    "    FFormat = '.png'\n",
    "\n",
    "#Standard cycle to make black and white images and dashed and line styles\n",
    "default_cycler = (cycler('color', ['0.00', '0.40', '0.60', '0.70']) + cycler(linestyle=['-', '-', '-', '-']))\n",
    "plt.rc('axes', prop_cycle=default_cycler)\n",
    "my_cmap = plt.get_cmap('gray')\n",
    "\n",
    "#Extra Headers:\n",
    "import os as os\n",
    "import pywt as py\n",
    "import statistics as st\n",
    "import os as os\n",
    "import random\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import platform\n",
    "\n",
    "from time import time as ti\n",
    "\n",
    "#import CoreFunctions as cf\n",
    "#from skimage.restoration import denoise_wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a3d9f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sciclone/home/dchendrickson01/.conda/envs/torchmpi/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from gluonts.dataset.multivariate_grouper import MultivariateGrouper\n",
    "from gluonts.dataset.repository.datasets import dataset_recipes, get_dataset\n",
    "from pts.model.tempflow import TempFlowEstimator\n",
    "from pts.model.transformer_tempflow import TransformerTempFlowEstimator\n",
    "from pts import Trainer\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.evaluation import MultivariateEvaluator\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c039616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "HostName = platform.node()\n",
    "\n",
    "if HostName == \"Server\":\n",
    "    Computer = \"Desktop\"   \n",
    "elif HostName[-6:] == 'wm.edu':\n",
    "    Computer = \"SciClone\"\n",
    "elif HostName == \"SchoolLaptop\":\n",
    "    Computer = \"LinLap\"\n",
    "elif HostName == \"WTC-TAB-512\":\n",
    "    Computer = \"PortLap\"\n",
    "else:\n",
    "    Computer = \"WinLap\"\n",
    "\n",
    "if Computer == \"SciClone\":\n",
    "    location = '/sciclone/home20/dchendrickson01/image/'\n",
    "elif Computer == \"WinLap\":\n",
    "    location = 'C:\\\\Data\\\\'\n",
    "elif Computer == \"Desktop\":\n",
    "    location = \"E:\\\\Backups\\\\Dan\\\\CraneData\\\\\"\n",
    "elif Computer == \"LinLap\":\n",
    "    location = '/home/dan/Output/'\n",
    "elif Computer == 'PortLap':\n",
    "    location = 'C:\\\\users\\\\dhendrickson\\\\Desktop\\\\AccelData\\\\'\n",
    "\n",
    "if Computer ==  \"SciClone\":\n",
    "    rootfolder = '/sciclone/home20/dchendrickson01/'\n",
    "    folder = '/sciclone/scr10/dchendrickson01/Recordings2/'\n",
    "    imageFolder = '/sciclone/scr10/dchendrickson01/Move3Dprint/'\n",
    "elif Computer == \"Desktop\":\n",
    "    rootfolder = location\n",
    "    folder = rootfolder + \"Recordings2\\\\\"\n",
    "elif Computer ==\"WinLap\":\n",
    "    rootfolder = location\n",
    "    folder = rootfolder + \"Recordings2\\\\\"   \n",
    "elif Computer == \"LinLap\":\n",
    "    rootfolder = '/home/dan/Data/'\n",
    "    folder = rootfolder + 'Recordings2/'\n",
    "elif Computer =='PortLap':\n",
    "    rootfolder = location \n",
    "    folder = rootfolder + 'Recordings2\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d4d9a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "Saving = False\n",
    "location = folder\n",
    "Titles = True\n",
    "Ledgends = True\n",
    "\n",
    "f = 0\n",
    "freq = \"2s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7ff1765",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['230418 recording1.csv','230419 recording1.csv']#,'230420 recording1.csv','230421 recording1.csv',\n",
    "         #'230418 recording2.csv','230419 recording2.csv','230420 recording2.csv','230421 recording2.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4916f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeDataframe(file):\n",
    "    dataset = pd.read_table(folder+file, delimiter =\", \", header=None, engine='python')\n",
    "\n",
    "    dataset = dataset.rename(columns={0:\"Day\"})\n",
    "    dataset = dataset.rename(columns={1:\"Second\"})\n",
    "    dataset = dataset.rename(columns={2:\"FracSec\"})\n",
    "    dataset = dataset.rename(columns={3:\"x\"})\n",
    "    dataset = dataset.rename(columns={4:\"y\"})\n",
    "    dataset = dataset.rename(columns={5:\"z\"})\n",
    "    dataset = dataset.rename(columns={6:\"Sensor\"})\n",
    "\n",
    "    dataset[['Day','Second']] = dataset[['Day','Second']].apply(lambda x: x.astype(int).astype(str).str.zfill(6))\n",
    "    dataset[['FracSec']] = dataset[['FracSec']].apply(lambda x: x.astype(int).astype(str).str.zfill(4))\n",
    "\n",
    "    dataset[\"timestamp\"] = pd.to_datetime(dataset.Day+dataset.Second+dataset.FracSec,format='%y%m%d%H%M%S%f')\n",
    "\n",
    "    dataset[\"x\"] = dataset.x - np.average(dataset.x)\n",
    "    dataset[\"y\"] = dataset.y - np.average(dataset.y)\n",
    "    dataset[\"z\"] = dataset.z - np.average(dataset.z)\n",
    "    dataset[\"r\"] = np.sqrt(dataset.x**2 + dataset.y**2 + dataset.z**2)\n",
    "\n",
    "    dataset.index = dataset.timestamp\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf7f7a",
   "metadata": {},
   "source": [
    "df = []\n",
    "for file in files:\n",
    "    tempDF = MakeDataframe(file)\n",
    "    df.append(tempDF)\n",
    "dataset = pd.concat(df)[[\"x\",\"y\",\"z\",\"r\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "904fe039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>r</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-04-18 00:00:00.172800</th>\n",
       "      <td>-0.002305</td>\n",
       "      <td>0.002889</td>\n",
       "      <td>0.004128</td>\n",
       "      <td>0.005541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-18 00:00:00.177600</th>\n",
       "      <td>0.015265</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>-0.016372</td>\n",
       "      <td>0.022385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-18 00:00:00.182300</th>\n",
       "      <td>-0.012805</td>\n",
       "      <td>0.003869</td>\n",
       "      <td>0.024888</td>\n",
       "      <td>0.028255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-18 00:00:00.187000</th>\n",
       "      <td>-0.014025</td>\n",
       "      <td>-0.011261</td>\n",
       "      <td>-0.004172</td>\n",
       "      <td>0.018464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-18 00:00:00.191700</th>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.008269</td>\n",
       "      <td>0.006328</td>\n",
       "      <td>0.011774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   x         y         z         r\n",
       "timestamp                                                         \n",
       "2023-04-18 00:00:00.172800 -0.002305  0.002889  0.004128  0.005541\n",
       "2023-04-18 00:00:00.177600  0.015265  0.000219 -0.016372  0.022385\n",
       "2023-04-18 00:00:00.182300 -0.012805  0.003869  0.024888  0.028255\n",
       "2023-04-18 00:00:00.187000 -0.014025 -0.011261 -0.004172  0.018464\n",
       "2023-04-18 00:00:00.191700  0.005495  0.008269  0.006328  0.011774"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Parallel(n_jobs=8)(delayed(MakeDataframe)(file) for file in files)\n",
    "dataset = pd.concat(data)[[\"x\",\"y\",\"z\",\"r\"]]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60b89fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.dataset.split import OffsetSplitter, DateSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80248d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database is locked')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "splitter = DateSplitter(\n",
    "    date=pd.Period('2023-04-18', freq='500ms'))\n",
    "\n",
    "train, test_template = splitter.split(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e30b6ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestTemplate(dataset=                                   x         y         z         r\n",
       "timestamp                                                         \n",
       "2023-04-18 00:00:00.172800 -0.002305  0.002889  0.004128  0.005541\n",
       "2023-04-18 00:00:00.177600  0.015265  0.000219 -0.016372  0.022385\n",
       "2023-04-18 00:00:00.182300 -0.012805  0.003869  0.024888  0.028255\n",
       "2023-04-18 00:00:00.187000 -0.014025 -0.011261 -0.004172  0.018464\n",
       "2023-04-18 00:00:00.191700  0.005495  0.008269  0.006328  0.011774\n",
       "...                              ...       ...       ...       ...\n",
       "2023-04-19 23:59:59.983200 -0.010660 -0.014843  0.023773  0.029985\n",
       "2023-04-19 23:59:59.987900 -0.010660  0.020057 -0.000147  0.022714\n",
       "2023-04-19 23:59:59.992600 -0.010660 -0.004343 -0.007227  0.013591\n",
       "2023-04-19 23:59:59.997300 -0.003090  0.004187  0.007173  0.008861\n",
       "2023-04-20 00:00:00.002000 -0.001620  0.009317 -0.010647  0.014241\n",
       "\n",
       "[37557076 rows x 4 columns], splitter=DateSplitter(date=Period('2023-04-18 00:00:00.000', '500L')))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddd10026",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_template.generate_instances(\n",
    "    prediction_length=7,\n",
    "    windows=2,\n",
    "    distance=3, # windows are three time steps apart from each other\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6c2458d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestData(dataset=                                   x         y         z         r\n",
       "timestamp                                                         \n",
       "2023-04-18 00:00:00.172800 -0.002305  0.002889  0.004128  0.005541\n",
       "2023-04-18 00:00:00.177600  0.015265  0.000219 -0.016372  0.022385\n",
       "2023-04-18 00:00:00.182300 -0.012805  0.003869  0.024888  0.028255\n",
       "2023-04-18 00:00:00.187000 -0.014025 -0.011261 -0.004172  0.018464\n",
       "2023-04-18 00:00:00.191700  0.005495  0.008269  0.006328  0.011774\n",
       "...                              ...       ...       ...       ...\n",
       "2023-04-19 23:59:59.983200 -0.010660 -0.014843  0.023773  0.029985\n",
       "2023-04-19 23:59:59.987900 -0.010660  0.020057 -0.000147  0.022714\n",
       "2023-04-19 23:59:59.992600 -0.010660 -0.004343 -0.007227  0.013591\n",
       "2023-04-19 23:59:59.997300 -0.003090  0.004187  0.007173  0.008861\n",
       "2023-04-20 00:00:00.002000 -0.001620  0.009317 -0.010647  0.014241\n",
       "\n",
       "[37557076 rows x 4 columns], splitter=DateSplitter(date=Period('2023-04-18 00:00:00.000', '500L')), prediction_length=7, windows=2, distance=3, max_history=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48abb15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MultivariateEvaluator(quantiles=(np.arange(20)/20.0)[1:],\n",
    "                                  target_agg_funcs={'sum': np.sum})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64c69178",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TempFlowEstimator(\n",
    "    target_dim=4,\n",
    "    prediction_length=test_dataset.prediction_length,\n",
    "    cell_type='GRU',\n",
    "    input_size=552,\n",
    "    freq=\"1T\",\n",
    "    scaling=True,\n",
    "    dequantize=True,\n",
    "    n_blocks=4,\n",
    "    trainer=Trainer(device=device,\n",
    "                    epochs=45,\n",
    "                    learning_rate=1e-3,\n",
    "                    num_batches_per_epoch=100,\n",
    "                    batch_size=64)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a1470f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "prefetch_factor option could only be specified in multiprocessing.let num_workers > 0 to enable multiprocessing, otherwise set prefetch_factor to None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predictor \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mtrain(train)\n\u001b[1;32m      2\u001b[0m forecast_it, ts_it \u001b[38;5;241m=\u001b[39m make_evaluation_predictions(dataset\u001b[38;5;241m=\u001b[39mdataset_test,\n\u001b[1;32m      3\u001b[0m                                              predictor\u001b[38;5;241m=\u001b[39mpredictor,\n\u001b[1;32m      4\u001b[0m                                              num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      5\u001b[0m forecasts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(forecast_it)\n",
      "File \u001b[0;32m~/.conda/envs/torchmpi/lib/python3.11/site-packages/pts/model/estimator.py:179\u001b[0m, in \u001b[0;36mPyTorchEstimator.train\u001b[0;34m(self, training_data, validation_data, num_workers, prefetch_factor, shuffle_buffer_length, cache_data, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    171\u001b[0m     training_data: Dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    178\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PyTorchPredictor:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_model(\n\u001b[1;32m    180\u001b[0m         training_data,\n\u001b[1;32m    181\u001b[0m         validation_data,\n\u001b[1;32m    182\u001b[0m         num_workers\u001b[38;5;241m=\u001b[39mnum_workers,\n\u001b[1;32m    183\u001b[0m         prefetch_factor\u001b[38;5;241m=\u001b[39mprefetch_factor,\n\u001b[1;32m    184\u001b[0m         shuffle_buffer_length\u001b[38;5;241m=\u001b[39mshuffle_buffer_length,\n\u001b[1;32m    185\u001b[0m         cache_data\u001b[38;5;241m=\u001b[39mcache_data,\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    187\u001b[0m     )\u001b[38;5;241m.\u001b[39mpredictor\n",
      "File \u001b[0;32m~/.conda/envs/torchmpi/lib/python3.11/site-packages/pts/model/estimator.py:119\u001b[0m, in \u001b[0;36mPyTorchEstimator.train_model\u001b[0;34m(self, training_data, validation_data, num_workers, prefetch_factor, shuffle_buffer_length, cache_data, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     training_instance_splitter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_instance_splitter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    109\u001b[0m training_iter_dataset \u001b[38;5;241m=\u001b[39m TransformedIterableDataset(\n\u001b[1;32m    110\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mtraining_data,\n\u001b[1;32m    111\u001b[0m     transform\u001b[38;5;241m=\u001b[39mtransformation\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m     cache_data\u001b[38;5;241m=\u001b[39mcache_data,\n\u001b[1;32m    117\u001b[0m )\n\u001b[0;32m--> 119\u001b[0m training_data_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m    120\u001b[0m     training_iter_dataset,\n\u001b[1;32m    121\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m    122\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39mnum_workers,\n\u001b[1;32m    123\u001b[0m     prefetch_factor\u001b[38;5;241m=\u001b[39mprefetch_factor,\n\u001b[1;32m    124\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    125\u001b[0m     worker_init_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_init_fn,\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    127\u001b[0m )\n\u001b[1;32m    129\u001b[0m validation_data_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/torchmpi/lib/python3.11/site-packages/torch/utils/data/dataloader.py:245\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout option should be non-negative\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_workers \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m prefetch_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprefetch_factor option could only be specified in multiprocessing.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    246\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlet num_workers > 0 to enable multiprocessing, otherwise set prefetch_factor to None.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m num_workers \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m prefetch_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     prefetch_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: prefetch_factor option could only be specified in multiprocessing.let num_workers > 0 to enable multiprocessing, otherwise set prefetch_factor to None."
     ]
    }
   ],
   "source": [
    "predictor = estimator.train(train)\n",
    "forecast_it, ts_it = make_evaluation_predictions(dataset=dataset_test,\n",
    "                                             predictor=predictor,\n",
    "                                             num_samples=100)\n",
    "forecasts = list(forecast_it)\n",
    "targets = list(ts_it)\n",
    "\n",
    "agg_metric, _ = evaluator(targets, forecasts, num_series=len(dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4878a2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchmpi",
   "language": "python",
   "name": "torchmpi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
