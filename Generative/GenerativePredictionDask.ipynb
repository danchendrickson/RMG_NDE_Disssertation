{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8fc2bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Header used on the projects\n",
    "\n",
    "#first the major packages used for math and graphing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "import scipy.special as sp\n",
    "import os as os\n",
    "import pywt as py\n",
    "import statistics as st\n",
    "import random\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import platform\n",
    "from time import time as ti\n",
    "from skimage.restoration import denoise_wavelet\n",
    "from typing import List, Optional, Callable, Iterable\n",
    "from itertools import islice\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "#Custome graph format style sheet\n",
    "#plt.style.use('Prospectus.mplstyle')\n",
    "\n",
    "#If being run by a seperate file, use the seperate file's graph format and saving paramaeters\n",
    "#otherwise set what is needed\n",
    "if not 'Saving' in locals():\n",
    "    Saving = False\n",
    "if not 'Titles' in locals():\n",
    "    Titles = True\n",
    "if not 'Ledgends' in locals():\n",
    "    Ledgends = True\n",
    "if not 'FFormat' in locals():\n",
    "    FFormat = '.png'\n",
    "\n",
    "#Standard cycle to make black and white images and dashed and line styles\n",
    "default_cycler = (cycler('color', ['0.00', '0.40', '0.60', '0.70']) + cycler(linestyle=['-', '-', '-', '-']))\n",
    "plt.rc('axes', prop_cycle=default_cycler)\n",
    "my_cmap = plt.get_cmap('gray')\n",
    "\n",
    "\n",
    "#import CoreFunctions as cf\n",
    "#from skimage.restoration import denoise_wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "509d00d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "HostName = platform.node()\n",
    "\n",
    "if HostName == \"Server\":\n",
    "    Computer = \"Desktop\"   \n",
    "elif HostName[-6:] == 'wm.edu':\n",
    "    Computer = \"SciClone\"\n",
    "elif HostName == \"SchoolLaptop\":\n",
    "    Computer = \"LinLap\"\n",
    "elif HostName == \"WTC-TAB-512\":\n",
    "    Computer = \"PortLap\"\n",
    "else:\n",
    "    Computer = \"WinLap\"\n",
    "\n",
    "if Computer == \"SciClone\":\n",
    "    location = '/sciclone/home20/dchendrickson01/image/'\n",
    "elif Computer == \"WinLap\":\n",
    "    location = 'C:\\\\Data\\\\'\n",
    "elif Computer == \"Desktop\":\n",
    "    location = \"E:\\\\Backups\\\\Dan\\\\CraneData\\\\\"\n",
    "elif Computer == \"LinLap\":\n",
    "    location = '/home/dan/Output/'\n",
    "elif Computer == 'PortLap':\n",
    "    location = 'C:\\\\users\\\\dhendrickson\\\\Desktop\\\\AccelData\\\\'\n",
    "\n",
    "if Computer ==  \"SciClone\":\n",
    "    rootfolder = '/sciclone/home20/dchendrickson01/'\n",
    "    folder = '/sciclone/scr10/dchendrickson01/Recordings2/'\n",
    "    imageFolder = '/sciclone/scr10/dchendrickson01/Move3Dprint/'\n",
    "elif Computer == \"Desktop\":\n",
    "    rootfolder = location\n",
    "    folder = rootfolder + \"Recordings2\\\\\"\n",
    "elif Computer ==\"WinLap\":\n",
    "    rootfolder = location\n",
    "    folder = rootfolder + \"Recordings2\\\\\"   \n",
    "elif Computer == \"LinLap\":\n",
    "    rootfolder = '/home/dan/Data/'\n",
    "    folder = rootfolder + 'Recordings2/'\n",
    "elif Computer =='PortLap':\n",
    "    rootfolder = location \n",
    "    folder = rootfolder + 'Recordings2\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e4e44de",
   "metadata": {},
   "outputs": [],
   "source": [
    "Saving = False\n",
    "location = folder\n",
    "Titles = True\n",
    "Ledgends = True\n",
    "f = 0\n",
    "freq = \"2s\"\n",
    "prediction_length=50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0a3c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['230418 recording1.csv','230419 recording1.csv','230420 recording1.csv','230421 recording1.csv',\n",
    "         '230418 recording2.csv','230419 recording2.csv','230420 recording2.csv','230421 recording2.csv']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25d0f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RollingStdDev(RawData, SmoothData, RollSize = 25):\n",
    "    StdDevs = []\n",
    "    for i in range(RollSize):\n",
    "        Diffs = RawData[0:i+1]-SmoothData[0:i+1]\n",
    "        Sqs = Diffs * Diffs\n",
    "        Var = sum(Sqs) / (i+1)\n",
    "        StdDev = np.sqrt(Var)\n",
    "        StdDevs.append(StdDev)\n",
    "    for i in range(len(RawData)-RollSize-1):\n",
    "        j = i + RollSize\n",
    "        Diffs = RawData[i:j]-SmoothData[i:j]\n",
    "        Sqs = Diffs * Diffs\n",
    "        Var = sum(Sqs) / RollSize\n",
    "        StdDev = np.sqrt(Var)\n",
    "        StdDevs.append(StdDev)  \n",
    "    return StdDevs\n",
    "\n",
    "def RollingSum(Data, Length = 100):\n",
    "    RollSumStdDev = []\n",
    "    for i in range(Length):\n",
    "        RollSumStdDev.append(sum(Data[0:i+1]))\n",
    "    for i in range(len(Data) - Length):\n",
    "        RollSumStdDev.append(sum(Data[i:i+Length]))\n",
    "    return RollSumStdDev\n",
    "\n",
    "def SquelchPattern(DataSet, StallRange = 5000, SquelchLevel = 0.02):\n",
    "    SquelchSignal = np.ones(len(DataSet))\n",
    "    for i in range(len(DataSet)-2*StallRange):\n",
    "        if np.average(DataSet[i:i+StallRange]) < SquelchLevel:\n",
    "            SquelchSignal[i+StallRange]=0\n",
    "    return SquelchSignal\n",
    "\n",
    "def getVelocity(Acceleration, Timestamps = 0.003, Squelch = [], corrected = 0):\n",
    "    velocity = np.zeros(len(Acceleration))\n",
    "    Acceleration -= np.average(Acceleration)\n",
    "    if len(Timestamps) == 1:\n",
    "        dTime = np.ones(len(Acceleration),dtype=float) * Timestamps\n",
    "    elif len(Timestamps) == len(Acceleration):\n",
    "        dTime = np.zeros(len(Timestamps), dtype=float)\n",
    "        dTime[0]=1\n",
    "        for i in range(len(Timestamps)-1):\n",
    "            j = i+1\n",
    "            if Timestamps[j] > Timestamps[i]:\n",
    "                dTime[j]=Timestamps[j]-Timestamps[i]\n",
    "            else:\n",
    "                dTime[j]=Timestamps[j]-Timestamps[i]+10000.0\n",
    "        dTime /= 10000.0\n",
    "    velocity[0] = Acceleration[0] * (dTime[0])\n",
    "    for i in range(len(Acceleration)-1):\n",
    "        j = i + 1\n",
    "        if corrected ==2:\n",
    "            if Squelch[j]==0:\n",
    "                velocity[j]=0\n",
    "            else:\n",
    "                velocity[j] = velocity[i] + Acceleration[j] * dTime[j]                \n",
    "        else:\n",
    "            velocity[j] = velocity[i] + Acceleration[j] * dTime[j]\n",
    "    if corrected == 1:\n",
    "        PointVairance = velocity[-1:] / len(velocity)\n",
    "        for i in range(len(velocity)):\n",
    "            velocity[i] -=  PointVairance * i\n",
    "    velocity *= 9.81\n",
    "    return velocity\n",
    "\n",
    "def MakeDTs(Seconds, Miliseconds):\n",
    "    dts = np.zeros(len(Miliseconds), dtype=float)\n",
    "    dts[0]=1\n",
    "    for i in range(len(MiliSeconds)-1):\n",
    "        j = i+1\n",
    "        if Seconds[j]==Seconds[i]:\n",
    "            dts[j]=Miliseconds[j]-Miliseconds[i]\n",
    "        else:\n",
    "            dts[j]=Miliseconds[j]-Miliseconds[i]+1000\n",
    "    dts /= 10000\n",
    "    return dts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90714f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(files)):\n",
    "    files[i]=folder+files[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c6ac37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sciclone/home/dchendrickson01/.conda/envs/test/lib/python3.11/site-packages/dask/dataframe/io/csv.py:629: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  head = reader(BytesIO(b_sample), nrows=sample_rows, **head_kwargs)\n"
     ]
    }
   ],
   "source": [
    "dataset = dd.read_csv(files, delimiter =\", \", header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fb7c0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename(columns={0:\"Day\"})\n",
    "dataset = dataset.rename(columns={1:\"Second\"})\n",
    "dataset = dataset.rename(columns={2:\"PartSec\"})\n",
    "dataset = dataset.rename(columns={3:\"p\"})\n",
    "dataset = dataset.rename(columns={4:\"h\"})\n",
    "dataset = dataset.rename(columns={5:\"v\"})\n",
    "dataset = dataset.rename(columns={6:\"Sensor\"})\n",
    "\n",
    "dataset['Day'] = dataset['Day'].apply(lambda x: str(x).zfill(6), meta=('Day', 'str'))\n",
    "dataset['Second'] = dataset['Second'].apply(lambda x: str(x).zfill(6), meta=('Second', 'str'))\n",
    "dataset['FracSec'] = dataset['PartSec'].apply(lambda x: str(x).zfill(4), meta=('PartSec', 'str'))\n",
    "\n",
    "dataset['DayTime']=dataset.Day.str.cat([dataset.Second])\n",
    "dataset['DayTime']=dataset.DayTime.str.cat([dataset.FracSec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "786bdcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['timestamp'] = dd.to_datetime(dataset.DayTime,unit='ns')\n",
    "\n",
    "dataset[\"timestamps\"] = dataset[\"timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e9c38b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method _Frame.head of Dask DataFrame Structure:\n",
       "                    Day  Second PartSec        p        h        v Sensor FracSec DayTime       timestamp      timestamps\n",
       "npartitions=100                                                                                                          \n",
       "                 object  object   int64  float64  float64  float64  int64  object  object  datetime64[ns]  datetime64[ns]\n",
       "                    ...     ...     ...      ...      ...      ...    ...     ...     ...             ...             ...\n",
       "...                 ...     ...     ...      ...      ...      ...    ...     ...     ...             ...             ...\n",
       "                    ...     ...     ...      ...      ...      ...    ...     ...     ...             ...             ...\n",
       "                    ...     ...     ...      ...      ...      ...    ...     ...     ...             ...             ...\n",
       "Dask Name: assign, 30 graph layers>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bab615",
   "metadata": {},
   "source": [
    "Works to here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90673228",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m asdf\n",
      "\u001b[0;31mNameError\u001b[0m: name 'asdf' is not defined"
     ]
    }
   ],
   "source": [
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e9a1b2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sciclone/home/dchendrickson01/.conda/envs/test/lib/python3.11/site-packages/dask/dataframe/io/csv.py:195: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = reader(bio, **kwargs)\n"
     ]
    },
    {
     "ename": "OutOfBoundsDatetime",
     "evalue": "Parsing \"2304190000001825\" to datetime overflows, at position 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/pandas/_libs/tslibs/parsing.pyx:679\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.dateutil_parse\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOverflowError\u001b[0m: signed integer is greater than maximum",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOutOfBoundsDatetime\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mp \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39maverage(dataset\u001b[38;5;241m.\u001b[39mp)\n\u001b[1;32m      2\u001b[0m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39maverage(dataset\u001b[38;5;241m.\u001b[39mh)\n\u001b[1;32m      3\u001b[0m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mv \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39maverage(dataset\u001b[38;5;241m.\u001b[39mv)\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/numpy/lib/function_base.py:511\u001b[0m, in \u001b[0;36maverage\u001b[0;34m(a, axis, weights, returned, keepdims)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_average_dispatcher)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maverage\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, returned\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    400\u001b[0m             keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m    401\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;124;03m    Compute the weighted average along the specified axis.\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;124;03m           [4.5]])\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m     a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(a)\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue:\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# Don't pass on the keepdims argument if one wasn't given.\u001b[39;00m\n\u001b[1;32m    515\u001b[0m         keepdims_kw \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/dask/dataframe/core.py:598\u001b[0m, in \u001b[0;36m_Frame.__array__\u001b[0;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[1;32m    599\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed)\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/dask/base.py:310\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    287\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/dask/base.py:595\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    592\u001b[0m     keys\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_keys__())\n\u001b[1;32m    593\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 595\u001b[0m results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/dask/threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pool, multiprocessing\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mPool):\n\u001b[1;32m     87\u001b[0m         pool \u001b[38;5;241m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 89\u001b[0m results \u001b[38;5;241m=\u001b[39m get_async(\n\u001b[1;32m     90\u001b[0m     pool\u001b[38;5;241m.\u001b[39msubmit,\n\u001b[1;32m     91\u001b[0m     pool\u001b[38;5;241m.\u001b[39m_max_workers,\n\u001b[1;32m     92\u001b[0m     dsk,\n\u001b[1;32m     93\u001b[0m     keys,\n\u001b[1;32m     94\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[1;32m     95\u001b[0m     get_id\u001b[38;5;241m=\u001b[39m_thread_get_id,\n\u001b[1;32m     96\u001b[0m     pack_exception\u001b[38;5;241m=\u001b[39mpack_exception,\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/dask/local.py:511\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m         _execute_task(task, data)  \u001b[38;5;66;03m# Re-execute locally\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         raise_exception(exc, tb)\n\u001b[1;32m    512\u001b[0m res, worker_id \u001b[38;5;241m=\u001b[39m loads(res_info)\n\u001b[1;32m    513\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m\"\u001b[39m][key] \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/dask/local.py:319\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(exc, tb)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 319\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/dask/local.py:224\u001b[0m, in \u001b[0;36mexecute_task\u001b[0;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     task, data \u001b[38;5;241m=\u001b[39m loads(task_info)\n\u001b[0;32m--> 224\u001b[0m     result \u001b[38;5;241m=\u001b[39m _execute_task(task, data)\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m get_id()\n\u001b[1;32m    226\u001b[0m     result \u001b[38;5;241m=\u001b[39m dumps((result, \u001b[38;5;28mid\u001b[39m))\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/dask/core.py:121\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    117\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m(_execute_task(a, cache) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args))\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/dask/optimization.py:992\u001b[0m, in \u001b[0;36mSubgraphCallable.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minkeys):\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m args, got \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minkeys), \u001b[38;5;28mlen\u001b[39m(args)))\n\u001b[0;32m--> 992\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m core\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdsk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutkey, \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minkeys, args)))\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/dask/core.py:151\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, out, cache)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m toposort(dsk):\n\u001b[1;32m    150\u001b[0m     task \u001b[38;5;241m=\u001b[39m dsk[key]\n\u001b[0;32m--> 151\u001b[0m     result \u001b[38;5;241m=\u001b[39m _execute_task(task, cache)\n\u001b[1;32m    152\u001b[0m     cache[key] \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m    153\u001b[0m result \u001b[38;5;241m=\u001b[39m _execute_task(out, cache)\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/dask/core.py:121\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    117\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m(_execute_task(a, cache) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args))\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/dask/core.py:121\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    117\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m(_execute_task(a, cache) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args))\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/dask/core.py:121\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    117\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m(_execute_task(a, cache) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args))\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/dask/utils.py:73\u001b[0m, in \u001b[0;36mapply\u001b[0;34m(func, args, kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Apply a function given its positional and keyword arguments.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03mEquivalent to ``func(*args, **kwargs)``\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m>>> dsk = {'task-name': task}  # adds the task to a low level Dask task graph\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/dask/dataframe/core.py:7089\u001b[0m, in \u001b[0;36mapply_and_enforce\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   7087\u001b[0m func \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_func\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7088\u001b[0m meta \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_meta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7089\u001b[0m df \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   7090\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dataframe_like(df) \u001b[38;5;129;01mor\u001b[39;00m is_series_like(df) \u001b[38;5;129;01mor\u001b[39;00m is_index_like(df):\n\u001b[1;32m   7091\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df):\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:1050\u001b[0m, in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1048\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1050\u001b[0m         values \u001b[38;5;241m=\u001b[39m convert_listlike(arg\u001b[38;5;241m.\u001b[39m_values, \u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1051\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39m_constructor(values, index\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc\u001b[38;5;241m.\u001b[39mMutableMapping)):\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:427\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    426\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot specify both format and unit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _to_datetime_with_unit(arg, unit, name, utc, errors)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg must be a string, datetime, list, tuple, 1-d array, or Series\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:539\u001b[0m, in \u001b[0;36m_to_datetime_with_unit\u001b[0;34m(arg, unit, name, utc, errors)\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    538\u001b[0m         arg \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 539\u001b[0m         arr, tz_parsed \u001b[38;5;241m=\u001b[39m tslib\u001b[38;5;241m.\u001b[39marray_with_unit_to_datetime(arg, unit, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;66;03m# Index constructor _may_ infer to DatetimeIndex\u001b[39;00m\n\u001b[1;32m    543\u001b[0m     result \u001b[38;5;241m=\u001b[39m Index\u001b[38;5;241m.\u001b[39m_with_infer(arr, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/pandas/_libs/tslib.pyx:272\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_with_unit_to_datetime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/pandas/_libs/tslib.pyx:551\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/pandas/_libs/tslib.pyx:516\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/pandas/_libs/tslibs/conversion.pyx:557\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.conversion.convert_str_to_tsobject\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/pandas/_libs/tslibs/parsing.pyx:329\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.parse_datetime_string\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/pandas/_libs/tslibs/parsing.pyx:687\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.dateutil_parse\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOutOfBoundsDatetime\u001b[0m: Parsing \"2304190000001825\" to datetime overflows, at position 0"
     ]
    }
   ],
   "source": [
    "dataset[\"p\"] = dataset.p - np.average(dataset.p)\n",
    "dataset[\"h\"] = dataset.h - np.average(dataset.h)\n",
    "dataset[\"v\"] = dataset.v - np.average(dataset.v)\n",
    "dataset[\"r\"] = np.sqrt(dataset.p**2 + dataset.h**2 + dataset.v**2)\n",
    "\n",
    "\n",
    "dataset[\"smoothP\"] = denoise_wavelet(dataset.p, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "dataset[\"SmoothH\"] = denoise_wavelet(dataset.h, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "dataset[\"SmoothV\"] = denoise_wavelet(dataset.v, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "\n",
    "StdDevsV = RollingStdDev(dataset.v, dataset.SmoothV)\n",
    "StdDevsV.append(0)\n",
    "StdDevsV = np.asarray(StdDevsV)\n",
    "SmoothDevV = denoise_wavelet(StdDevsV, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "\n",
    "Max = np.max(SmoothDevV)\n",
    "buckets = int(Max / 0.005) + 1\n",
    "bins = np.linspace(0,buckets*0.005,buckets+1)\n",
    "counts, bins = np.histogram(SmoothDevV,bins=bins)\n",
    "\n",
    "CummCount = 0\n",
    "HalfWay = 0\n",
    "for i in range(len(counts)):\n",
    "    CummCount += counts[i]\n",
    "    if CummCount / len(SmoothDevZ) >= 0.5:\n",
    "        if HalfWay == 0:\n",
    "            HalfWay = i\n",
    "\n",
    "SquelchLevel = bins[HalfWay] \n",
    "dataset[\"IsMoving\"] = SquelchPattern(SmoothDevV, 4000, SquelchLevel)\n",
    "\n",
    "dataset[\"velocity\"] = getVelocity(dataset.r, dataset.PartSec, dataset.IsMoving, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c683f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf09908",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Parallel(n_jobs=8)(delayed(MakeDataframe)(file) for file in files)\n",
    "dataset = pd.concat(data)[[\"timestamps\",\"x\",\"y\",\"z\",\"r\",\"IsMoving\",\"Velocity\"]]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9868582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.torch import DeepAREstimator\n",
    "#from gluonts.torch.trainer import Trainer\n",
    "from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
    "\n",
    "\n",
    "def train_and_predict(dataset, estimator):\n",
    "    predictor = estimator.train(dataset)\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=dataset, predictor=predictor\n",
    "    )\n",
    "    evaluator = Evaluator(quantiles=(np.arange(20) / 20.0)[1:])\n",
    "    agg_metrics, item_metrics = evaluator(ts_it, forecast_it, num_series=len(dataset))\n",
    "    return agg_metrics[\"MSE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dc5241",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = DeepAREstimator(\n",
    "    freq=freq, prediction_length=prediction_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591f8faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.dataset.pandas import PandasDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d688cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds = PandasDataset(dataset, target=\"x\", freq=freq)\n",
    "ds = PandasDataset.from_long_dataframe(\n",
    "    dataframe=dataset,\n",
    "    item_id=\"timestamp\",\n",
    "    timestamp=\"timestamps\",\n",
    "    freq=\"ns\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b09675",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_predict(dataset, estimator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
