{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bec0f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Header used on the projects\n",
    "\n",
    "#first the major packages used for math and graphing\n",
    "import numpy as np\n",
    " \n",
    "from cycler import cycler\n",
    "import scipy.special as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Custome graph format style sheet\n",
    "#plt.style.use('Prospectus.mplstyle')\n",
    "\n",
    "#If being run by a seperate file, use the seperate file's graph format and saving paramaeters\n",
    "#otherwise set what is needed\n",
    "if not 'Saving' in locals():\n",
    "    Saving = False\n",
    "if not 'Titles' in locals():\n",
    "    Titles = True\n",
    "if not 'Ledgends' in locals():\n",
    "    Ledgends = True\n",
    "if not 'FFormat' in locals():\n",
    "    FFormat = '.png'\n",
    "\n",
    "#Standard cycle to make black and white images and dashed and line styles\n",
    "default_cycler = (cycler('color', ['0.00', '0.40', '0.60', '0.70']) + cycler(linestyle=['-', '-', '-', '-']))\n",
    "plt.rc('axes', prop_cycle=default_cycler)\n",
    "my_cmap = plt.get_cmap('gray')\n",
    "\n",
    "#Extra Headers:\n",
    "import os as os\n",
    "import pywt as py\n",
    "import statistics as st\n",
    "import os as os\n",
    "import random\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import platform\n",
    "\n",
    "from time import time as ti\n",
    "\n",
    "#import CoreFunctions as cf\n",
    "#from skimage.restoration import denoise_wavelet\n",
    "\n",
    "#Standard Header used on the projects\n",
    "\n",
    "#first the major packages used for math and graphing\n",
    "import numpy as np\n",
    " \n",
    "from cycler import cycler\n",
    "import scipy.special as sp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75c3d7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custome graph format style sheet\n",
    "#plt.style.use('Prospectus.mplstyle')\n",
    "\n",
    "#If being run by a seperate file, use the seperate file's graph format and saving paramaeters\n",
    "#otherwise set what is needed\n",
    "if not 'Saving' in locals():\n",
    "    Saving = False\n",
    "if not 'Titles' in locals():\n",
    "    Titles = True\n",
    "if not 'Ledgends' in locals():\n",
    "    Ledgends = True\n",
    "if not 'FFormat' in locals():\n",
    "    FFormat = '.png'\n",
    "\n",
    "#Standard cycle to make black and white images and dashed and line styles\n",
    "default_cycler = (cycler('color', ['0.00', '0.40', '0.60', '0.70']) + cycler(linestyle=['-', '-', '-', '-']))\n",
    "plt.rc('axes', prop_cycle=default_cycler)\n",
    "my_cmap = plt.get_cmap('gray')\n",
    "\n",
    "#Extra Headers:\n",
    "import os as os\n",
    "import pywt as py\n",
    "import statistics as st\n",
    "import os as os\n",
    "import random\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import platform\n",
    "\n",
    "from time import time as ti\n",
    "\n",
    "#import CoreFunctions as cf\n",
    "from skimage.restoration import denoise_wavelet\n",
    "\n",
    "from typing import List, Optional, Callable, Iterable\n",
    "from itertools import islice\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "HostName = platform.node()\n",
    "\n",
    "if HostName == \"Server\":\n",
    "    Computer = \"Desktop\"   \n",
    "elif HostName[-6:] == 'wm.edu':\n",
    "    Computer = \"SciClone\"\n",
    "elif HostName == \"SchoolLaptop\":\n",
    "    Computer = \"LinLap\"\n",
    "elif HostName == \"WTC-TAB-512\":\n",
    "    Computer = \"PortLap\"\n",
    "else:\n",
    "    Computer = \"WinLap\"\n",
    "\n",
    "if Computer == \"SciClone\":\n",
    "    location = '/sciclone/home20/dchendrickson01/image/'\n",
    "elif Computer == \"WinLap\":\n",
    "    location = 'C:\\\\Data\\\\'\n",
    "elif Computer == \"Desktop\":\n",
    "    location = \"E:\\\\Backups\\\\Dan\\\\CraneData\\\\\"\n",
    "elif Computer == \"LinLap\":\n",
    "    location = '/home/dan/Output/'\n",
    "elif Computer == 'PortLap':\n",
    "    location = 'C:\\\\users\\\\dhendrickson\\\\Desktop\\\\AccelData\\\\'\n",
    "\n",
    "if Computer ==  \"SciClone\":\n",
    "    rootfolder = '/sciclone/home20/dchendrickson01/'\n",
    "    folder = '/sciclone/scr10/dchendrickson01/Recordings2/'\n",
    "    imageFolder = '/sciclone/scr10/dchendrickson01/Move3Dprint/'\n",
    "elif Computer == \"Desktop\":\n",
    "    rootfolder = location\n",
    "    folder = rootfolder + \"Recordings2\\\\\"\n",
    "elif Computer ==\"WinLap\":\n",
    "    rootfolder = location\n",
    "    folder = rootfolder + \"Recordings2\\\\\"   \n",
    "elif Computer == \"LinLap\":\n",
    "    rootfolder = '/home/dan/Data/'\n",
    "    folder = rootfolder + 'Recordings2/'\n",
    "elif Computer =='PortLap':\n",
    "    rootfolder = location \n",
    "    folder = rootfolder + 'Recordings2\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c9780e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Saving = False\n",
    "location = folder\n",
    "Titles = True\n",
    "Ledgends = True\n",
    "\n",
    "f = 0\n",
    "freq = \"2s\"\n",
    "prediction_length=50\n",
    "\n",
    "files = ['230418 recording1.csv','230419 recording1.csv','230420 recording1.csv','230421 recording1.csv']#,\n",
    "#         '230418 recording2.csv','230419 recording2.csv','230420 recording2.csv','230421 recording2.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a34469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RollingStdDev(RawData, SmoothData, RollSize = 25):\n",
    "    StdDevs = []\n",
    "    for i in range(RollSize):\n",
    "        Diffs = RawData.iloc[0:i+1]-SmoothData.iloc[0:i+1]\n",
    "        Sqs = Diffs * Diffs\n",
    "        Var = sum(Sqs) / (i+1)\n",
    "        StdDev = np.sqrt(Var)\n",
    "        StdDevs.append(StdDev)\n",
    "    for i in range(len(RawData)-RollSize-1):\n",
    "        j = i + RollSize\n",
    "        Diffs = RawData.iloc[i:j]-SmoothData.iloc[i:j]\n",
    "        Sqs = Diffs * Diffs\n",
    "        Var = sum(Sqs) / RollSize\n",
    "        StdDev = np.sqrt(Var)\n",
    "        StdDevs.append(StdDev)\n",
    "\n",
    "    return StdDevs\n",
    "\n",
    "def RollingSum(Data, Length = 100):\n",
    "    RollSumStdDev = []\n",
    "    for i in range(Length):\n",
    "        RollSumStdDev.append(sum(Data[0:i+1]))\n",
    "    for i in range(len(Data) - Length):\n",
    "        RollSumStdDev.append(sum(Data.iloc[i:i+Length]))\n",
    "    return RollSumStdDev\n",
    "\n",
    "def SquelchPattern(DataSet, StallRange = 5000, SquelchLevel = 0.02):\n",
    "    SquelchSignal = np.ones(len(DataSet))\n",
    "\n",
    "    for i in range(len(DataSet)-2*StallRange):\n",
    "        if np.average(DataSet[i:i+StallRange]) < SquelchLevel:\n",
    "            SquelchSignal[i+StallRange]=0\n",
    "\n",
    "    return SquelchSignal\n",
    "\n",
    "def getVelocity(Acceleration, Timestamps = 0.003, Squelch = [], corrected = 0):\n",
    "    velocity = np.zeros(len(Acceleration))\n",
    "    \n",
    "    Acceleration -= np.average(Acceleration)\n",
    "    \n",
    "    if len(Timestamps) == 1:\n",
    "        dTime = np.ones(len(Acceleration),dtype=float) * Timestamps\n",
    "    elif len(Timestamps) == len(Acceleration):\n",
    "        dTime = np.zeros(len(Timestamps), dtype=float)\n",
    "        dTime[0]=1\n",
    "        for i in range(len(Timestamps)-1):\n",
    "            j = i+1\n",
    "            if Timestamps.iloc[j] > Timestamps.iloc[i]:\n",
    "                dTime[j]=Timestamps.iloc[j]-Timestamps.iloc[i]\n",
    "            else:\n",
    "                dTime[j]=Timestamps.iloc[j]-Timestamps.iloc[i]+10000.0\n",
    "        dTime /= 10000.0\n",
    "\n",
    "    velocity[0] = Acceleration.iloc[0] * (dTime[0])\n",
    "\n",
    "    for i in range(len(Acceleration)-1):\n",
    "        j = i + 1\n",
    "        if corrected ==2:\n",
    "            if Squelch.iloc[j]==0:\n",
    "                velocity[j]=0\n",
    "            else:\n",
    "                velocity[j] = velocity[i] + Acceleration.iloc[j] * dTime[j]                \n",
    "        else:\n",
    "            velocity[j] = velocity[i] + Acceleration.iloc[j] * dTime[j]\n",
    "\n",
    "    if corrected == 1:\n",
    "        PointVairance = velocity[-1:] / len(velocity)\n",
    "        for i in range(len(velocity)):\n",
    "            velocity[i] -=  PointVairance * i\n",
    "    \n",
    "    velocity *= 9.81\n",
    "\n",
    "    return velocity\n",
    "\n",
    "def MakeDTs(Seconds, Miliseconds):\n",
    "    dts = np.zeros(len(Miliseconds), dtype=float)\n",
    "    dts[0]=1\n",
    "    for i in range(len(MiliSeconds)-1):\n",
    "        j = i+1\n",
    "        if Seconds[j]==Seconds[i]:\n",
    "            dts[j]=Miliseconds[j]-Miliseconds[i]\n",
    "        else:\n",
    "            dts[j]=Miliseconds[j]-Miliseconds[i]+1000\n",
    "    dts /= 10000\n",
    "    return dts\n",
    "\n",
    "def MakeDataframe(file):\n",
    "    dataset = pd.read_table(folder+file, delimiter =\", \", header=None, engine='python')\n",
    "\n",
    "    dataset = dataset.rename(columns={0:\"Day\"})\n",
    "    dataset = dataset.rename(columns={1:\"Second\"})\n",
    "    dataset = dataset.rename(columns={2:\"PartSec\"})\n",
    "    dataset = dataset.rename(columns={3:\"p\"})\n",
    "    dataset = dataset.rename(columns={4:\"h\"})\n",
    "    dataset = dataset.rename(columns={5:\"v\"})\n",
    "    dataset = dataset.rename(columns={6:\"Sensor\"})\n",
    "\n",
    "    dataset[['Day','Second']] = dataset[['Day','Second']].apply(lambda x: x.astype(int).astype(str).str.zfill(6))\n",
    "    dataset[['FracSec']] = dataset[['PartSec']].apply(lambda x: x.astype(int).astype(str).str.zfill(4))\n",
    "\n",
    "    dataset[\"timestamp\"] = pd.to_datetime(dataset.Day+dataset.Second\n",
    "                                          +dataset.FracSec, format='%y%m%d%H%M%S%f')\n",
    "    dataset[\"dt\"]=dataset.timestamp - dataset.timestamp.shift(1, fill_value=dataset.timestamp[0])\n",
    "    \n",
    "    dataset[\"timestamps\"] = pd.date_range(dataset.timestamp.iloc[0], dataset.timestamp.iloc[-1],len(dataset.timestamp))\n",
    "   \n",
    "    dataset[\"p\"] = dataset.p - np.average(dataset.p)\n",
    "    dataset[\"h\"] = dataset.h - np.average(dataset.h)\n",
    "    dataset[\"v\"] = dataset.v - np.average(dataset.v)\n",
    "    dataset[\"r\"] = np.sqrt(dataset.p**2 + dataset.h**2 + dataset.v**2)\n",
    "\n",
    "    #dataset.index = dataset.timestamps\n",
    "\n",
    "    dataset[\"SmoothV\"] = denoise_wavelet(dataset.v, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "\n",
    "    StdDevsV = RollingStdDev(dataset.v, dataset.SmoothV)\n",
    "    StdDevsV.append(0)\n",
    "    StdDevsV = np.asarray(StdDevsV)\n",
    "    SmoothDevV = denoise_wavelet(StdDevsV, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "\n",
    "    Max = np.max(SmoothDevV)\n",
    "    buckets = int(Max / 0.005) + 1\n",
    "    bins = np.linspace(0,buckets*0.005,buckets+1)\n",
    "    counts, bins = np.histogram(SmoothDevV,bins=bins)\n",
    "\n",
    "    CummCount = 0\n",
    "    HalfWay = 0\n",
    "    for i in range(len(counts)):\n",
    "        CummCount += counts[i]\n",
    "        if CummCount / len(SmoothDevV) >= 0.5:\n",
    "            if HalfWay == 0:\n",
    "                HalfWay = i\n",
    "\n",
    "    SquelchLevel = bins[HalfWay] \n",
    "    dataset[\"IsMoving\"] = SquelchPattern(SmoothDevV, 4000, SquelchLevel)\n",
    "\n",
    "    dataset[\"velocity\"] = getVelocity(dataset.r, dataset.PartSec, dataset.IsMoving, 2)\n",
    "    \n",
    "    #dataset.timestamps.apply(pd.Timestamp.strip('[]'))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaaf4320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting data\n",
      "Have data\n"
     ]
    }
   ],
   "source": [
    "print('getting data')\n",
    "data = Parallel(n_jobs=16)(delayed(MakeDataframe)(file) for file in files)\n",
    "#data=MakeDataframe(files[0])\n",
    "dataset = pd.concat(data)[[\"p\",\"h\",\"v\",\"r\",\"IsMoving\",\"velocity\",\"timestamps\",\"timestamp\"]]\n",
    "dataset.head()\n",
    "print('Have data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03bbd49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                  p         h         v         r  IsMoving    velocity  \\\n",
       "0        -0.002305  0.002889  0.004128 -0.023450       1.0   -0.000023   \n",
       "1         0.015265  0.000219 -0.016372 -0.006606       1.0   -0.000334   \n",
       "2        -0.012805  0.003869  0.024888 -0.000735       1.0   -0.000368   \n",
       "3        -0.014025 -0.011261 -0.004172 -0.010527       1.0   -0.000853   \n",
       "4         0.005495  0.008269  0.006328 -0.017217       1.0   -0.001647   \n",
       "...            ...       ...       ...       ...       ...         ...   \n",
       "18968155 -0.007091  0.009128 -0.010855 -0.011538       1.0  100.457198   \n",
       "18968156 -0.015881  0.018398  0.007205 -0.002045       1.0  100.457104   \n",
       "18968157  0.015849  0.012788 -0.039665  0.017193       1.0  100.457897   \n",
       "18968158  0.002909 -0.002342  0.026245 -0.000885       1.0  100.457856   \n",
       "18968159  0.015609  0.009368 -0.030875  0.008448       1.0  100.458245   \n",
       "\n",
       "                            timestamps                  timestamp  \n",
       "0        2023-04-18 00:00:00.172800000 2023-04-18 00:00:00.172800  \n",
       "1        2023-04-18 00:00:00.177430433 2023-04-18 00:00:00.177600  \n",
       "2        2023-04-18 00:00:00.182060866 2023-04-18 00:00:00.182300  \n",
       "3        2023-04-18 00:00:00.186691300 2023-04-18 00:00:00.187000  \n",
       "4        2023-04-18 00:00:00.191321733 2023-04-18 00:00:00.191700  \n",
       "...                                ...                        ...  \n",
       "18968155 2023-04-21 23:59:59.985880026 2023-04-21 23:59:59.985300  \n",
       "18968156 2023-04-21 23:59:59.990435019 2023-04-21 23:59:59.990000  \n",
       "18968157 2023-04-21 23:59:59.994990013 2023-04-21 23:59:59.994700  \n",
       "18968158 2023-04-21 23:59:59.999545006 2023-04-21 23:59:59.999400  \n",
       "18968159 2023-04-22 00:00:00.004100000 2023-04-22 00:00:00.004100  \n",
       "\n",
       "[75629566 rows x 8 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75d47e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.torch import DeepAREstimator\n",
    "#from gluonts.torch.trainer import Trainer\n",
    "from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
    "\n",
    "\n",
    "def train_and_predict(dataset, estimator):\n",
    "    predictor = estimator.train(dataset)\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=dataset, predictor=predictor\n",
    "    )\n",
    "    evaluator = Evaluator(quantiles=(np.arange(20) / 20.0)[1:])\n",
    "    agg_metrics, item_metrics = evaluator(ts_it, forecast_it, num_series=len(dataset))\n",
    "    return agg_metrics[\"MSE\"]\n",
    "\n",
    "estimator = DeepAREstimator(\n",
    "    freq=freq, prediction_length=prediction_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e732e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model set up\n"
     ]
    }
   ],
   "source": [
    "print('Model set up')\n",
    "\n",
    "#dataset.timestamps.apply(pd.Timestamp.strip('[]'))\n",
    "\n",
    "\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "\n",
    "#ds = PandasDataset(dataset, target=\"x\", freq=freq)\n",
    "ds = PandasDataset.from_long_dataframe(\n",
    "    dataframe=dataset,\n",
    "    item_id=\"timestamps\",\n",
    "    #timestamp=\"timestamps\",\n",
    "    freq=\"ns\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c527494",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_predict(ds, estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdab78f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(estimator.state_dict(), 'TorchGen')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchmpi",
   "language": "python",
   "name": "torchmpi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
