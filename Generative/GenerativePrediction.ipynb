{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8fc2bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Header used on the projects\n",
    "\n",
    "#first the major packages used for math and graphing\n",
    "import numpy as np\n",
    " \n",
    "from cycler import cycler\n",
    "import scipy.special as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Custome graph format style sheet\n",
    "#plt.style.use('Prospectus.mplstyle')\n",
    "\n",
    "#If being run by a seperate file, use the seperate file's graph format and saving paramaeters\n",
    "#otherwise set what is needed\n",
    "if not 'Saving' in locals():\n",
    "    Saving = False\n",
    "if not 'Titles' in locals():\n",
    "    Titles = True\n",
    "if not 'Ledgends' in locals():\n",
    "    Ledgends = True\n",
    "if not 'FFormat' in locals():\n",
    "    FFormat = '.png'\n",
    "\n",
    "#Standard cycle to make black and white images and dashed and line styles\n",
    "default_cycler = (cycler('color', ['0.00', '0.40', '0.60', '0.70']) + cycler(linestyle=['-', '-', '-', '-']))\n",
    "plt.rc('axes', prop_cycle=default_cycler)\n",
    "my_cmap = plt.get_cmap('gray')\n",
    "\n",
    "#Extra Headers:\n",
    "import os as os\n",
    "import pywt as py\n",
    "import statistics as st\n",
    "import os as os\n",
    "import random\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import platform\n",
    "\n",
    "from time import time as ti\n",
    "\n",
    "#import CoreFunctions as cf\n",
    "#from skimage.restoration import denoise_wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecb90bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.restoration import denoise_wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f932ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Callable, Iterable\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "982c6621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.dates as mdates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "509d00d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "HostName = platform.node()\n",
    "\n",
    "if HostName == \"Server\":\n",
    "    Computer = \"Desktop\"   \n",
    "elif HostName[-6:] == 'wm.edu':\n",
    "    Computer = \"SciClone\"\n",
    "elif HostName == \"SchoolLaptop\":\n",
    "    Computer = \"LinLap\"\n",
    "elif HostName == \"WTC-TAB-512\":\n",
    "    Computer = \"PortLap\"\n",
    "else:\n",
    "    Computer = \"WinLap\"\n",
    "\n",
    "if Computer == \"SciClone\":\n",
    "    location = '/sciclone/home20/dchendrickson01/image/'\n",
    "elif Computer == \"WinLap\":\n",
    "    location = 'C:\\\\Data\\\\'\n",
    "elif Computer == \"Desktop\":\n",
    "    location = \"E:\\\\Backups\\\\Dan\\\\CraneData\\\\\"\n",
    "elif Computer == \"LinLap\":\n",
    "    location = '/home/dan/Output/'\n",
    "elif Computer == 'PortLap':\n",
    "    location = 'C:\\\\users\\\\dhendrickson\\\\Desktop\\\\AccelData\\\\'\n",
    "\n",
    "if Computer ==  \"SciClone\":\n",
    "    rootfolder = '/sciclone/home20/dchendrickson01/'\n",
    "    folder = '/sciclone/scr10/dchendrickson01/Recordings2/'\n",
    "    imageFolder = '/sciclone/scr10/dchendrickson01/Move3Dprint/'\n",
    "elif Computer == \"Desktop\":\n",
    "    rootfolder = location\n",
    "    folder = rootfolder + \"Recordings2\\\\\"\n",
    "elif Computer ==\"WinLap\":\n",
    "    rootfolder = location\n",
    "    folder = rootfolder + \"Recordings2\\\\\"   \n",
    "elif Computer == \"LinLap\":\n",
    "    rootfolder = '/home/dan/Data/'\n",
    "    folder = rootfolder + 'Recordings2/'\n",
    "elif Computer =='PortLap':\n",
    "    rootfolder = location \n",
    "    folder = rootfolder + 'Recordings2\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e4e44de",
   "metadata": {},
   "outputs": [],
   "source": [
    "Saving = False\n",
    "location = folder\n",
    "Titles = True\n",
    "Ledgends = True\n",
    "\n",
    "f = 0\n",
    "freq = \"2s\"\n",
    "prediction_length=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0a3c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['230418 recording1.csv','230419 recording1.csv','230420 recording1.csv','230421 recording1.csv',\n",
    "         '230418 recording2.csv','230419 recording2.csv','230420 recording2.csv','230421 recording2.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21990b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25d0f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RollingStdDev(RawData, SmoothData, RollSize = 25):\n",
    "    StdDevs = []\n",
    "    for i in range(RollSize):\n",
    "        Diffs = RawData[0:i+1]-SmoothData[0:i+1]\n",
    "        Sqs = Diffs * Diffs\n",
    "        Var = sum(Sqs) / (i+1)\n",
    "        StdDev = np.sqrt(Var)\n",
    "        StdDevs.append(StdDev)\n",
    "    for i in range(len(RawData)-RollSize-1):\n",
    "        j = i + RollSize\n",
    "        Diffs = RawData[i:j]-SmoothData[i:j]\n",
    "        Sqs = Diffs * Diffs\n",
    "        Var = sum(Sqs) / RollSize\n",
    "        StdDev = np.sqrt(Var)\n",
    "        StdDevs.append(StdDev)  \n",
    "    \n",
    "    return StdDevs\n",
    "\n",
    "def RollingSum(Data, Length = 100):\n",
    "    RollSumStdDev = []\n",
    "    for i in range(Length):\n",
    "        RollSumStdDev.append(sum(Data[0:i+1]))\n",
    "    for i in range(len(Data) - Length):\n",
    "        RollSumStdDev.append(sum(Data[i:i+Length]))\n",
    "    return RollSumStdDev\n",
    "\n",
    "def SquelchPattern(DataSet, StallRange = 5000, SquelchLevel = 0.02):\n",
    "    SquelchSignal = np.ones(len(DataSet))\n",
    "\n",
    "    for i in range(len(DataSet)-2*StallRange):\n",
    "        if np.average(DataSet[i:i+StallRange]) < SquelchLevel:\n",
    "            SquelchSignal[i+StallRange]=0\n",
    "\n",
    "    return SquelchSignal\n",
    "\n",
    "def getVelocity(Acceleration, Timestamps = 0.003, Squelch = [], corrected = 0):\n",
    "    velocity = np.zeros(len(Acceleration))\n",
    "    \n",
    "    Acceleration -= np.average(Acceleration)\n",
    "    \n",
    "    if len(Timestamps) == 1:\n",
    "        dTime = np.ones(len(Acceleration),dtype=float) * Timestamps\n",
    "    elif len(Timestamps) == len(Acceleration):\n",
    "        dTime = np.zeros(len(Timestamps), dtype=float)\n",
    "        dTime[0]=1\n",
    "        for i in range(len(Timestamps)-1):\n",
    "            j = i+1\n",
    "            if Timestamps[j] > Timestamps[i]:\n",
    "                dTime[j]=Timestamps[j]-Timestamps[i]\n",
    "            else:\n",
    "                dTime[j]=Timestamps[j]-Timestamps[i]+10000.0\n",
    "        dTime /= 10000.0\n",
    "\n",
    "    velocity[0] = Acceleration[0] * (dTime[0])\n",
    "\n",
    "    for i in range(len(Acceleration)-1):\n",
    "        j = i + 1\n",
    "        if corrected ==2:\n",
    "            if Squelch[j]==0:\n",
    "                velocity[j]=0\n",
    "            else:\n",
    "                velocity[j] = velocity[i] + Acceleration[j] * dTime[j]                \n",
    "        else:\n",
    "            velocity[j] = velocity[i] + Acceleration[j] * dTime[j]\n",
    "\n",
    "    if corrected == 1:\n",
    "        PointVairance = velocity[-1:] / len(velocity)\n",
    "        for i in range(len(velocity)):\n",
    "            velocity[i] -=  PointVairance * i\n",
    "    \n",
    "    velocity *= 9.81\n",
    "\n",
    "    return velocity\n",
    "\n",
    "def MakeDTs(Seconds, Miliseconds):\n",
    "    dts = np.zeros(len(Miliseconds), dtype=float)\n",
    "    dts[0]=1\n",
    "    for i in range(len(MiliSeconds)-1):\n",
    "        j = i+1\n",
    "        if Seconds[j]==Seconds[i]:\n",
    "            dts[j]=Miliseconds[j]-Miliseconds[i]\n",
    "        else:\n",
    "            dts[j]=Miliseconds[j]-Miliseconds[i]+1000\n",
    "    dts /= 10000\n",
    "    return dts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e9a1b2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def MakeDataframe(file):\n",
    "    dataset = pd.read_table(folder+file, delimiter =\", \", header=None, engine='python')\n",
    "\n",
    "    dataset = dataset.rename(columns={0:\"Day\"})\n",
    "    dataset = dataset.rename(columns={1:\"Second\"})\n",
    "    dataset = dataset.rename(columns={2:\"PartSec\"})\n",
    "    dataset = dataset.rename(columns={3:\"p\"})\n",
    "    dataset = dataset.rename(columns={4:\"h\"})\n",
    "    dataset = dataset.rename(columns={5:\"v\"})\n",
    "    dataset = dataset.rename(columns={6:\"Sensor\"})\n",
    "\n",
    "    dataset[['Day','Second']] = dataset[['Day','Second']].apply(lambda x: x.astype(int).astype(str).str.zfill(6))\n",
    "    dataset[['FracSec']] = dataset[['PartSec']].apply(lambda x: x.astype(int).astype(str).str.zfill(4))\n",
    "\n",
    "    dataset[\"timestamp\"] = pd.to_datetime(dataset.Day+dataset.Second\n",
    "                                          +dataset.FracSec, format='%y%m%d%H%M%S%f')\n",
    "    dataset[\"dt\"]=dataset.timestamp - dataset.timestamp.shift(1, fill_value=dataset.timestamp[0])\n",
    "    \n",
    "    dataset[\"timestamps\"] = pd.Series(pd.date_range(dataset.timestamp.dt.date[0], periods=len(dataset),freq=dataset.dt.mean())).dt.time\n",
    "\n",
    "    dataset[\"p\"] = dataset.p - np.average(dataset.p)\n",
    "    dataset[\"h\"] = dataset.h - np.average(dataset.h)\n",
    "    dataset[\"v\"] = dataset.v - np.average(dataset.v)\n",
    "    dataset[\"r\"] = np.sqrt(dataset.p**2 + dataset.h**2 + dataset.v**2)\n",
    "\n",
    "    dataset.index = dataset.timestamps\n",
    "\n",
    "    dataset[\"SmoothV\"] = denoise_wavelet(dataset.v, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "\n",
    "    StdDevsV = RollingStdDev(dataset.v, dataset.SmoothV)\n",
    "    StdDevsV.append(0)\n",
    "    StdDevsV = np.asarray(StdDevsV)\n",
    "    SmoothDevV = denoise_wavelet(StdDevsV, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "\n",
    "    Max = np.max(SmoothDevV)\n",
    "    buckets = int(Max / 0.005) + 1\n",
    "    bins = np.linspace(0,buckets*0.005,buckets+1)\n",
    "    counts, bins = np.histogram(SmoothDevV,bins=bins)\n",
    "\n",
    "    CummCount = 0\n",
    "    HalfWay = 0\n",
    "    for i in range(len(counts)):\n",
    "        CummCount += counts[i]\n",
    "        if CummCount / len(SmoothDevV) >= 0.5:\n",
    "            if HalfWay == 0:\n",
    "                HalfWay = i\n",
    "\n",
    "    SquelchLevel = bins[HalfWay] \n",
    "    dataset[\"IsMoving\"] = SquelchPattern(SmoothDevV, 4000, SquelchLevel)\n",
    "\n",
    "    dataset[\"velocity\"] = getVelocity(dataset.r, dataset.PartSec, dataset.IsMoving, 2)\n",
    "    \n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdf09908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>h</th>\n",
       "      <th>v</th>\n",
       "      <th>r</th>\n",
       "      <th>IsMoving</th>\n",
       "      <th>velocity</th>\n",
       "      <th>timestamps</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamps</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00:00:00</th>\n",
       "      <td>-0.002305</td>\n",
       "      <td>0.002889</td>\n",
       "      <td>0.004128</td>\n",
       "      <td>-0.023450</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>2023-04-18 00:00:00.172800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00:00:00.004630</th>\n",
       "      <td>0.015265</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>-0.016372</td>\n",
       "      <td>-0.006606</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.000334</td>\n",
       "      <td>00:00:00.004630</td>\n",
       "      <td>2023-04-18 00:00:00.177600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00:00:00.009260</th>\n",
       "      <td>-0.012805</td>\n",
       "      <td>0.003869</td>\n",
       "      <td>0.024888</td>\n",
       "      <td>-0.000735</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.000368</td>\n",
       "      <td>00:00:00.009260</td>\n",
       "      <td>2023-04-18 00:00:00.182300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00:00:00.013891</th>\n",
       "      <td>-0.014025</td>\n",
       "      <td>-0.011261</td>\n",
       "      <td>-0.004172</td>\n",
       "      <td>-0.010527</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.000853</td>\n",
       "      <td>00:00:00.013891</td>\n",
       "      <td>2023-04-18 00:00:00.187000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00:00:00.018521</th>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.008269</td>\n",
       "      <td>0.006328</td>\n",
       "      <td>-0.017217</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.001647</td>\n",
       "      <td>00:00:00.018521</td>\n",
       "      <td>2023-04-18 00:00:00.191700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        p         h         v         r  IsMoving  velocity  \\\n",
       "timestamps                                                                    \n",
       "00:00:00        -0.002305  0.002889  0.004128 -0.023450       1.0 -0.000023   \n",
       "00:00:00.004630  0.015265  0.000219 -0.016372 -0.006606       1.0 -0.000334   \n",
       "00:00:00.009260 -0.012805  0.003869  0.024888 -0.000735       1.0 -0.000368   \n",
       "00:00:00.013891 -0.014025 -0.011261 -0.004172 -0.010527       1.0 -0.000853   \n",
       "00:00:00.018521  0.005495  0.008269  0.006328 -0.017217       1.0 -0.001647   \n",
       "\n",
       "                      timestamps                  timestamp  \n",
       "timestamps                                                   \n",
       "00:00:00                00:00:00 2023-04-18 00:00:00.172800  \n",
       "00:00:00.004630  00:00:00.004630 2023-04-18 00:00:00.177600  \n",
       "00:00:00.009260  00:00:00.009260 2023-04-18 00:00:00.182300  \n",
       "00:00:00.013891  00:00:00.013891 2023-04-18 00:00:00.187000  \n",
       "00:00:00.018521  00:00:00.018521 2023-04-18 00:00:00.191700  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Parallel(n_jobs=4)(delayed(MakeDataframe)(file) for file in files[:2])\n",
    "dataset = pd.concat(data)[[\"p\",\"h\",\"v\",\"r\",\"IsMoving\",\"velocity\",\"timestamps\",\"timestamp\"]]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9868582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.torch import DeepAREstimator\n",
    "#from gluonts.torch.trainer import Trainer\n",
    "from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
    "\n",
    "\n",
    "def train_and_predict(dataset, estimator):\n",
    "    predictor = estimator.train(dataset)\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=dataset, predictor=predictor\n",
    "    )\n",
    "    evaluator = Evaluator(quantiles=(np.arange(20) / 20.0)[1:])\n",
    "    agg_metrics, item_metrics = evaluator(ts_it, forecast_it, num_series=len(dataset))\n",
    "    return agg_metrics[\"MSE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7aed076",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = DeepAREstimator(\n",
    "    freq=freq, prediction_length=prediction_length\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "591f8faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.dataset.pandas import PandasDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d688cd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "<class 'datetime.time'> is not convertible to datetime, at position 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#ds = PandasDataset(dataset, target=\"x\", freq=freq)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ds \u001b[38;5;241m=\u001b[39m PandasDataset\u001b[38;5;241m.\u001b[39mfrom_long_dataframe(\n\u001b[1;32m      3\u001b[0m     dataframe\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m      4\u001b[0m     item_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m#timestamp=\"timestamps\",\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mns\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/torchmpi/lib/python3.11/site-packages/gluonts/dataset/pandas.py:290\u001b[0m, in \u001b[0;36mPandasDataset.from_long_dataframe\u001b[0;34m(cls, dataframe, item_id, timestamp, static_feature_columns, static_features, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataframe\u001b[38;5;241m.\u001b[39mindex, DatetimeIndexOpsMixin):\n\u001b[1;32m    289\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting index into DatetimeIndex.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 290\u001b[0m     dataframe\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(dataframe\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m static_feature_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    293\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollecting features from columns \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatic_feature_columns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    295\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/torchmpi/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:1059\u001b[0m, in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1057\u001b[0m         result \u001b[38;5;241m=\u001b[39m _convert_and_box_cache(arg, cache_array, name\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1059\u001b[0m         result \u001b[38;5;241m=\u001b[39m convert_listlike(arg, \u001b[38;5;28mformat\u001b[39m, name\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(arg):\n\u001b[1;32m   1061\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1062\u001b[0m         \u001b[38;5;66;03m# error: Argument 1 to \"_maybe_cache\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m         \u001b[38;5;66;03m# \"Union[float, str, datetime, List[Any], Tuple[Any, ...], ExtensionArray,\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m         \u001b[38;5;66;03m# ndarray[Any, Any], Series]\"; expected \"Union[List[Any], Tuple[Any, ...],\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m         \u001b[38;5;66;03m# Union[Union[ExtensionArray, ndarray[Any, Any]], Index, Series], Series]\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/torchmpi/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:455\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _array_strptime_with_fallback(arg, name, utc, \u001b[38;5;28mformat\u001b[39m, exact, errors)\n\u001b[0;32m--> 455\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m objects_to_datetime64ns(\n\u001b[1;32m    456\u001b[0m     arg,\n\u001b[1;32m    457\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[1;32m    458\u001b[0m     yearfirst\u001b[38;5;241m=\u001b[39myearfirst,\n\u001b[1;32m    459\u001b[0m     utc\u001b[38;5;241m=\u001b[39mutc,\n\u001b[1;32m    460\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    461\u001b[0m     allow_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    462\u001b[0m )\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     dta \u001b[38;5;241m=\u001b[39m DatetimeArray(result, dtype\u001b[38;5;241m=\u001b[39mtz_to_dtype(tz_parsed))\n",
      "File \u001b[0;32m~/.conda/envs/torchmpi/lib/python3.11/site-packages/pandas/core/arrays/datetimes.py:2177\u001b[0m, in \u001b[0;36mobjects_to_datetime64ns\u001b[0;34m(data, dayfirst, yearfirst, utc, errors, allow_object)\u001b[0m\n\u001b[1;32m   2174\u001b[0m \u001b[38;5;66;03m# if str-dtype, convert\u001b[39;00m\n\u001b[1;32m   2175\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(data, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mobject_)\n\u001b[0;32m-> 2177\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m tslib\u001b[38;5;241m.\u001b[39marray_to_datetime(\n\u001b[1;32m   2178\u001b[0m     data,\n\u001b[1;32m   2179\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m   2180\u001b[0m     utc\u001b[38;5;241m=\u001b[39mutc,\n\u001b[1;32m   2181\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[1;32m   2182\u001b[0m     yearfirst\u001b[38;5;241m=\u001b[39myearfirst,\n\u001b[1;32m   2183\u001b[0m )\n\u001b[1;32m   2185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2186\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[1;32m   2187\u001b[0m     \u001b[38;5;66;03m#  is in UTC\u001b[39;00m\n\u001b[1;32m   2188\u001b[0m     \u001b[38;5;66;03m# Return i8 values to denote unix timestamps\u001b[39;00m\n\u001b[1;32m   2189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi8\u001b[39m\u001b[38;5;124m\"\u001b[39m), tz_parsed\n",
      "File \u001b[0;32m~/.conda/envs/torchmpi/lib/python3.11/site-packages/pandas/_libs/tslib.pyx:402\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/torchmpi/lib/python3.11/site-packages/pandas/_libs/tslib.pyx:551\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/torchmpi/lib/python3.11/site-packages/pandas/_libs/tslib.pyx:540\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: <class 'datetime.time'> is not convertible to datetime, at position 0"
     ]
    }
   ],
   "source": [
    "#ds = PandasDataset(dataset, target=\"x\", freq=freq)\n",
    "ds = PandasDataset.from_long_dataframe(\n",
    "    dataframe=dataset,\n",
    "    item_id=\"timestamp\",\n",
    "    #timestamp=\"timestamps\",\n",
    "    freq=\"ns\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b09675",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_predict(ds, estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fa76eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Header used on the projects\n",
    "\n",
    "#first the major packages used for math and graphing\n",
    "import numpy as np\n",
    " \n",
    "from cycler import cycler\n",
    "import scipy.special as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Custome graph format style sheet\n",
    "#plt.style.use('Prospectus.mplstyle')\n",
    "\n",
    "#If being run by a seperate file, use the seperate file's graph format and saving paramaeters\n",
    "#otherwise set what is needed\n",
    "if not 'Saving' in locals():\n",
    "    Saving = False\n",
    "if not 'Titles' in locals():\n",
    "    Titles = True\n",
    "if not 'Ledgends' in locals():\n",
    "    Ledgends = True\n",
    "if not 'FFormat' in locals():\n",
    "    FFormat = '.png'\n",
    "\n",
    "#Standard cycle to make black and white images and dashed and line styles\n",
    "default_cycler = (cycler('color', ['0.00', '0.40', '0.60', '0.70']) + cycler(linestyle=['-', '-', '-', '-']))\n",
    "plt.rc('axes', prop_cycle=default_cycler)\n",
    "my_cmap = plt.get_cmap('gray')\n",
    "\n",
    "#Extra Headers:\n",
    "import os as os\n",
    "import pywt as py\n",
    "import statistics as st\n",
    "import os as os\n",
    "import random\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import platform\n",
    "\n",
    "from time import time as ti\n",
    "\n",
    "#import CoreFunctions as cf\n",
    "#from skimage.restoration import denoise_wavelet\n",
    "\n",
    "#Standard Header used on the projects\n",
    "\n",
    "#first the major packages used for math and graphing\n",
    "import numpy as np\n",
    " \n",
    "from cycler import cycler\n",
    "import scipy.special as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Custome graph format style sheet\n",
    "#plt.style.use('Prospectus.mplstyle')\n",
    "\n",
    "#If being run by a seperate file, use the seperate file's graph format and saving paramaeters\n",
    "#otherwise set what is needed\n",
    "if not 'Saving' in locals():\n",
    "    Saving = False\n",
    "if not 'Titles' in locals():\n",
    "    Titles = True\n",
    "if not 'Ledgends' in locals():\n",
    "    Ledgends = True\n",
    "if not 'FFormat' in locals():\n",
    "    FFormat = '.png'\n",
    "\n",
    "#Standard cycle to make black and white images and dashed and line styles\n",
    "default_cycler = (cycler('color', ['0.00', '0.40', '0.60', '0.70']) + cycler(linestyle=['-', '-', '-', '-']))\n",
    "plt.rc('axes', prop_cycle=default_cycler)\n",
    "my_cmap = plt.get_cmap('gray')\n",
    "\n",
    "#Extra Headers:\n",
    "import os as os\n",
    "import pywt as py\n",
    "import statistics as st\n",
    "import os as os\n",
    "import random\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import platform\n",
    "\n",
    "from time import time as ti\n",
    "\n",
    "#import CoreFunctions as cf\n",
    "from skimage.restoration import denoise_wavelet\n",
    "\n",
    "from typing import List, Optional, Callable, Iterable\n",
    "from itertools import islice\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "HostName = platform.node()\n",
    "\n",
    "if HostName == \"Server\":\n",
    "    Computer = \"Desktop\"   \n",
    "elif HostName[-6:] == 'wm.edu':\n",
    "    Computer = \"SciClone\"\n",
    "elif HostName == \"SchoolLaptop\":\n",
    "    Computer = \"LinLap\"\n",
    "elif HostName == \"WTC-TAB-512\":\n",
    "    Computer = \"PortLap\"\n",
    "else:\n",
    "    Computer = \"WinLap\"\n",
    "\n",
    "if Computer == \"SciClone\":\n",
    "    location = '/sciclone/home20/dchendrickson01/image/'\n",
    "elif Computer == \"WinLap\":\n",
    "    location = 'C:\\\\Data\\\\'\n",
    "elif Computer == \"Desktop\":\n",
    "    location = \"E:\\\\Backups\\\\Dan\\\\CraneData\\\\\"\n",
    "elif Computer == \"LinLap\":\n",
    "    location = '/home/dan/Output/'\n",
    "elif Computer == 'PortLap':\n",
    "    location = 'C:\\\\users\\\\dhendrickson\\\\Desktop\\\\AccelData\\\\'\n",
    "\n",
    "if Computer ==  \"SciClone\":\n",
    "    rootfolder = '/sciclone/home20/dchendrickson01/'\n",
    "    folder = '/sciclone/scr10/dchendrickson01/Recordings2/'\n",
    "    imageFolder = '/sciclone/scr10/dchendrickson01/Move3Dprint/'\n",
    "elif Computer == \"Desktop\":\n",
    "    rootfolder = location\n",
    "    folder = rootfolder + \"Recordings2\\\\\"\n",
    "elif Computer ==\"WinLap\":\n",
    "    rootfolder = location\n",
    "    folder = rootfolder + \"Recordings2\\\\\"   \n",
    "elif Computer == \"LinLap\":\n",
    "    rootfolder = '/home/dan/Data/'\n",
    "    folder = rootfolder + 'Recordings2/'\n",
    "elif Computer =='PortLap':\n",
    "    rootfolder = location \n",
    "    folder = rootfolder + 'Recordings2\\\\'\n",
    "\n",
    "Saving = False\n",
    "location = folder\n",
    "Titles = True\n",
    "Ledgends = True\n",
    "\n",
    "f = 0\n",
    "freq = \"2s\"\n",
    "prediction_length=50\n",
    "\n",
    "files = ['230418 recording1.csv','230419 recording1.csv','230420 recording1.csv','230421 recording1.csv',\n",
    "         '230418 recording2.csv','230419 recording2.csv','230420 recording2.csv','230421 recording2.csv']\n",
    "\n",
    "def RollingStdDev(RawData, SmoothData, RollSize = 25):\n",
    "    StdDevs = []\n",
    "    for i in range(RollSize):\n",
    "        Diffs = RawData.iloc[0:i+1]-SmoothData.iloc[0:i+1]\n",
    "        Sqs = Diffs * Diffs\n",
    "        Var = sum(Sqs) / (i+1)\n",
    "        StdDev = np.sqrt(Var)\n",
    "        StdDevs.append(StdDev)\n",
    "    for i in range(len(RawData)-RollSize-1):\n",
    "        j = i + RollSize\n",
    "        Diffs = RawData.iloc[i:j]-SmoothData.iloc[i:j]\n",
    "        Sqs = Diffs * Diffs\n",
    "        Var = sum(Sqs) / RollSize\n",
    "        StdDev = np.sqrt(Var)\n",
    "        StdDevs.append(StdDev)\n",
    "\n",
    "    return StdDevs\n",
    "\n",
    "def RollingSum(Data, Length = 100):\n",
    "    RollSumStdDev = []\n",
    "    for i in range(Length):\n",
    "        RollSumStdDev.append(sum(Data[0:i+1]))\n",
    "    for i in range(len(Data) - Length):\n",
    "        RollSumStdDev.append(sum(Data.iloc[i:i+Length]))\n",
    "    return RollSumStdDev\n",
    "\n",
    "def SquelchPattern(DataSet, StallRange = 5000, SquelchLevel = 0.02):\n",
    "    SquelchSignal = np.ones(len(DataSet))\n",
    "\n",
    "    for i in range(len(DataSet)-2*StallRange):\n",
    "        if np.average(DataSet[i:i+StallRange]) < SquelchLevel:\n",
    "            SquelchSignal[i+StallRange]=0\n",
    "\n",
    "    return SquelchSignal\n",
    "\n",
    "def getVelocity(Acceleration, Timestamps = 0.003, Squelch = [], corrected = 0):\n",
    "    velocity = np.zeros(len(Acceleration))\n",
    "    \n",
    "    Acceleration -= np.average(Acceleration)\n",
    "    \n",
    "    if len(Timestamps) == 1:\n",
    "        dTime = np.ones(len(Acceleration),dtype=float) * Timestamps\n",
    "    elif len(Timestamps) == len(Acceleration):\n",
    "        dTime = np.zeros(len(Timestamps), dtype=float)\n",
    "        dTime[0]=1\n",
    "        for i in range(len(Timestamps)-1):\n",
    "            j = i+1\n",
    "            if Timestamps.iloc[j] > Timestamps.iloc[i]:\n",
    "                dTime[j]=Timestamps.iloc[j]-Timestamps.iloc[i]\n",
    "            else:\n",
    "                dTime[j]=Timestamps.iloc[j]-Timestamps.iloc[i]+10000.0\n",
    "        dTime /= 10000.0\n",
    "\n",
    "    velocity[0] = Acceleration.iloc[0] * (dTime[0])\n",
    "\n",
    "    for i in range(len(Acceleration)-1):\n",
    "        j = i + 1\n",
    "        if corrected ==2:\n",
    "            if Squelch.iloc[j]==0:\n",
    "                velocity[j]=0\n",
    "            else:\n",
    "                velocity[j] = velocity[i] + Acceleration.iloc[j] * dTime[j]                \n",
    "        else:\n",
    "            velocity[j] = velocity[i] + Acceleration.iloc[j] * dTime[j]\n",
    "\n",
    "    if corrected == 1:\n",
    "        PointVairance = velocity[-1:] / len(velocity)\n",
    "        for i in range(len(velocity)):\n",
    "            velocity[i] -=  PointVairance * i\n",
    "    \n",
    "    velocity *= 9.81\n",
    "\n",
    "    return velocity\n",
    "\n",
    "def MakeDTs(Seconds, Miliseconds):\n",
    "    dts = np.zeros(len(Miliseconds), dtype=float)\n",
    "    dts[0]=1\n",
    "    for i in range(len(MiliSeconds)-1):\n",
    "        j = i+1\n",
    "        if Seconds[j]==Seconds[i]:\n",
    "            dts[j]=Miliseconds[j]-Miliseconds[i]\n",
    "        else:\n",
    "            dts[j]=Miliseconds[j]-Miliseconds[i]+1000\n",
    "    dts /= 10000\n",
    "    return dts\n",
    "\n",
    "def MakeDataframe(file):\n",
    "    dataset = pd.read_table(folder+file, delimiter =\", \", header=None, engine='python')\n",
    "\n",
    "    dataset = dataset.rename(columns={0:\"Day\"})\n",
    "    dataset = dataset.rename(columns={1:\"Second\"})\n",
    "    dataset = dataset.rename(columns={2:\"PartSec\"})\n",
    "    dataset = dataset.rename(columns={3:\"p\"})\n",
    "    dataset = dataset.rename(columns={4:\"h\"})\n",
    "    dataset = dataset.rename(columns={5:\"v\"})\n",
    "    dataset = dataset.rename(columns={6:\"Sensor\"})\n",
    "\n",
    "    dataset[['Day','Second']] = dataset[['Day','Second']].apply(lambda x: x.astype(int).astype(str).str.zfill(6))\n",
    "    dataset[['FracSec']] = dataset[['PartSec']].apply(lambda x: x.astype(int).astype(str).str.zfill(4))\n",
    "\n",
    "    dataset[\"timestamp\"] = pd.to_datetime(dataset.Day+dataset.Second\n",
    "                                          +dataset.FracSec, format='%y%m%d%H%M%S%f')\n",
    "    dataset[\"dt\"]=dataset.timestamp - dataset.timestamp.shift(1, fill_value=dataset.timestamp[0])\n",
    "    \n",
    "    dataset[\"timestamps\"] = pd.date_range(dataset.timestamp.iloc[0], dataset.timestamp.iloc[-1],len(dataset.timestamp))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    dataset[\"p\"] = dataset.p - np.average(dataset.p)\n",
    "    dataset[\"h\"] = dataset.h - np.average(dataset.h)\n",
    "    dataset[\"v\"] = dataset.v - np.average(dataset.v)\n",
    "    dataset[\"r\"] = np.sqrt(dataset.p**2 + dataset.h**2 + dataset.v**2)\n",
    "\n",
    "    dataset.index = dataset.timestamps\n",
    "\n",
    "    dataset[\"SmoothV\"] = denoise_wavelet(dataset.v, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "\n",
    "    StdDevsV = RollingStdDev(dataset.v, dataset.SmoothV)\n",
    "    StdDevsV.append(0)\n",
    "    StdDevsV = np.asarray(StdDevsV)\n",
    "    SmoothDevV = denoise_wavelet(StdDevsV, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "\n",
    "    Max = np.max(SmoothDevV)\n",
    "    buckets = int(Max / 0.005) + 1\n",
    "    bins = np.linspace(0,buckets*0.005,buckets+1)\n",
    "    counts, bins = np.histogram(SmoothDevV,bins=bins)\n",
    "\n",
    "    CummCount = 0\n",
    "    HalfWay = 0\n",
    "    for i in range(len(counts)):\n",
    "        CummCount += counts[i]\n",
    "        if CummCount / len(SmoothDevV) >= 0.5:\n",
    "            if HalfWay == 0:\n",
    "                HalfWay = i\n",
    "\n",
    "    SquelchLevel = bins[HalfWay] \n",
    "    dataset[\"IsMoving\"] = SquelchPattern(SmoothDevV, 4000, SquelchLevel)\n",
    "\n",
    "    dataset[\"velocity\"] = getVelocity(dataset.r, dataset.PartSec, dataset.IsMoving, 2)\n",
    "    \n",
    "    #dataset.timestamps.apply(pd.Timestamp.strip('[]'))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d17d0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting data\n"
     ]
    }
   ],
   "source": [
    "print('getting data')\n",
    "#data = Parallel(n_jobs=16)(delayed(MakeDataframe)(file) for file in files)\n",
    "data=MakeDataframe(files[0])\n",
    "dataset = pd.concat(data)[[\"p\",\"h\",\"v\",\"r\",\"IsMoving\",\"velocity\",\"timestamps\",\"timestamp\"]]\n",
    "dataset.head()\n",
    "print('Have data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aba380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7ad9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.torch import DeepAREstimator\n",
    "#from gluonts.torch.trainer import Trainer\n",
    "from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
    "\n",
    "\n",
    "def train_and_predict(dataset, estimator):\n",
    "    predictor = estimator.train(dataset)\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=dataset, predictor=predictor\n",
    "    )\n",
    "    evaluator = Evaluator(quantiles=(np.arange(20) / 20.0)[1:])\n",
    "    agg_metrics, item_metrics = evaluator(ts_it, forecast_it, num_series=len(dataset))\n",
    "    return agg_metrics[\"MSE\"]\n",
    "\n",
    "estimator = DeepAREstimator(\n",
    "    freq=freq, prediction_length=prediction_length\n",
    ")\n",
    "\n",
    "print('Model set up')\n",
    "\n",
    "#dataset.timestamps.apply(pd.Timestamp.strip('[]'))\n",
    "\n",
    "\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "\n",
    "#ds = PandasDataset(dataset, target=\"x\", freq=freq)\n",
    "ds = PandasDataset.from_long_dataframe(\n",
    "    dataframe=dataset,\n",
    "    item_id=\"timestamps\",\n",
    "    #timestamp=\"timestamps\",\n",
    "    freq=\"ns\",\n",
    ")\n",
    "\n",
    "train_and_predict(ds, estimator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchmpi",
   "language": "python",
   "name": "torchmpi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
