{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Together Now\n",
    "## Fingerprints for many wavelets, clustering, then sorting\n",
    "\n",
    "Combines the code from 10, 11, 12.  Temporary in Jupyter Notebook, probably going to be converted to .py so it can run headless once it is trustworthy.  Tested on 3 files, going to go to 16.  Will do same stack, 2 cranes before and after tamping, 4 days of each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Header used on the projects\n",
    "\n",
    "#first the major packages used for math and graphing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "import scipy.special as sp\n",
    "\n",
    "#Standard cycle to make black and white images and dashed and line styles\n",
    "default_cycler = (cycler('color', ['0.00', '0.40', '0.60', '0.70']) + cycler(linestyle=['-', '-', '-', '-']))\n",
    "plt.rc('axes', prop_cycle=default_cycler)\n",
    "my_cmap = plt.get_cmap('gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wavelet Imports Extra Headers:\n",
    "import os as os\n",
    "import pywt as py\n",
    "import statistics as st\n",
    "import os as os\n",
    "import random\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import platform\n",
    "\n",
    "from time import time as ti\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CoreFunctions as cf\n",
    "from skimage.restoration import denoise_wavelet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-03 16:28:57.968821: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# ML Imports Imports\n",
    "#from keras.preprocessing import image\n",
    "import keras.utils as image\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import os, shutil, glob, os.path\n",
    "from PIL import Image as pil_image\n",
    "image.LOAD_TRUNCATED_IMAGES = True \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Choosing Platform\n",
    " Working is beinging conducted on several computers, and author needs to be able to run code on all without rewriting..  This segment of determines which computer is being used, and sets the directories accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HostName = platform.node()\n",
    "\n",
    "if HostName == \"Server\":\n",
    "    Computer = \"Desktop\"   \n",
    "elif HostName[-6:] == 'wm.edu':\n",
    "    Computer = \"SciClone\"\n",
    "elif HostName == \"SchoolLaptop\":\n",
    "    Computer = \"LinLap\"\n",
    "elif HostName == \"WTC-TAB-512\":\n",
    "    Computer = \"PortLap\"\n",
    "else:\n",
    "    Computer = \"WinLap\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Computer == \"SciClone\":\n",
    "    location = '/sciclone/home20/dchendrickson01/image/'\n",
    "elif Computer == \"WinLap\":\n",
    "    location = 'C:\\\\Data\\\\'\n",
    "elif Computer == \"Desktop\":\n",
    "    location = \"E:\\\\Backups\\\\Dan\\\\CraneData\\\\\"\n",
    "elif Computer == \"LinLap\":\n",
    "    location = '/home/dan/Output/'\n",
    "elif Computer == 'PortLap':\n",
    "    location = 'C:\\\\users\\\\dhendrickson\\\\Desktop\\\\AccelData\\\\'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Computer ==  \"SciClone\":\n",
    "    rootfolder = '/sciclone/home20/dchendrickson01/'\n",
    "    folder = '/sciclone/scr10/dchendrickson01/Recordings2/'\n",
    "    imageFolder = '/sciclone/scr10/dchendrickson01/Move3Dprint/'\n",
    "elif Computer == \"Desktop\":\n",
    "    rootfolder = location\n",
    "    folder = rootfolder + \"Recordings2\\\\\"\n",
    "elif Computer ==\"WinLap\":\n",
    "    rootfolder = location\n",
    "    folder = rootfolder + \"Recordings2\\\\\"   \n",
    "elif Computer == \"LinLap\":\n",
    "    rootfolder = '/home/dan/Data/'\n",
    "    folder = rootfolder + 'Recordings2/'\n",
    "elif Computer =='PortLap':\n",
    "    rootfolder = location \n",
    "    folder = rootfolder + 'Recordings2\\\\'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "import sys, os\n",
    "\n",
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:  \n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maunally chooseing before and after tamping for same track\n",
    "\n",
    "files = ['221206 recording1.csv','221207 recording1.csv','221208 recording1.csv','221209 recording1.csv',\n",
    "         '221206 recording2.csv','221207 recording2.csv','221208 recording2.csv','221209 recording2.csv',\n",
    "         '230418 recording1.csv','230419 recording1.csv','230420 recording1.csv','230421 recording1.csv',\n",
    "         '230418 recording2.csv','230419 recording2.csv','230420 recording2.csv','230421 recording2.csv']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClustersWanted = 11\n",
    "scales= 100\n",
    "skips = 1\n",
    "minLength = 750\n",
    "\n",
    "subfolder ='wvltSort/'\n",
    "#subfolder = 'scaleSort/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Specific Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RollingStdDev(RawData, SmoothData, RollSize = 25):\n",
    "    StdDevs = []\n",
    "    for i in range(RollSize):\n",
    "        Diffs = RawData[0:i+1]-SmoothData[0:i+1]\n",
    "        Sqs = Diffs * Diffs\n",
    "        Var = sum(Sqs) / (i+1)\n",
    "        StdDev = np.sqrt(Var)\n",
    "        StdDevs.append(StdDev)\n",
    "    for i in range(len(RawData)-RollSize-1):\n",
    "        j = i + RollSize\n",
    "        Diffs = RawData[i:j]-SmoothData[i:j]\n",
    "        Sqs = Diffs * Diffs\n",
    "        Var = sum(Sqs) / RollSize\n",
    "        StdDev = np.sqrt(Var)\n",
    "        StdDevs.append(StdDev)  \n",
    "    \n",
    "    return StdDevs\n",
    "\n",
    "def RollingSum(Data, Length = 100):\n",
    "    RollSumStdDev = []\n",
    "    for i in range(Length):\n",
    "        RollSumStdDev.append(sum(Data[0:i+1]))\n",
    "    for i in range(len(Data) - Length):\n",
    "        RollSumStdDev.append(sum(Data[i:i+Length]))\n",
    "    return RollSumStdDev\n",
    "\n",
    "def SquelchPattern(DataSet, StallRange = 5000, SquelchLevel = 0.02):\n",
    "    SquelchSignal = np.ones(len(DataSet))\n",
    "\n",
    "    for i in range(len(DataSet)-2*StallRange):\n",
    "        if np.average(DataSet[i:i+StallRange]) < SquelchLevel:\n",
    "            SquelchSignal[i+StallRange]=0\n",
    "\n",
    "    return SquelchSignal\n",
    "\n",
    "def getVelocity(Acceleration, Timestamps = 0.003, Squelch = [], corrected = 0):\n",
    "    velocity = np.zeros(len(Acceleration))\n",
    "    \n",
    "    Acceleration -= np.average(Acceleration)\n",
    "    \n",
    "    if len(Timestamps) == 1:\n",
    "        dTime = np.ones(len(Acceleration),dtype=float) * Timestamps\n",
    "    elif len(Timestamps) == len(Acceleration):\n",
    "        dTime = np.zeros(len(Timestamps), dtype=float)\n",
    "        dTime[0]=1\n",
    "        for i in range(len(Timestamps)-1):\n",
    "            j = i+1\n",
    "            if Timestamps[j] > Timestamps[i]:\n",
    "                dTime[j]=Timestamps[j]-Timestamps[i]\n",
    "            else:\n",
    "                dTime[j]=Timestamps[j]-Timestamps[i]+10000.0\n",
    "        dTime /= 10000.0\n",
    "\n",
    "    velocity[0] = Acceleration[0] * (dTime[0])\n",
    "\n",
    "    for i in range(len(Acceleration)-1):\n",
    "        j = i + 1\n",
    "        if corrected ==2:\n",
    "            if Squelch[j]==0:\n",
    "                velocity[j]=0\n",
    "            else:\n",
    "                velocity[j] = velocity[i] + Acceleration[j] * dTime[j]                \n",
    "        else:\n",
    "            velocity[j] = velocity[i] + Acceleration[j] * dTime[j]\n",
    "\n",
    "    if corrected == 1:\n",
    "        PointVairance = velocity[-1:] / len(velocity)\n",
    "        for i in range(len(velocity)):\n",
    "            velocity[i] -=  PointVairance * i\n",
    "    \n",
    "    velocity *= 9.81\n",
    "\n",
    "    return velocity\n",
    "\n",
    "def MakeDTs(Seconds, Miliseconds):\n",
    "    dts = np.zeros(len(Miliseconds), dtype=float)\n",
    "    dts[0]=1\n",
    "    for i in range(len(MiliSeconds)-1):\n",
    "        j = i+1\n",
    "        if Seconds[j]==Seconds[i]:\n",
    "            dts[j]=Miliseconds[j]-Miliseconds[i]\n",
    "        else:\n",
    "            dts[j]=Miliseconds[j]-Miliseconds[i]+1000\n",
    "    dts /= 10000\n",
    "    return dts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smooth = cf.Smoothing(ODataSet[:,3],2) #,50)\n",
    "def DeviationVelocity(file):\n",
    "    if file[-3:] =='csv':\n",
    "        ODataSet = np.genfromtxt(open(folder+file,'r'), delimiter=',',skip_header=0,missing_values=0,invalid_raise=False)\n",
    "        SmoothX = denoise_wavelet(ODataSet[:,3], method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        SmoothY = denoise_wavelet(ODataSet[:,4], method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        SmoothZ = denoise_wavelet(ODataSet[:,5], method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        SmoothX -= np.average(SmoothX)\n",
    "        SmoothY -= np.average(SmoothY)\n",
    "        SmoothZ -= np.average(SmoothZ)\n",
    "        StdDevsX = RollingStdDev(ODataSet[:,3],SmoothX)\n",
    "        StdDevsX.append(0)\n",
    "        StdDevsX = np.asarray(StdDevsX)\n",
    "        SmoothDevX = denoise_wavelet(StdDevsX, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        SquelchSignal = SquelchPattern(SmoothDevX, 2000, 0.03)\n",
    "        #Velocity = getVelocity(ODataSet[:,3], ODataSet[:,2],SquelchSignal, 2)\n",
    "        #Velocity = np.asarray(Velocity)\n",
    "        MoveMatrix = np.matrix([SmoothX, SmoothY, SmoothZ])\n",
    "        return [SquelchSignal,MoveMatrix,SmoothDevX,file[:-3]]\n",
    "    else:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SepreateMovements(SquelchSignal, RawData, FileName):\n",
    "    Moves= []\n",
    "    MoveNames = []\n",
    "    Move = np.zeros((1,3), dtype=float)\n",
    "    i = 0\n",
    "    for j in range(len(SquelchSignal)-1):\n",
    "        if SquelchSignal[j] == 1:\n",
    "            try:\n",
    "                Move = np.concatenate((Move, RawData[j,:]), axis=0)\n",
    "            except:\n",
    "                print(j)\n",
    "            if SquelchSignal[j+1] == 0:\n",
    "                #Move = np.matrix(Move)\n",
    "                Moves.append(Move)\n",
    "                MoveNames.append(FileName + str(i).zfill(3))\n",
    "                i+=1\n",
    "                Move = np.zeros((1,3), dtype=float)\n",
    "                #Move[0,2]=0\n",
    "    Moves.append(Move)\n",
    "    MoveNames.append(FileName + str(i).zfill(3))\n",
    "    return Moves, MoveNames\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitLong(Moves, maxLength = 4000, minLength = 1000, MoveNames = []):\n",
    "    if len(MoveNames) <=1:\n",
    "        MoveNames = ['null'  for x in range(len(Moves))]\n",
    "    Xmoves = []\n",
    "    Xnames = []\n",
    "    for i in range(len(Moves)):\n",
    "        if np.shape(move)[0] > maxLength: \n",
    "            Xmoves.append(Moves[i][:int(len(Moves[i])/2),:])\n",
    "            Xnames.append(MoveNames[i] + 'a')\n",
    "            Xmoves.append(Moves[i][int(len(Moves[i])/2):,:])\n",
    "            Xnames.append(MoveNames[i] + 'b')\n",
    "        else:\n",
    "            if np.shape(Moves[i])[0] < minLength:\n",
    "                pass\n",
    "            else:\n",
    "                Xmoves.append(Moves[i])\n",
    "                Xnames.append(MoveNames[i])\n",
    "    return Xmoves, Xnames\n",
    "\n",
    "def findMaxLength(Moves):\n",
    "    maxLength = 0\n",
    "    LongMove = 0\n",
    "    for i in range(len(Moves)):\n",
    "        if np.shape(Moves[i])[0] > maxLength: \n",
    "            maxLength =  np.shape(Moves[i])[0]\n",
    "            LongMove = i\n",
    "    return maxLength, LongMove\n",
    "\n",
    "def findMinLength(Moves):\n",
    "    minLength = 9999999\n",
    "    SmallMove = 0\n",
    "    for i in range(len(Moves)):\n",
    "        if np.shape(Moves[i])[0] < minLength: \n",
    "            minLength =  np.shape(Moves[i])[0]\n",
    "            SmallMove = i\n",
    "    return minLength, SmallMove\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as ss\n",
    "def MakeSpectrogramImages(data, title, something=300, nperseg = 512, novrelap=256, folder=imageFolder):\n",
    "    f, t, Szz = ss.signal.spectrogram(data,something,nperseg = nperseg, noverlap=novrelap)\n",
    "    fig = plt.figure(figsize=(8,3), dpi=800)\n",
    "    ax = plt.axes()\n",
    "    ax.set_axis_off()\n",
    "    plt.pcolormesh(t, f, Szz[0],cmap='gist_ncar')\n",
    "    plt.savefig(folder+'spec/'+title+'.png',bbox_inches='tight', pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sortClusters(folder):\n",
    "    \n",
    "    filelist = glob.glob(os.path.join(folder, '*.png'))\n",
    "    filelist.sort()\n",
    "    \n",
    "    if len(filelist) > 10:\n",
    "    \n",
    "        with suppress_stdout():\n",
    "            model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "        sampleName = folder.split('/')[-2]\n",
    "        print(sampleName)\n",
    "\n",
    "        if os.path.exists(imageFolder+subfolder+sampleName+'/') == False:\n",
    "            os.mkdir(imageFolder+subfolder+sampleName+'/')\n",
    "\n",
    "        # Variables\n",
    "        imdir = folder # DIR containing images\n",
    "        targetdir =imageFolder+subfolder+sampleName+'/' # DIR to copy clustered images to\n",
    "        number_clusters = ClustersWanted\n",
    "\n",
    "        # Loop over files and get features\n",
    "\n",
    "        featurelist = []\n",
    "        for i, imagepath in enumerate(filelist):\n",
    "            #try:\n",
    "            #if i %100 == 0 : print(\"    Status: %s / %s\" %(i, len(filelist)), end=\"\\r\")\n",
    "            img = image.load_img(imagepath, target_size=(224, 448))\n",
    "            img_data = image.img_to_array(img)\n",
    "            img_data = np.expand_dims(img_data, axis=0)\n",
    "            img_data = preprocess_input(img_data)\n",
    "            with suppress_stdout():\n",
    "                features = np.array(model.predict(img_data))\n",
    "            featurelist.append(features.flatten())\n",
    "            #except:\n",
    "            #    continue\n",
    "\n",
    "            # Clustering\n",
    "        kmeans = KMeans(n_clusters=number_clusters, random_state=0).fit(np.array(featurelist))\n",
    "\n",
    "        # Copy images renamed by cluster \n",
    "        # Check if target dir exists\n",
    "        try:\n",
    "            os.makedirs(targetdir)\n",
    "        except OSError:\n",
    "            pass\n",
    "        # Copy with cluster name\n",
    "\n",
    "        for i, m in enumerate(kmeans.labels_):\n",
    "            try:\n",
    "                shutil.copy(filelist[i], targetdir + str(m) + \"_\" + filelist[i].split('/')[-1])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LoopFiles = 3\n",
    "loops = int(len(files) / LoopFiles) \n",
    "if len(files)%LoopFiles != 0:\n",
    "    loops += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SquelchSignal = []\n",
    "RawData=[]\n",
    "OrderedFileNames=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "st = ti()\n",
    "\n",
    "for k in range(loops):\n",
    "    if k == loops -1:\n",
    "        tfiles = files[k*LoopFiles:]\n",
    "    else:\n",
    "        tfiles = files[k*LoopFiles:(k+1)*LoopFiles]\n",
    "    Results = Parallel(n_jobs=LoopFiles)(delayed(DeviationVelocity)(file) for file in tfiles)\n",
    "    \n",
    "    for i in range(len(Results)):       \n",
    "        SquelchSignal.append(Results[i][0])\n",
    "        RawData.append(np.matrix(Results[i][1]).T)\n",
    "        OrderedFileNames.append(Results[i][3])\n",
    "    print(k, np.shape(Results), (ti()-st)/60.0)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MoveData = Parallel(n_jobs=31)(delayed(SepreateMovements)(SquelchSignal[i], RawData[i], OrderedFileNames[i])\n",
    "                                       for i in range(len(RawData)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Movements = []\n",
    "GroupNames = []\n",
    "for move in MoveData:\n",
    "    Movements.append(move[0])\n",
    "    GroupNames.append(move[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Moves=[]\n",
    "for Groups in Movements:\n",
    "    for Move in Groups:\n",
    "        Moves.append(Move)\n",
    "\n",
    "MoveNames = []\n",
    "for Groups in GroupNames:\n",
    "    for name in Groups:\n",
    "        MoveNames.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "del SquelchSignal\n",
    "del RawData\n",
    "del Movements\n",
    "del GroupNames\n",
    "del MoveData\n",
    "del OrderedFileNames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longMove, MoveNumb = findMaxLength(Moves)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WvltFam = py.families()\n",
    "Wvlts = []\n",
    "for Fam in WvltFam:\n",
    "    temp = py.wavelist(Fam)\n",
    "    for wvlt in temp:\n",
    "        Wvlts.append(wvlt)\n",
    "        \n",
    "trys = Wvlts\n",
    "\n",
    "trys.append('beta')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Moves, MoveNames = splitLong(Moves, longMove+1, minLength, MoveNames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StorageFolder = imageFolder + 'wvltTest/'\n",
    "#StorageFolder = imageFolder + 'scaleTest/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 0\n",
    "\n",
    "#wvlt = 'beta'\n",
    "\n",
    "for tri in trys:\n",
    "\n",
    "    if os.path.exists(StorageFolder+tri+'/') == False:\n",
    "        os.mkdir(StorageFolder+tri+'/')\n",
    "\n",
    "    #FPimages = Parallel(n_jobs=60)(delayed(cf.makeMPFast)(Moves[i].T, wvlt, scales, skips,StorageFolder+tri + '/Move '+ MoveNames[i]) for i in range(len(Moves)))\n",
    "    FPimages = Parallel(n_jobs=60)(delayed(cf.makeMatrixImages)(Moves[i].T, wvlt, scales, skips,StorageFolder+tri + '/Move '+ MoveNames[i]) for i in range(len(Moves)))\n",
    "##FPimages = Parallel(n_jobs=60)(delayed(cf.makeMPFast)(Moves[MoveNum].T, tri, scales, skips, imageFolder+'wvltTest/' + tri + '_LongMove') for tri in trys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(StorageFolder+'spec/') == False:\n",
    "        os.mkdir(StorageFolder+'spec/')\n",
    "\n",
    "FPimages = Parallel(n_jobs=60)(delayed(MakeSpectrogramImages)(Moves[i].T, 'Move '+ MoveNames[i], 300, 512, 505, StorageFolder) for i in range(len(Moves)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del FPimages\n",
    "del Moves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Started the Unsupervised Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = glob.glob(StorageFolder + '*/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxSameTime = 2\n",
    "J = int(len(folders) / MaxSameTime)\n",
    "if len(folders) % MaxSameTime != 0:\n",
    "    J +=1\n",
    "    #sorting = Parallel(n_jobs=3)(delayed(sortClusters)(folder) for folder in folders[53:59])\n",
    "for j in range(J):\n",
    "    if j!=J:\n",
    "        sorting = Parallel(n_jobs=3)(delayed(sortClusters)(folder) for folder in folders[MaxSameTime * j:MaxSameTime*(j+1)])\n",
    "    else:\n",
    "        sorting = Parallel(n_jobs=3)(delayed(sortClusters)(folder) for folder in folders[MaxSameTime * j:])\n",
    "    print(str(j) + ' of ' + str(J))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for comaprison of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetResultsDataFrame(folder):\n",
    "    files = os.listdir(folder)\n",
    "    if len(files) > 2:\n",
    "        TypeName = folder.split('/')[-2]\n",
    "        Results = []\n",
    "        for file in files:\n",
    "            Group = int(file.split('_')[0])\n",
    "            Move = file.split('_')[1][5:-4]\n",
    "            Results.append([Group,Move])\n",
    "        Results = np.matrix(Results)\n",
    "        temp_dict = {\n",
    "                \"MoveName\" : np.asarray(Results[:,1]).flatten(),\n",
    "                TypeName: np.asarray(Results[:,0]).flatten()\n",
    "            }\n",
    "        DataSet = pd.DataFrame(temp_dict)\n",
    "        return DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#targetdir = imageFolder+'scaleSort/'\n",
    "targetdir = imageFolder+'wvltSort/'\n",
    "\n",
    "folders = glob.glob(targetdir+'*/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RealReturns = False\n",
    "for folder in folders:\n",
    "    FolderDF = GetResultsDataFrame(folder)\n",
    "    #print(folder.split('/')[-2], len(FolderDF))\n",
    "    if len(FolderDF) > 2:\n",
    "        if RealReturns == True:\n",
    "            AllData = pd.merge(AllData, FolderDF, on ='MoveName', how =\"outer\")\n",
    "        else:\n",
    "            AllData = FolderDF\n",
    "            RealReturns = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllData.to_csv('ClusteredResults2.csv')\n",
    "#AllData.to_csv('ScalesResults.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Simple",
   "language": "python",
   "name": "simple"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
