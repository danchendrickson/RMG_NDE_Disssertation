{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at accelerometer data \n",
    "\n",
    "Finding Zero velocity times by rail axis acceleration noise levels, making summary statistics for the noise levels across the whole day files.  Spot check graphs to see what works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Header used on the projects\n",
    "\n",
    "#first the major packages used for math and graphing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "import scipy.special as sp\n",
    "\n",
    "#Custome graph format style sheet\n",
    "#plt.style.use('Prospectus.mplstyle')\n",
    "\n",
    "#If being run by a seperate file, use the seperate file's graph format and saving paramaeters\n",
    "#otherwise set what is needed\n",
    "if not 'Saving' in locals():\n",
    "    Saving = False\n",
    "if not 'Titles' in locals():\n",
    "    Titles = True\n",
    "if not 'Ledgends' in locals():\n",
    "    Ledgends = True\n",
    "if not 'FFormat' in locals():\n",
    "    FFormat = '.png'\n",
    "\n",
    "#Standard cycle to make black and white images and dashed and line styles\n",
    "default_cycler = (cycler('color', ['0.00', '0.40', '0.60', '0.70']) + cycler(linestyle=['-', '--', ':', '-.']))\n",
    "plt.rc('axes', prop_cycle=default_cycler)\n",
    "my_cmap = plt.get_cmap('gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra Headers:\n",
    "import os as os\n",
    "import pywt as py\n",
    "import statistics as st\n",
    "import os as os\n",
    "import random\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import platform\n",
    "\n",
    "from time import time as ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CoreFunctions as cf\n",
    "from skimage.restoration import denoise_wavelet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing Platform\n",
    "Working is beinging conducted on several computers, and author needs to be able to run code on all without rewriting..  This segment of determines which computer is being used, and sets the directories accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "HostName = platform.node()\n",
    "\n",
    "if HostName == \"Server\":\n",
    "    Computer = \"Desktop\"   \n",
    "elif HostName[-6:] == 'wm.edu':\n",
    "    Computer = \"SciClone\"\n",
    "elif HostName == \"SchoolLaptop\":\n",
    "    Computer = \"LinLap\"\n",
    "elif HostName == \"WTC-TAB-512\":\n",
    "    Computer = \"PortLap\"\n",
    "else:\n",
    "    Computer = \"WinLap\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Computer == \"SciClone\":\n",
    "    location = '/sciclone/home20/dchendrickson01/image/'\n",
    "elif Computer == \"WinLap\":\n",
    "    location = 'C:\\\\Data\\\\'\n",
    "elif Computer == \"Desktop\":\n",
    "    location = \"E:\\\\Backups\\\\Dan\\\\CraneData\\\\\"\n",
    "elif Computer == \"LinLap\":\n",
    "    location = '/home/dan/Output/'\n",
    "elif Computer == 'PortLap':\n",
    "    location = 'C:\\\\users\\\\dhendrickson\\\\Desktop\\\\AccelData\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Computer ==  \"SciClone\":\n",
    "    rootfolder = '/sciclone/home20/dchendrickson01/'\n",
    "    folder = '/sciclone/scr10/dchendrickson01/Recordings2/'\n",
    "elif Computer == \"Desktop\":\n",
    "    rootfolder = location\n",
    "    folder = rootfolder + \"Recordings2\\\\\"\n",
    "elif Computer ==\"WinLap\":\n",
    "    rootfolder = location\n",
    "    folder = rootfolder + \"Recordings2\\\\\"   \n",
    "elif Computer == \"LinLap\":\n",
    "    rootfolder = '/home/dan/Data/'\n",
    "    folder = rootfolder + 'Recordings2/'\n",
    "elif Computer =='PortLap':\n",
    "    rootfolder = location \n",
    "    folder = rootfolder + 'Recordings2\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(folder)\n",
    "files=files[39:41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Saving = False\n",
    "location = folder\n",
    "Titles = True\n",
    "Ledgends = True\n",
    "\n",
    "f = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RollingStdDev(RawData, SmoothData, RollSize = 25):\n",
    "    StdDevs = []\n",
    "    for i in range(RollSize):\n",
    "        Diffs = RawData[0:i+1]-SmoothData[0:i+1]\n",
    "        Sqs = Diffs * Diffs\n",
    "        Var = sum(Sqs) / (i+1)\n",
    "        StdDev = np.sqrt(Var)\n",
    "        StdDevs.append(StdDev)\n",
    "    for i in range(len(RawData)-RollSize-1):\n",
    "        j = i + RollSize\n",
    "        Diffs = RawData[i:j]-SmoothData[i:j]\n",
    "        Sqs = Diffs * Diffs\n",
    "        Var = sum(Sqs) / RollSize\n",
    "        StdDev = np.sqrt(Var)\n",
    "        StdDevs.append(StdDev)  \n",
    "    \n",
    "    return StdDevs\n",
    "\n",
    "def RollingSum(Data, Length = 100):\n",
    "    RollSumStdDev = []\n",
    "    for i in range(Length):\n",
    "        RollSumStdDev.append(sum(Data[0:i+1]))\n",
    "    for i in range(len(Data) - Length):\n",
    "        RollSumStdDev.append(sum(Data[i:i+Length]))\n",
    "    return RollSumStdDev\n",
    "\n",
    "def SquelchPattern(DataSet, StallRange = 5000, SquelchLevel = 0.0086):\n",
    "    SquelchSignal = np.ones(len(DataSet))\n",
    "\n",
    "    for i in range(len(DataSet)-2*StallRange):\n",
    "        if np.average(DataSet[i:i+StallRange]) < SquelchLevel:\n",
    "            SquelchSignal[i+StallRange]=0\n",
    "\n",
    "    return SquelchSignal\n",
    "\n",
    "def getVelocity(Acceleration, Timestamps = 0.003, Squelch = [], corrected = 0):\n",
    "    velocity = np.zeros(len(Acceleration))\n",
    "    \n",
    "    Acceleration -= np.average(Acceleration)\n",
    "    \n",
    "    if len(Timestamps) == 1:\n",
    "        dTime = np.ones(len(Acceleration),dtype=float) * Timestamps\n",
    "    elif len(Timestamps) == len(Acceleration):\n",
    "        dTime = np.zeros(len(Timestamps), dtype=float)\n",
    "        dTime[0]=1\n",
    "        for i in range(len(Timestamps)-1):\n",
    "            j = i+1\n",
    "            if Timestamps[j] > Timestamps[i]:\n",
    "                dTime[j]=Timestamps[j]-Timestamps[i]\n",
    "            else:\n",
    "                dTime[j]=Timestamps[j]-Timestamps[i]+10000.0\n",
    "        dTime /= 10000.0\n",
    "\n",
    "    velocity[0] = Acceleration[0] * (dTime[0])\n",
    "\n",
    "    for i in range(len(Acceleration)-1):\n",
    "        j = i + 1\n",
    "        if corrected ==2:\n",
    "            if Squelch[j]==0:\n",
    "                velocity[j]=0\n",
    "            else:\n",
    "                velocity[j] = velocity[i] + Acceleration[j] * dTime[j]                \n",
    "        else:\n",
    "            velocity[j] = velocity[i] + Acceleration[j] * dTime[j]\n",
    "\n",
    "    if corrected == 1:\n",
    "        PointVairance = velocity[-1:] / len(velocity)\n",
    "        for i in range(len(velocity)):\n",
    "            velocity[i] -=  PointVairance * i\n",
    "    \n",
    "    velocity *= 9.81\n",
    "\n",
    "    return velocity\n",
    "\n",
    "def MakeDTs(Seconds, Miliseconds):\n",
    "    dts = np.zeros(len(Miliseconds), dtype=float)\n",
    "    dts[0]=1\n",
    "    for i in range(len(MiliSeconds)-1):\n",
    "        j = i+1\n",
    "        if Seconds[j]==Seconds[i]:\n",
    "            dts[j]=Miliseconds[j]-Miliseconds[i]\n",
    "        else:\n",
    "            dts[j]=Miliseconds[j]-Miliseconds[i]+1000\n",
    "    dts /= 10000\n",
    "    return dts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smooth = cf.Smoothing(ODataSet[:,3],2) #,50)\n",
    "def DeviationVelocity(file):\n",
    "    if file[-3:] =='csv':\n",
    "        ODataSet = np.genfromtxt(open(folder+file,'r'), delimiter=',',skip_header=0,missing_values=0,invalid_raise=False)\n",
    "        SmoothX = denoise_wavelet(ODataSet[:,3], method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        SmoothX -= np.average(SmoothX)\n",
    "        StdDevsX = RollingStdDev(ODataSet[:,3],SmoothX)\n",
    "        StdDevsX.append(0)\n",
    "        StdDevsX = np.asarray(StdDevsX)\n",
    "        SmoothDevX = denoise_wavelet(StdDevsX, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        SquelchSignal = SquelchPattern(SmoothDevX, 1500, 0.04)\n",
    "        Velocity = getVelocity(ODataSet[:,3], ODataSet[:,2],SquelchSignal, 2)\n",
    "        Velocity = np.asarray(Velocity)\n",
    "        return [Velocity, StdDevsX, SmoothDevX, SquelchSignal,SmoothX,ODataSet[:,3:]]\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files = fi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maunally chooseing before and after tamping for same track\n",
    "\n",
    "files = ['230103 recording3.csv','230104 recording3.csv','230105 recording3.csv','230106 recording3.csv' ,\n",
    "         '230103 recording4.csv','230104 recording4.csv','230105 recording4.csv','230106 recording4.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LoopFiles = 4\n",
    "loops = int(len(files) / LoopFiles) \n",
    "if len(files)%LoopFiles != 0:\n",
    "    loops += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Movements = []\n",
    "Velocities = []\n",
    "StdDevsX = []\n",
    "SmoothDevX = []\n",
    "SquelchSignal = []\n",
    "Accels=[]\n",
    "RawData=[]\n",
    "\n",
    "st = ti()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m Results \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mLoopFiles)(delayed(DeviationVelocity)(file) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m tfiles)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(Results)):       \n\u001b[0;32m----> 9\u001b[0m     Velocities\u001b[38;5;241m.\u001b[39mappend(\u001b[43mResults\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     10\u001b[0m     StdDevsX\u001b[38;5;241m.\u001b[39mappend(Results[i][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     11\u001b[0m     SmoothDevX\u001b[38;5;241m.\u001b[39mappend(Results[i][\u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "for k in range(loops):\n",
    "    if k == loops -1:\n",
    "        tfiles = files[k*LoopFiles:]\n",
    "    else:\n",
    "        tfiles = files[k*LoopFiles:(k+1)*LoopFiles]\n",
    "    Results = Parallel(n_jobs=LoopFiles)(delayed(DeviationVelocity)(file) for file in tfiles)\n",
    "    \n",
    "    for i in range(len(Results)):       \n",
    "        Velocities.append(Results[i][0])\n",
    "        StdDevsX.append(Results[i][1])\n",
    "        SmoothDevX.append(Results[i][2])\n",
    "        SquelchSignal.append(Results[i][3])\n",
    "        Accels.append(Results[i][4])\n",
    "        RawData.append(Results[i][5])\n",
    "    \n",
    "    print(k, np.shape(Results), (ti()-st)/60.0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaner=(np.linspace(0,1,len(Velocities[3])))*np.max(Velocities[3])\n",
    "#Velocities[3] /= Cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(6.67,3.75),dpi=800,linewidth=0.5) \n",
    "\n",
    "PlotLength = min(len(Velocities[f]), len(StdDevsX[f]))\n",
    "v = SquelchSignal[f][:PlotLength]\n",
    "sd = SmoothDevX[f][:PlotLength]\n",
    "\n",
    "ax1.set_xlabel('Time') \n",
    "ax1.set_ylabel('Velocity', color = 'red') \n",
    "ax1.plot(range(PlotLength), v, color = 'red', linestyle = 'dashed', label='Velocity' )\n",
    "ax1.tick_params(axis ='y', labelcolor = 'red') \n",
    "#plt.ylim(-6,6)\n",
    "legend_1 = ax1.legend(loc=2)\n",
    "legend_1.remove()\n",
    "\n",
    "# Adding Twin Axes\n",
    "\n",
    "ax2 = ax1.twinx() \n",
    "\n",
    "ax2.set_ylabel('Acceleration', color = 'blue') \n",
    "ax2.plot(range(PlotLength), sd, color = 'blue', label='Std Dev of Acceleration') \n",
    "ax2.tick_params(axis ='y', labelcolor = 'blue') \n",
    "#plt.ylim(0.0,0.6)\n",
    "ax2.legend(loc=1)\n",
    "ax2.add_artist(legend_1)\n",
    "# Show plot\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SepreateMovements(SquelchSignal, RawData):\n",
    "    Moves=[]\n",
    "    Move = []\n",
    "    for j in range(len(SquelchSignal)-1):\n",
    "        if SquelchSignal[j] == 1:\n",
    "            Move.append(RawData[j])\n",
    "            if SquelchSignal[j+1] == 0:\n",
    "                Move = np.matrix(Move)\n",
    "                Moves.append(Move)\n",
    "                Move = []\n",
    "    Moves.append(Move)\n",
    "    return Moves\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Movements = Parallel(n_jobs=LoopFiles)(delayed(SepreateMovements)(SquelchSignal[i], RawData[i])\n",
    "                                       for i in range(len(SquelchSignal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Moves=[]\n",
    "for Groups in Movements:\n",
    "    for Move in Groups:\n",
    "        Moves.append(Move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=np.matrix(Moves[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MoveNum = 1007\n",
    "fig = plt.figure()\n",
    "plt.plot(Moves[MoveNum][:,0], label='x')\n",
    "plt.plot(Moves[MoveNum][:,1], label='y')\n",
    "plt.plot(Moves[MoveNum][:,2], label='z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xmoves = []\n",
    "Ymoves = []\n",
    "Zmoves = []\n",
    "\n",
    "for move in Moves:\n",
    "    g = np.shape(move)[0]\n",
    "    if g > 1000:\n",
    "        xmove = []\n",
    "        ymove = []\n",
    "        zmove = []\n",
    "        move = np.matrix(move)\n",
    "        for i in range(g):\n",
    "            xmove.append(move[i,0])\n",
    "            ymove.append(move[i,1])\n",
    "            zmove.append(move[i,2])\n",
    "        Xmoves.append(xmove)\n",
    "        Ymoves.append(ymove)\n",
    "        Zmoves.append(zmove)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(Xmoves[MoveNum], label='x')\n",
    "plt.plot(Ymoves[MoveNum], label='y')\n",
    "plt.plot(Zmoves[MoveNum], label='z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SplitRatio = 0.9\n",
    "Split = int(SplitRatio * len(Xmoves))\n",
    "Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLength = 0\n",
    "for move in Xmoves:\n",
    "    if len(move)> maxLength: maxLength = len(move)\n",
    "\n",
    "XMoveMatrix = np.zeros((len(Xmoves), maxLength),dtype=float)\n",
    "for i in range(len(Xmoves)):\n",
    "    for j in range(len(Xmoves[i])):\n",
    "        XMoveMatrix[i,j] = Xmoves[i][j]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "\n",
    "Train_data = XMoveMatrix[-Split:]\n",
    "Test_data = XMoveMatrix[:-Split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Self Supervised\n",
    "#https://medium.com/@jetnew/anomaly-detection-of-time-series-data-e0cb6b382e33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Dense, RepeatVector, TimeDistributed\n",
    "from keras.models import Sequential\n",
    "\n",
    "class LSTM_Autoencoder:\n",
    "  def __init__(self, optimizer='adam', loss='mse'):\n",
    "    self.optimizer = optimizer\n",
    "    self.loss = loss\n",
    "    self.n_features = 1\n",
    "    \n",
    "  def build_model(self):\n",
    "    timesteps = self.timesteps\n",
    "    n_features = self.n_features\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Encoder\n",
    "    model.add(LSTM(timesteps, activation='relu', input_shape=(None, n_features), return_sequences=True))\n",
    "    model.add(LSTM(16, activation='relu', return_sequences=True))\n",
    "    model.add(LSTM(1, activation='relu'))\n",
    "    model.add(RepeatVector(timesteps))\n",
    "    \n",
    "    # Decoder\n",
    "    model.add(LSTM(timesteps, activation='relu', return_sequences=True))\n",
    "    model.add(LSTM(16, activation='relu', return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(n_features)))\n",
    "    \n",
    "    model.compile(optimizer=self.optimizer, loss=self.loss)\n",
    "    model.summary()\n",
    "    self.model = model\n",
    "    \n",
    "  def fit(self, X, epochs=3, batch_size=32):\n",
    "    self.timesteps = X.shape[1]\n",
    "    self.build_model()\n",
    "    \n",
    "    input_X = np.expand_dims(X, axis=2)\n",
    "    self.model.fit(input_X, input_X, epochs=epochs, batch_size=batch_size)\n",
    "    \n",
    "  def predict(self, X):\n",
    "    input_X = np.expand_dims(X, axis=2)\n",
    "    output_X = self.model.predict(input_X)\n",
    "    reconstruction = np.squeeze(output_X)\n",
    "    return np.linalg.norm(X - reconstruction, axis=-1)\n",
    "  \n",
    "  def plot(self, scores, timeseries, threshold=0.95):\n",
    "    sorted_scores = sorted(scores)\n",
    "    threshold_score = sorted_scores[round(len(scores) * threshold)]\n",
    "    \n",
    "    plt.title(\"Reconstruction Error\")\n",
    "    plt.plot(scores)\n",
    "    plt.plot([threshold_score]*len(scores), c='r')\n",
    "    plt.show()\n",
    "    \n",
    "    anomalous = np.where(scores > threshold_score)\n",
    "    normal = np.where(scores <= threshold_score)\n",
    "    \n",
    "    plt.title(\"Anomalies\")\n",
    "    plt.scatter(normal, timeseries[normal][:,-1], s=3)\n",
    "    plt.scatter(anomalous, timeseries[anomalous][:,-1], s=5, c='r')\n",
    "    plt.show()\n",
    "    \n",
    "lstm_autoencoder = LSTM_Autoencoder(optimizer='adam', loss='mse')\n",
    "lstm_autoencoder.fit(Train_data, epochs=3, batch_size=32)\n",
    "scores = lstm_autoencoder.predict(Test_data)\n",
    "lstm_autoencoder.plot(scores, Test_data, threshold=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give error and stop code on run all\n",
    "adsfasdfasdfasdfasdfasdfasdfasdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iForest \n",
    "Requires data in Pandas data frames\n",
    "\n",
    "https://towardsdatascience.com/unsupervised-anomaly-detection-in-python-f2e61be17c2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.anomaly import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Move_dict = dict(Xmoves)\n",
    "df_Move = pd.DataFrame.from_dict(Move_dict, oreint='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_Move[:-Split]\n",
    "df_unseen = df_Move[-Split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "anom = setup(data = df_train, silent = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_model = create_model(model = 'iforest', fraction = 0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = assign_model(anom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(anom_model, plot = 'tsne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(anom_model, plot = 'umap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_model.predict(df_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_model.predict_proba(df_unseen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_model.decision_function(df_unseen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heirarchal clustering\n",
    "https://medium.com/@jetnew/anomaly-detection-of-time-series-data-e0cb6b382e33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "clusters = 3\n",
    "y_pred = AgglomerativeClustering(n_clusters=clusters).fit_predict(Test_data)\n",
    "\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "\n",
    "clusters=5\n",
    "cls = linkage(Test_data, method='ward')\n",
    "y_pred = fcluster(cls, t=clusters, criterion='maxclust')\n",
    "\n",
    "dendrogram(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Techniques\n",
    "https://www.kaggle.com/code/victorambonati/unsupervised-anomaly-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "#%matplotlib notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import seaborn\n",
    "import matplotlib.dates as md\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "#from pyemma import msm # not available on Kaggle Kernel\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some function for later\n",
    "\n",
    "# return Series of distance between each point and his distance with the closest centroid\n",
    "def getDistanceByPoint(data, model):\n",
    "    distance = pd.Series()\n",
    "    for i in range(0,len(data)):\n",
    "        Xa = np.array(data.loc[i])\n",
    "        Xb = model.cluster_centers_[model.labels_[i]-1]\n",
    "        distance.set_value(i, np.linalg.norm(Xa-Xb))\n",
    "    return distance\n",
    "\n",
    "# train markov model to get transition matrix\n",
    "def getTransitionMatrix (df):\n",
    "\tdf = np.array(df)\n",
    "\tmodel = msm.estimate_markov_model(df, 1)\n",
    "\treturn model.transition_matrix\n",
    "\n",
    "def markovAnomaly(df, windows_size, threshold):\n",
    "    transition_matrix = getTransitionMatrix(df)\n",
    "    real_threshold = threshold**windows_size\n",
    "    df_anomaly = []\n",
    "    for j in range(0, len(df)):\n",
    "        if (j < windows_size):\n",
    "            df_anomaly.append(0)\n",
    "        else:\n",
    "            sequence = df[j-windows_size:j]\n",
    "            sequence = sequence.reset_index(drop=True)\n",
    "            df_anomaly.append(anomalyElement(sequence, real_threshold, transition_matrix))\n",
    "    return df_anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In 13\n",
    "# calculate with different number of centroids to see the loss plot (elbow method)\n",
    "n_cluster = range(1, 20)\n",
    "kmeans = [KMeans(n_clusters=i).fit(Train_data) for i in n_cluster]\n",
    "scores = [kmeans[i].score(Train_data) for i in range(len(kmeans))]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(n_cluster, scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not clear for me, I choose 15 centroids arbitrarily and add these data to the central dataframe\n",
    "df['cluster'] = kmeans[14].predict(Train_data)\n",
    "df['principal_feature1'] = Train_data[0]\n",
    "df['principal_feature2'] = Train_data[1]\n",
    "df['cluster'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFGPU",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "178f6c3502586c94dc93af50f98dbd15c5205250cbf2345a6eb57380f8c77d96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
