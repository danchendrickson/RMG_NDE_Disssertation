{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#Standard Header used on the projects\r\n",
    "\r\n",
    "#first the major packages used for math and graphing\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from cycler import cycler\r\n",
    "import scipy.special as sp\r\n",
    "\r\n",
    "#Custome graph format style sheet\r\n",
    "plt.style.use('Prospectus.mplstyle')\r\n",
    "\r\n",
    "#If being run by a seperate file, use the seperate file's graph format and saving paramaeters\r\n",
    "#otherwise set what is needed\r\n",
    "if not 'Saving' in locals():\r\n",
    "    Saving = False\r\n",
    "if not 'Titles' in locals():\r\n",
    "    Titles = True\r\n",
    "if not 'Ledgends' in locals():\r\n",
    "    Ledgends = True\r\n",
    "if not 'FFormat' in locals():\r\n",
    "    FFormat = '.eps'\r\n",
    "if not 'location' in locals():\r\n",
    "    #save location.  First one is for running on home PC, second for running on the work laptop.  May need to make a global change\r\n",
    "    location = 'E:\\\\Documents\\\\Dan\\\\Code\\\\FigsAndPlots\\\\FigsAndPlotsDocument\\\\Figures\\\\'\r\n",
    "    #location = 'C:\\\\Users\\\\dhendrickson\\\\Documents\\\\github\\\\FigsAndPlots\\\\FigsAndPlotsDocument\\\\Figures\\\\'\r\n",
    "\r\n",
    "my_cmap = plt.get_cmap('gray')\r\n",
    "#Standard cycle for collors and line styles\r\n",
    "default_cycler = (cycler('color', ['0.00', '0.40', '0.60', '0.70']) + cycler(linestyle=['-', '--', ':', '-.']))\r\n",
    "plt.rc('axes', prop_cycle=default_cycler)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#Project Specific packages:\r\n",
    "import zipfile\r\n",
    "#import DWFT as fp\r\n",
    "import os as os\r\n",
    "import pandas as pd\r\n",
    "import random\r\n",
    "import multiprocessing\r\n",
    "from joblib import Parallel, delayed\r\n",
    "import tictoc as tt\r\n",
    "\r\n",
    "import pywt\r\n",
    "from pywt._extensions._pywt import (DiscreteContinuousWavelet, ContinuousWavelet,\r\n",
    "                                Wavelet, _check_dtype)\r\n",
    "from pywt._functions import integrate_wavelet, scale2frequency\r\n",
    "from time import time as ti"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#DataSet = np.genfromtxt(open(\"E:\\\\Documents\\\\Dan\\\\PhD\\\\Play\\\\ASC\\\\60kPoints-210709-1026.csv\",'r'), delimiter=',',skip_header=4)\r\n",
    "#DataSet = np.genfromtxt(open(\"C:\\\\Users\\\\dhendrickson\\\\Pone Drive\\\\OneDrive - The Port of Virginia\\\\Shared with Everyone\\\\60kAccel-210713-1700.csv\",'r'), delimiter=',',skip_header=4)\r\n",
    "#folder = \"C:\\\\Users\\\\hendrickson\\\\Desktop\\\\temp\\\\\"\r\n",
    "#folder = \"g:\\\\\"\r\n",
    "folder = \"g:\\\\Excel Versions\\\\\"\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def cwt_fixed(data, scales, wavelet, sampling_period=1.):\r\n",
    "    \"\"\"\r\n",
    "    COPIED AND FIXED FROM pywt.cwt TO BE ABLE TO USE WAVELET FAMILIES SUCH\r\n",
    "    AS COIF AND DB\r\n",
    "\r\n",
    "    COPIED From Spenser Kirn\r\n",
    "    \r\n",
    "    All wavelet work except bior family, rbio family, haar, and db1.\r\n",
    "    \r\n",
    "    cwt(data, scales, wavelet)\r\n",
    "\r\n",
    "    One dimensional Continuous Wavelet Transform.\r\n",
    "\r\n",
    "    Parameters\r\n",
    "    ----------\r\n",
    "    data : array_like\r\n",
    "        Input signal\r\n",
    "    scales : array_like\r\n",
    "        scales to use\r\n",
    "    wavelet : Wavelet object or name\r\n",
    "        Wavelet to use\r\n",
    "    sampling_period : float\r\n",
    "        Sampling period for frequencies output (optional)\r\n",
    "\r\n",
    "    Returns\r\n",
    "    -------\r\n",
    "    coefs : array_like\r\n",
    "        Continous wavelet transform of the input signal for the given scales\r\n",
    "        and wavelet\r\n",
    "    frequencies : array_like\r\n",
    "        if the unit of sampling period are seconds and given, than frequencies\r\n",
    "        are in hertz. Otherwise Sampling period of 1 is assumed.\r\n",
    "\r\n",
    "    Notes\r\n",
    "    -----\r\n",
    "    Size of coefficients arrays depends on the length of the input array and\r\n",
    "    the length of given scales.\r\n",
    "\r\n",
    "    Examples\r\n",
    "    --------\r\n",
    "    >>> import pywt\r\n",
    "    >>> import numpy as np\r\n",
    "    >>> import matplotlib.pyplot as plt\r\n",
    "    >>> x = np.arange(512)\r\n",
    "    >>> y = np.sin(2*np.pi*x/32)\r\n",
    "    >>> coef, freqs=pywt.cwt(y,np.arange(1,129),'gaus1')\r\n",
    "    >>> plt.matshow(coef) # doctest: +SKIP\r\n",
    "    >>> plt.show() # doctest: +SKIP\r\n",
    "    ----------\r\n",
    "    >>> import pywt\r\n",
    "    >>> import numpy as np\r\n",
    "    >>> import matplotlib.pyplot as plt\r\n",
    "    >>> t = np.linspace(-1, 1, 200, endpoint=False)\r\n",
    "    >>> sig  = np.cos(2 * np.pi * 7 * t) + np.real(np.exp(-7*(t-0.4)**2)*np.exp(1j*2*np.pi*2*(t-0.4)))\r\n",
    "    >>> widths = np.arange(1, 31)\r\n",
    "    >>> cwtmatr, freqs = pywt.cwt(sig, widths, 'mexh')\r\n",
    "    >>> plt.imshow(cwtmatr, extent=[-1, 1, 1, 31], cmap='PRGn', aspect='auto',\r\n",
    "    ...            vmax=abs(cwtmatr).max(), vmin=-abs(cwtmatr).max())  # doctest: +SKIP\r\n",
    "    >>> plt.show() # doctest: +SKIP\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    # accept array_like input; make a copy to ensure a contiguous array\r\n",
    "    dt = _check_dtype(data)\r\n",
    "    data = np.array(data, dtype=dt)\r\n",
    "    if not isinstance(wavelet, (ContinuousWavelet, Wavelet)):\r\n",
    "        wavelet = DiscreteContinuousWavelet(wavelet)\r\n",
    "    if np.isscalar(scales):\r\n",
    "        scales = np.array([scales])\r\n",
    "    if data.ndim == 1:\r\n",
    "        try:\r\n",
    "            if wavelet.complex_cwt:\r\n",
    "                out = np.zeros((np.size(scales), data.size), dtype=complex)\r\n",
    "            else:\r\n",
    "                out = np.zeros((np.size(scales), data.size))\r\n",
    "        except AttributeError:\r\n",
    "            out = np.zeros((np.size(scales), data.size))\r\n",
    "        for i in np.arange(np.size(scales)):\r\n",
    "            precision = 10\r\n",
    "            int_psi, x = integrate_wavelet(wavelet, precision=precision)\r\n",
    "            step = x[1] - x[0]\r\n",
    "            j = np.floor(\r\n",
    "                np.arange(scales[i] * (x[-1] - x[0]) + 1) / (scales[i] * step))\r\n",
    "            if np.max(j) >= np.size(int_psi):\r\n",
    "                j = np.delete(j, np.where((j >= np.size(int_psi)))[0])\r\n",
    "            coef = - np.sqrt(scales[i]) * np.diff(\r\n",
    "                np.convolve(data, int_psi[j.astype(np.int)][::-1]))\r\n",
    "            d = (coef.size - data.size) / 2.\r\n",
    "            out[i, :] = coef[int(np.floor(d)):int(-np.ceil(d))]\r\n",
    "        frequencies = scale2frequency(wavelet, scales, precision)\r\n",
    "        if np.isscalar(frequencies):\r\n",
    "            frequencies = np.array([frequencies])\r\n",
    "        for i in np.arange(len(frequencies)):\r\n",
    "            frequencies[i] /= sampling_period\r\n",
    "        return out, frequencies\r\n",
    "    else:\r\n",
    "        raise ValueError(\"Only dim == 1 supported\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def getThumbprint(data, wvt, ns=50, numslices=5, slicethickness=0.12, \r\n",
    "                  valleysorpeaks='both', normconstant=1, plot=True):\r\n",
    "    '''\r\n",
    "    STarted with Spenser Kirn's code, modifed by DCH\r\n",
    "    Updated version of the DWFT function above that allows plotting of just\r\n",
    "    valleys or just peaks or both. To plot just valleys set valleysorpeaks='valleys'\r\n",
    "    to plot just peaks set valleysorpeaks='peaks' or 'both' to plot both.\r\n",
    "    '''\r\n",
    "    # First take the wavelet transform and then normalize to one\r\n",
    "    cfX, freqs = cwt_fixed(data, np.arange(1,ns+1), wvt)\r\n",
    "    cfX = np.true_divide(cfX, abs(cfX).max()*normconstant)\r\n",
    "    \r\n",
    "    fp = np.zeros((len(data), ns), dtype=int)\r\n",
    "    \r\n",
    "    # Create the list of locations between -1 and 1 to preform slices. Valley\r\n",
    "    # slices will all be below 0 and peak slices will all be above 0.\r\n",
    "    if valleysorpeaks == 'both':\r\n",
    "        slicelocations1 = np.arange(-1 ,0.0/numslices, 1.0/numslices)\r\n",
    "        slicelocations2 = np.arange(1.0/numslices, 1+1.0/numslices, 1.0/numslices)\r\n",
    "        slicelocations = np.array(np.append(slicelocations1,slicelocations2))\r\n",
    "        \r\n",
    "    for loc in slicelocations:\r\n",
    "        for y in range(0, ns):\r\n",
    "            for x in range(0, len(data)):\r\n",
    "                if cfX[y, x]>=(loc-(slicethickness/2)) and cfX[y,x]<= (loc+(slicethickness/2)):\r\n",
    "                    fp[x,y] = 1\r\n",
    "                    \r\n",
    "    fp = np.transpose(fp[:,:ns])\r\n",
    "    return fp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def RidgeCount(fingerprint):\r\n",
    "    '''\r\n",
    "    From Spencer Kirn\r\n",
    "    Count the number of times the fingerprint changes from 0 to 1 or 1 to 0 in \r\n",
    "    consective rows. Gives a vector representation of the DWFT\r\n",
    "    '''\r\n",
    "    diff = np.zeros((fingerprint.shape))\r\n",
    "    \r\n",
    "    for i, row in enumerate(fingerprint):\r\n",
    "        if i==0:\r\n",
    "            prev = row\r\n",
    "        else:\r\n",
    "            # First row (i=0) of diff will always be 0s because it does not\r\n",
    "            # matter what values are present. \r\n",
    "            # First find where the rows differ\r\n",
    "            diff_vec = abs(row-prev)\r\n",
    "            # Then set those differences to 1 to be added later\r\n",
    "            diff_vec[diff_vec != 0] = 1\r\n",
    "            diff[i, :] = diff_vec\r\n",
    "            \r\n",
    "            prev = row\r\n",
    "            \r\n",
    "    ridgeCount = diff.sum(axis=0)\r\n",
    "    \r\n",
    "    return ridgeCount"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def getAcceleration(FileName):\r\n",
    "    try:\r\n",
    "        DataSet = np.genfromtxt(open(folder+FileName,'r'), delimiter=',',skip_header=4)\r\n",
    "        return [[FileName,'x',DataSet[:,2]],[FileName,'y',DataSet[:,3]],[FileName,'z',DataSet[:,4]]]\r\n",
    "    except:\r\n",
    "        return [False,FileName,False]\r\n",
    "\r\n",
    "def makePrints(DataArray):\r\n",
    "    FingerPrint = getThumbprint(DataArray[2],'gaus2')\r\n",
    "    return [DataArray[0],DataArray[1],FingerPrint]\r\n",
    "\r\n",
    "def getResults(FPnMd):\r\n",
    "    Ridges = RidgeCount(FPnMd[2][:,500:59500])\r\n",
    "    return [FPnMd[0],FPnMd[1],Ridges]\r\n",
    "\r\n",
    "def CountAboveThreshold(Ridges, Threshold = 10):\r\n",
    "    Cnum = np.count_nonzero(Ridges[2] >= Threshold)\r\n",
    "    return [Ridges[0],Ridges[1],Cnum]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "num_cores = multiprocessing.cpu_count()\r\n",
    "files = os.listdir(folder)\r\n",
    "GroupSize = 250\r\n",
    "loops = int(float(np.size(files))/float(GroupSize))+1\r\n",
    "\r\n",
    "\r\n",
    "if __name__ == \"__main__\":\r\n",
    "    for i in range(loops):\r\n",
    "        AllAccels = Parallel(n_jobs=num_cores)(delayed(getAcceleration)(file) for file in files[i*GroupSize:((i+1)*GroupSize)])\r\n",
    "        Flattened = []\r\n",
    "        for j in range(np.shape(AllAccels)[0]):\r\n",
    "            if AllAccels[j][0] == False:\r\n",
    "                print(j,AllAccels[j][1])\r\n",
    "            else: \r\n",
    "                for k in range(3):\r\n",
    "                    Flattened.append(AllAccels[j][k])\r\n",
    "        AllFingers =  Parallel(n_jobs=num_cores)(delayed(makePrints)(datas) for datas in Flattened)\r\n",
    "        AllRidges = Parallel(n_jobs=num_cores)(delayed(getResults)(datas) for datas in AllFingers)\r\n",
    "        Events=[]\r\n",
    "        Events = Parallel(n_jobs=num_cores)(delayed(CountAboveThreshold)(datas) for datas in AllRidges)\r\n",
    "        \r\n",
    "        Events = np.matrix(Events)\r\n",
    "        df = pd.DataFrame(data=Events)\r\n",
    "        df.to_csv('G:\\\\Results\\\\Random Check' + str(i) + '.csv', sep=',', index = False, header=False,quotechar='\"')\r\n",
    "        print(str(i+1)+' of '+str(loops))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 of 36\n",
      "2 of 36\n",
      "3 of 36\n",
      "4 of 36\n",
      "5 of 36\n",
      "6 of 36\n",
      "7 of 36\n",
      "110 60kAccel-210727-1649.csv\n",
      "125 60kAccel-210727-1910.csv\n",
      "137 60kAccel-210727-2103.csv\n",
      "140 60kAccel-210727-2131.csv\n",
      "162 60kAccel-210728-0057.csv\n",
      "167 60kAccel-210728-0144.csv\n",
      "169 60kAccel-210728-0203.csv\n",
      "181 60kAccel-210728-0355.csv\n",
      "183 60kAccel-210728-0414.csv\n",
      "198 60kAccel-210728-0634.csv\n",
      "201 60kAccel-210728-0701.csv\n",
      "208 60kAccel-210728-0807.csv\n",
      "224 60kAccel-210728-1035.csv\n",
      "230 60kAccel-210728-1131.csv\n",
      "237 60kAccel-210728-1236.csv\n",
      "244 60kAccel-210728-1341.csv\n",
      "246 60kAccel-210728-1400.csv\n",
      "248 60kAccel-210728-1418.csv\n",
      "8 of 36\n",
      "9 60kAccel-210728-1600.csv\n",
      "14 60kAccel-210728-1647.csv\n",
      "18 60kAccel-210728-1724.csv\n",
      "19 60kAccel-210728-1733.csv\n",
      "32 60kAccel-210728-1933.csv\n",
      "41 60kAccel-210728-2057.csv\n",
      "53 60kAccel-210728-2248.csv\n",
      "62 60kAccel-210729-0012.csv\n",
      "77 60kAccel-210729-0231.csv\n",
      "82 60kAccel-210729-0317.csv\n",
      "86 60kAccel-210729-0354.csv\n",
      "91 60kAccel-210729-0441.csv\n",
      "104 60kAccel-210729-0641.csv\n",
      "105 60kAccel-210729-0651.csv\n",
      "110 60kAccel-210729-0737.csv\n",
      "113 60kAccel-210729-0805.csv\n",
      "132 60kAccel-210729-1101.csv\n",
      "140 60kAccel-210729-1215.csv\n",
      "141 60kAccel-210729-1224.csv\n",
      "144 60kAccel-210729-1252.csv\n",
      "145 60kAccel-210729-1301.csv\n",
      "146 60kAccel-210729-1311.csv\n",
      "168 60kAccel-210729-1634.csv\n",
      "171 60kAccel-210729-1702.csv\n",
      "174 60kAccel-210729-1730.csv\n",
      "184 60kAccel-210729-1903.csv\n",
      "187 60kAccel-210729-1931.csv\n",
      "193 60kAccel-210729-2026.csv\n",
      "204 60kAccel-210729-2208.csv\n",
      "205 60kAccel-210729-2217.csv\n",
      "209 60kAccel-210729-2254.csv\n",
      "213 60kAccel-210729-2332.csv\n",
      "214 60kAccel-210729-2341.csv\n",
      "216 60kAccel-210729-2359.csv\n",
      "218 60kAccel-210730-0018.csv\n",
      "221 60kAccel-210730-0046.csv\n",
      "226 60kAccel-210730-0132.csv\n",
      "227 60kAccel-210730-0141.csv\n",
      "237 60kAccel-210730-0314.csv\n",
      "238 60kAccel-210730-0323.csv\n",
      "241 60kAccel-210730-0351.csv\n",
      "9 of 36\n",
      "4 60kAccel-210730-0550.csv\n",
      "7 60kAccel-210730-0617.csv\n",
      "8 60kAccel-210730-0626.csv\n",
      "18 60kAccel-210730-0756.csv\n",
      "23 60kAccel-210730-0841.csv\n",
      "27 60kAccel-210730-0917.csv\n",
      "28 60kAccel-210730-0926.csv\n",
      "37 60kAccel-210730-1048.csv\n",
      "39 60kAccel-210730-1106.csv\n",
      "88 60kAccel-210730-1904.csv\n",
      "91 60kAccel-210730-1933.csv\n",
      "92 60kAccel-210730-1942.csv\n",
      "96 60kAccel-210730-2020.csv\n",
      "98 60kAccel-210730-2038.csv\n",
      "107 60kAccel-210730-2203.csv\n",
      "123 60kAccel-210731-0034.csv\n",
      "137 60kAccel-210731-0245.csv\n",
      "142 60kAccel-210731-0332.csv\n",
      "162 60kAccel-210731-0640.csv\n",
      "163 60kAccel-210731-0649.csv\n",
      "165 60kAccel-210731-0708.csv\n",
      "170 60kAccel-210731-0755.csv\n",
      "171 60kAccel-210731-0804.csv\n",
      "172 60kAccel-210731-0814.csv\n",
      "194 60kAccel-210731-1140.csv\n",
      "199 60kAccel-210731-1227.csv\n",
      "202 60kAccel-210731-1255.csv\n",
      "231 60kAccel-210731-1727.csv\n",
      "232 60kAccel-210731-1736.csv\n",
      "249 60kAccel-210731-2016.csv\n",
      "10 of 36\n",
      "5 60kAccel-210731-2112.csv\n",
      "7 60kAccel-210731-2131.csv\n",
      "12 60kAccel-210731-2217.csv\n",
      "13 60kAccel-210731-2227.csv\n",
      "19 60kAccel-210731-2322.csv\n",
      "26 60kAccel-210801-0026.csv\n",
      "28 60kAccel-210801-0044.csv\n",
      "30 60kAccel-210801-0103.csv\n",
      "31 60kAccel-210801-0112.csv\n",
      "42 60kAccel-210801-0253.csv\n",
      "47 60kAccel-210801-0339.csv\n",
      "48 60kAccel-210801-0348.csv\n",
      "55 60kAccel-210801-0453.csv\n",
      "114 60kAccel-210801-1405.csv\n",
      "153 60kAccel-210801-2010.csv\n",
      "154 60kAccel-210801-2019.csv\n",
      "157 60kAccel-210801-2047.csv\n",
      "161 60kAccel-210801-2124.csv\n",
      "163 60kAccel-210801-2143.csv\n",
      "164 60kAccel-210801-2153.csv\n",
      "172 60kAccel-210801-2308.csv\n",
      "174 60kAccel-210801-2327.csv\n",
      "177 60kAccel-210801-2355.csv\n",
      "178 60kAccel-210802-0004.csv\n",
      "181 60kAccel-210802-0032.csv\n",
      "200 60kAccel-210802-0331.csv\n",
      "218 60kAccel-210802-0620.csv\n",
      "222 60kAccel-210802-0657.csv\n",
      "247 60kAccel-210802-1044.csv\n",
      "248 60kAccel-210802-1053.csv\n",
      "11 of 36\n",
      "2 60kAccel-210802-1131.csv\n",
      "48 60kAccel-210802-1840.csv\n",
      "49 60kAccel-210802-1849.csv\n",
      "57 60kAccel-210802-2001.csv\n",
      "61 60kAccel-210802-2037.csv\n",
      "67 60kAccel-210802-2141.csv\n",
      "74 60kAccel-210802-2244.csv\n",
      "75 60kAccel-210802-2253.csv\n",
      "76 60kAccel-210802-2302.csv\n",
      "98 60kAccel-210803-0221.csv\n",
      "107 60kAccel-210803-0342.csv\n",
      "109 60kAccel-210803-0400.csv\n",
      "115 60kAccel-210803-0454.csv\n",
      "120 60kAccel-210803-0539.csv\n",
      "121 60kAccel-210803-0548.csv\n",
      "122 60kAccel-210803-0557.csv\n",
      "123 60kAccel-210803-0606.csv\n",
      "124 60kAccel-210803-0615.csv\n",
      "134 60kAccel-210803-0745.csv\n",
      "138 60kAccel-210803-0821.csv\n",
      "233 60kAccel-210803-2249.csv\n",
      "248 60kAccel-210804-0104.csv\n",
      "249 60kAccel-210804-0113.csv\n",
      "12 of 36\n",
      "0 60kAccel-210804-0122.csv\n",
      "2 60kAccel-210804-0140.csv\n",
      "55 60kAccel-210804-0937.csv\n",
      "66 60kAccel-210804-1116.csv\n",
      "87 60kAccel-210804-1426.csv\n",
      "139 60kAccel-210804-2218.csv\n",
      "13 of 36\n",
      "63 60kAccel-210806-0037.csv\n",
      "96 60kAccel-210806-0542.csv\n",
      "14 of 36\n",
      "15 of 36\n",
      "16 of 36\n",
      "17 of 36\n",
      "18 of 36\n",
      "19 of 36\n",
      "20 of 36\n",
      "21 of 36\n",
      "22 of 36\n",
      "23 of 36\n",
      "24 of 36\n",
      "25 of 36\n",
      "80 60kGyro-210727-1804.csv\n",
      "90 60kGyro-210727-1938.csv\n",
      "114 60kGyro-210727-2324.csv\n",
      "115 60kGyro-210727-2333.csv\n",
      "137 60kGyro-210728-0259.csv\n",
      "148 60kGyro-210728-0442.csv\n",
      "172 60kGyro-210728-0825.csv\n",
      "196 60kGyro-210728-1208.csv\n",
      "208 60kGyro-210728-1400.csv\n",
      "224 60kGyro-210728-1628.csv\n",
      "230 60kGyro-210728-1724.csv\n",
      "239 60kGyro-210728-1847.csv\n",
      "26 of 36\n",
      "14 60kGyro-210728-2239.csv\n",
      "139 60kGyro-210729-1758.csv\n",
      "147 60kGyro-210729-1913.csv\n",
      "176 60kGyro-210729-2341.csv\n",
      "208 60kGyro-210730-0438.csv\n",
      "249 60kGyro-210730-1049.csv\n",
      "27 of 36\n",
      "11 60kGyro-210730-1257.csv\n",
      "28 60kGyro-210730-1537.csv\n",
      "38 60kGyro-210730-1711.csv\n",
      "52 60kGyro-210730-1923.csv\n",
      "68 60kGyro-210730-2154.csv\n",
      "126 60kGyro-210731-0659.csv\n",
      "129 60kGyro-210731-0727.csv\n",
      "165 60kGyro-210731-1305.csv\n",
      "166 60kGyro-210731-1314.csv\n",
      "181 60kGyro-210731-1534.csv\n",
      "187 60kGyro-210731-1630.csv\n",
      "188 60kGyro-210731-1640.csv\n",
      "195 60kGyro-210731-1746.csv\n",
      "207 60kGyro-210731-1939.csv\n",
      "212 60kGyro-210731-2026.csv\n",
      "216 60kGyro-210731-2103.csv\n",
      "236 60kGyro-210801-0008.csv\n",
      "28 of 36\n",
      "22 60kGyro-210801-0540.csv\n",
      "31 60kGyro-210801-0703.csv\n",
      "55 60kGyro-210801-1047.csv\n",
      "58 60kGyro-210801-1115.csv\n",
      "29 of 36\n",
      "30 of 36\n",
      "31 of 36\n",
      "32 of 36\n",
      "33 of 36\n",
      "34 of 36\n",
      "35 of 36\n",
      "36 of 36\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}