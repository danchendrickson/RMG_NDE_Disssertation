{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sciclone/home20/dchendrickson01/.conda/envs/testenv2/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have Data\n",
      "Have fingerprints\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Standard Header used on the projects\n",
    "\n",
    "#first the major packages used for math and graphing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "import scipy.special as sp\n",
    "\n",
    "#Custome graph format style sheet\n",
    "plt.style.use('Prospectus.mplstyle')\n",
    "\n",
    "#If being run by a seperate file, use the seperate file's graph format and saving paramaeters\n",
    "#otherwise set what is needed\n",
    "if not 'Saving' in locals():\n",
    "    Saving = False\n",
    "if not 'Titles' in locals():\n",
    "    Titles = True\n",
    "if not 'Ledgends' in locals():\n",
    "    Ledgends = True\n",
    "if not 'FFormat' in locals():\n",
    "    FFormat = '.eps'\n",
    "if not 'location' in locals():\n",
    "    #save location.  First one is for running on home PC, second for running on the work laptop.  May need to make a global change\n",
    "    location = 'E:\\\\Documents\\\\Dan\\\\Code\\\\FigsAndPlots\\\\FigsAndPlotsDocument\\\\Figures\\\\'\n",
    "    #location = 'C:\\\\Users\\\\dhendrickson\\\\Documents\\\\github\\\\FigsAndPlots\\\\FigsAndPlotsDocument\\\\Figures\\\\'\n",
    "\n",
    "my_cmap = plt.get_cmap('gray')\n",
    "#Standard cycle for collors and line styles\n",
    "default_cycler = (cycler('color', ['0.00', '0.40', '0.60', '0.70']) + cycler(linestyle=['-', '--', ':', '-.']))\n",
    "plt.rc('axes', prop_cycle=default_cycler)\n",
    "\n",
    "#Project Specific packages:\n",
    "import random\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from pywt._extensions._pywt import (DiscreteContinuousWavelet, ContinuousWavelet,\n",
    "                                Wavelet, _check_dtype)\n",
    "from pywt._functions import integrate_wavelet, scale2frequency\n",
    "from time import time as ti\n",
    "import datetime\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model \n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    "  \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "import os\n",
    "import platform\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"  #Use for GPU    \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"  #use for CPU\n",
    "\n",
    "import tensorflow as tf\n",
    "from itertools import compress\n",
    "\n",
    "\n",
    "HostName = platform.node()\n",
    "\n",
    "if HostName == \"Server\":\n",
    "    Computer = \"Desktop\"   \n",
    "elif HostName[-6:] == 'wm.edu':\n",
    "    Computer = \"SciClone\"\n",
    "elif HostName == \"SchoolLaptop\":\n",
    "    Computer = \"LinLap\"\n",
    "elif HostName == \"WTC-TAB-512\":\n",
    "    Computer = \"PortLap\"\n",
    "else:\n",
    "    Computer = \"WinLap\"\n",
    "\n",
    "if Computer == \"SciClone\":\n",
    "    location = '/sciclone/home20/dchendrickson01/image/'\n",
    "elif Computer == \"WinLap\":\n",
    "    location = 'C:\\\\Data\\\\'\n",
    "elif Computer == \"Desktop\":\n",
    "    location = \"E:\\\\Backups\\\\Dan\\\\CraneData\\\\\"\n",
    "elif Computer == \"LinLap\":\n",
    "    location = '/home/dan/Output/'\n",
    "elif Computer == 'PortLap':\n",
    "    location = 'C:\\\\users\\\\dhendrickson\\\\Desktop\\\\AccelData\\\\'\n",
    "    \n",
    "\n",
    "if Computer ==  \"SciClone\":\n",
    "    rootfolder = '/sciclone/home20/dchendrickson01/'\n",
    "    folder = '/sciclone/data10/dchendrickson01/SmallCopy/'\n",
    "elif Computer == \"Desktop\":\n",
    "    rootfolder = location\n",
    "    folder = rootfolder + \"SmallCopy\\\\\"\n",
    "elif Computer ==\"WinLap\":\n",
    "    rootfolder = location\n",
    "    folder = rootfolder + \"SmallCopy\\\\\"   \n",
    "elif Computer == \"LinLap\":\n",
    "    rootfolder = '/home/dan/Data/'\n",
    "    folder = rootfolder + 'SmallCopy/'\n",
    "elif Computer =='PortLap':\n",
    "    rootfolder = location \n",
    "    folder = rootfolder + 'SmallCopy\\\\'\n",
    "\n",
    "scales = 500\n",
    "img_height , img_width = scales, 100\n",
    "FrameLength = img_width\n",
    "numberFrames = 600\n",
    "DoSomeFiles = True\n",
    "NumberOfFiles = 150\n",
    "SmoothType = 1  # 0 = none, 1 = rolling average, 2 = rolling StdDev\n",
    "SmoothDistance=10\n",
    "TrainEpochs = 4\n",
    "num_cores = multiprocessing.cpu_count() -1\n",
    "SensorPositonFile = rootfolder + 'SensorStatsSmall.csv'\n",
    "\n",
    "#SaveModelFolder = rootfolder + 'SavedModel\\\\'\n",
    "SaveModelFolder = rootfolder + 'SavedModel/'\n",
    "\n",
    "files = os.listdir(folder)\n",
    "if DoSomeFiles: files = random.sample(files,NumberOfFiles)\n",
    "\n",
    "GroupSize = 2*num_cores\n",
    "\n",
    "OutputVectors = np.genfromtxt(open(SensorPositonFile,'r'), delimiter=',',skip_header=1,dtype=int, missing_values=0)\n",
    "\n",
    "def cwt_fixed(data, scales, wavelet, sampling_period=1.):\n",
    "    \"\"\"\n",
    "    COPIED AND FIXED FROM pywt.cwt TO BE ABLE TO USE WAVELET FAMILIES SUCH\n",
    "    AS COIF AND DB\n",
    "\n",
    "    COPIED From Spenser Kirn\n",
    "    \n",
    "    All wavelet work except bior family, rbio family, haar, and db1.\n",
    "    \n",
    "    cwt(data, scales, wavelet)\n",
    "\n",
    "    One dimensional Continuous Wavelet Transform.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : array_like\n",
    "        Input signal\n",
    "    scales : array_like\n",
    "        scales to use\n",
    "    wavelet : Wavelet object or name\n",
    "        Wavelet to use\n",
    "    sampling_period : float\n",
    "        Sampling period for frequencies output (optional)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    coefs : array_like\n",
    "        Continous wavelet transform of the input signal for the given scales\n",
    "        and wavelet\n",
    "    frequencies : array_like\n",
    "        if the unit of sampling period are seconds and given, than frequencies\n",
    "        are in hertz. Otherwise Sampling period of 1 is assumed.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Size of coefficients arrays depends on the length of the input array and\n",
    "    the length of given scales.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pywt\n",
    "    >>> import numpy as np\n",
    "    >>> import matplotlib.pyplot as plt\n",
    "    >>> x = np.arange(512)\n",
    "    >>> y = np.sin(2*np.pi*x/32)\n",
    "    >>> coef, freqs=pywt.cwt(y,np.arange(1,129),'gaus1')\n",
    "    >>> plt.matshow(coef) # doctest: +SKIP\n",
    "    >>> plt.show() # doctest: +SKIP\n",
    "    ----------\n",
    "    >>> import pywt\n",
    "    >>> import numpy as np\n",
    "    >>> import matplotlib.pyplot as plt\n",
    "    >>> t = np.linspace(-1, 1, 200, endpoint=False)\n",
    "    >>> sig  = np.cos(2 * np.pi * 7 * t) + np.real(np.exp(-7*(t-0.4)**2)*np.exp(1j*2*np.pi*2*(t-0.4)))\n",
    "    >>> widths = np.arange(1, 31)\n",
    "    >>> cwtmatr, freqs = pywt.cwt(sig, widths, 'mexh')\n",
    "    >>> plt.imshow(cwtmatr, extent=[-1, 1, 1, 31], cmap='PRGn', aspect='auto',\n",
    "    ...            vmax=abs(cwtmatr).max(), vmin=-abs(cwtmatr).max())  # doctest: +SKIP\n",
    "    >>> plt.show() # doctest: +SKIP\n",
    "    \"\"\"\n",
    "\n",
    "    # accept array_like input; make a copy to ensure a contiguous array\n",
    "    dt = _check_dtype(data)\n",
    "    data = np.array(data, dtype=dt)\n",
    "    if not isinstance(wavelet, (ContinuousWavelet, Wavelet)):\n",
    "        wavelet = DiscreteContinuousWavelet(wavelet)\n",
    "    if np.isscalar(scales):\n",
    "        scales = np.array([scales])\n",
    "    if data.ndim == 1:\n",
    "        try:\n",
    "            if wavelet.complex_cwt:\n",
    "                out = np.zeros((np.size(scales), data.size), dtype=complex)\n",
    "            else:\n",
    "                out = np.zeros((np.size(scales), data.size))\n",
    "        except AttributeError:\n",
    "            out = np.zeros((np.size(scales), data.size))\n",
    "        for i in np.arange(np.size(scales)):\n",
    "            precision = 10\n",
    "            int_psi, x = integrate_wavelet(wavelet, precision=precision)\n",
    "            step = x[1] - x[0]\n",
    "            j = np.floor(\n",
    "                np.arange(scales[i] * (x[-1] - x[0]) + 1) / (scales[i] * step))\n",
    "            if np.max(j) >= np.size(int_psi):\n",
    "                j = np.delete(j, np.where((j >= np.size(int_psi)))[0])\n",
    "            coef = - np.sqrt(scales[i]) * np.diff(\n",
    "                np.convolve(data, int_psi[j.astype(int)][::-1]))\n",
    "            d = (coef.size - data.size) / 2.\n",
    "            out[i, :] = coef[int(np.floor(d)):int(-np.ceil(d))]\n",
    "        frequencies = scale2frequency(wavelet, scales, precision)\n",
    "        if np.isscalar(frequencies):\n",
    "            frequencies = np.array([frequencies])\n",
    "        for i in np.arange(len(frequencies)):\n",
    "            frequencies[i] /= sampling_period\n",
    "        return out, frequencies\n",
    "    else:\n",
    "        raise ValueError(\"Only dim == 1 supported\")\n",
    "\n",
    "def getThumbprint(data, wvt, ns=scales, numslices=5, slicethickness=0.12, \n",
    "                  valleysorpeaks='both', normconstant=1, plot=True):\n",
    "    '''\n",
    "    STarted with Spenser Kirn's code, modifed by DCH\n",
    "    Updated version of the DWFT function above that allows plotting of just\n",
    "    valleys or just peaks or both. To plot just valleys set valleysorpeaks='valleys'\n",
    "    to plot just peaks set valleysorpeaks='peaks' or 'both' to plot both.\n",
    "    '''\n",
    "    # First take the wavelet transform and then normalize to one\n",
    "    cfX, freqs = cwt_fixed(data, np.arange(1,ns+1), wvt)\n",
    "    cfX = np.true_divide(cfX, abs(cfX).max()*normconstant)\n",
    "    \n",
    "    fp = np.zeros((len(data), ns), dtype=int)\n",
    "    \n",
    "    # Create the list of locations between -1 and 1 to preform slices. Valley\n",
    "    # slices will all be below 0 and peak slices will all be above 0.\n",
    "    if valleysorpeaks == 'both':\n",
    "        slicelocations1 = np.arange(-1 ,0.0/numslices, 1.0/numslices)\n",
    "        slicelocations2 = np.arange(1.0/numslices, 1+1.0/numslices, 1.0/numslices)\n",
    "        slicelocations = np.array(np.append(slicelocations1,slicelocations2))\n",
    "        \n",
    "    for loc in slicelocations:\n",
    "        for y in range(0, ns):\n",
    "            for x in range(0, len(data)):\n",
    "                if cfX[y, x]>=(loc-(slicethickness/2)) and cfX[y,x]<= (loc+(slicethickness/2)):\n",
    "                    fp[x,y] = 1\n",
    "                    \n",
    "    fp = np.transpose(fp[:,:ns])\n",
    "    return fp\n",
    "\n",
    "def RidgeCount(fingerprint):\n",
    "    '''\n",
    "    From Spencer Kirn\n",
    "    Count the number of times the fingerprint changes from 0 to 1 or 1 to 0 in \n",
    "    consective rows. Gives a vector representation of the DWFT\n",
    "    '''\n",
    "    diff = np.zeros((fingerprint.shape))\n",
    "    \n",
    "    for i, row in enumerate(fingerprint):\n",
    "        if i==0:\n",
    "            prev = row\n",
    "        else:\n",
    "            # First row (i=0) of diff will always be 0s because it does not\n",
    "            # matter what values are present. \n",
    "            # First find where the rows differ\n",
    "            diff_vec = abs(row-prev)\n",
    "            # Then set those differences to 1 to be added later\n",
    "            diff_vec[diff_vec != 0] = 1\n",
    "            diff[i, :] = diff_vec\n",
    "            \n",
    "            prev = row\n",
    "            \n",
    "    ridgeCount = diff.sum(axis=0)\n",
    "    \n",
    "    return ridgeCount\n",
    "\n",
    "def Smoothing(RawData): #, SmoothType = 1, SmoothDistance=15):\n",
    "\n",
    "    if SmoothType == 0:\n",
    "        SmoothedData = RawData\n",
    "    elif SmoothType ==1:\n",
    "        SmoothedData = RawData\n",
    "        if np.shape(np.shape(RawData))== 2:\n",
    "            for i in range(SmoothDistance-1):\n",
    "                for j in range(3):\n",
    "                    SmoothedData[j,i+1]=np.average(RawData[j,0:i+1])\n",
    "            for i in range(np.shape(RawData)[0]-SmoothDistance):\n",
    "                for j in range(3):\n",
    "                    SmoothedData[j,i+SmoothDistance]=np.average(RawData[j,i:i+SmoothDistance])\n",
    "        elif np.shape(np.shape(RawData))== 1:\n",
    "            for i in range(SmoothDistance-1):\n",
    "                SmoothedData[i+1]=np.average(RawData[0:i+1])\n",
    "            for i in range(np.shape(RawData)[0]-SmoothDistance):\n",
    "                SmoothedData[i+SmoothDistance]=np.average(RawData[i:i+SmoothDistance])\n",
    "\n",
    "    return SmoothedData\n",
    "\n",
    "def IsMovers(Moves):\n",
    "\n",
    "    maxes = np.max(Moves[500:])\n",
    "    mins = np.min(Moves[500:])\n",
    "\n",
    "    Keep = mins-maxes\n",
    "    \n",
    "    #fKeep = np.zeros(np.size(files))\n",
    "\n",
    "    if maxes > 0.01 and mins < -0.01:\n",
    "        Keep=1\n",
    "        #if XorAll == 'All': Keep[i+1]=1\n",
    "                #if XorAll == 'All': Keep[i+2]=1\n",
    "                #fIndex = int(i/3)\n",
    "                #fKeep[fIndex] = 1\n",
    "                \n",
    "\n",
    "    #Keep = np.array(Keep, dtype='bool')\n",
    "    #fKeep = np.array(fKeep, dtype='bool')\n",
    "       \n",
    "    #SmallMoves = Moves[Keep,:]\n",
    "    #SmallFiles = list(compress(files, fKeep))\n",
    "\n",
    "    return Keep #, SmallFiles\n",
    "\n",
    "def getRAcceleration(Data):\n",
    "    rVals = []\n",
    "    for i in range(np.shape(Data)[0]):\n",
    "        rVals.append(np.sqrt(Data[i,0]**2+Data[i,1]**2+Data[i,2]**2))\n",
    "    return rVals\n",
    "\n",
    "def getAcceleration(FileName):\n",
    "        #try:\n",
    "        DataSet = np.genfromtxt(open(folder+FileName,'r'), delimiter=',',skip_header=0)\n",
    "        rData = getRAcceleration(DataSet[:,2:5])\n",
    "        rSmoothed = Smoothing(rData)\n",
    "        #ISMove = IsMovers(rSmoothed)\n",
    "        #return [[FileName,'x',DataSet[:,2]],[FileName,'y',DataSet[:,3]],[FileName,'z',DataSet[:,4]],[FileName,'r',rData]]\n",
    "        #if ISMove ==1: \n",
    "        return [FileName, 'r', rSmoothed]\n",
    "        #else: #except:\n",
    "        #    return [False,FileName + str(ISMove),False]\n",
    "\n",
    "def makePrints(DataArray):\n",
    "    try:\n",
    "        FingerPrint = getThumbprint(DataArray[2],'gaus2')\n",
    "        return [DataArray[0],DataArray[1],FingerPrint]\n",
    "    except:\n",
    "        return [DataArray[0], 'Fail', np.zeros(60000,500)]\n",
    "\n",
    "def getResults(FPnMd):\n",
    "    Ridges = RidgeCount(FPnMd[2][:,500:59500])\n",
    "    return [FPnMd[0],FPnMd[1],Ridges]\n",
    "\n",
    "def CountAboveThreshold(Ridges, Threshold = 10):\n",
    "    Cnum = np.count_nonzero(Ridges[2] >= Threshold)\n",
    "    return [Ridges[0],Ridges[1],Cnum]\n",
    "\n",
    "def truthVector(Filename):\n",
    "    # Parses the filename, and compares it against the record of sensor position on cranes\n",
    "    # inputs: filename\n",
    "    # outputs: truth vector\n",
    "\n",
    "\n",
    "    #Parsing the file name.  Assuming it is in the standard format\n",
    "    sSensor = Filename[23]\n",
    "    sDate = datetime.datetime.strptime('20'+Filename[10:21],\"%Y%m%d-%H%M\")\n",
    "\n",
    "    mask = []\n",
    "\n",
    "    i=0\n",
    "    #loops through the known sensor movements, and creates a filter mask\n",
    "    for spf in OutputVectors:\n",
    "        \n",
    "        startDate = datetime.datetime.strptime(str(spf[0])+str(spf[1]).zfill(2)+str(spf[2]).zfill(2)\n",
    "            +str(spf[3]).zfill(2)+str(spf[4]).zfill(2),\"%Y%m%d%H%M\")\n",
    "        #datetime.date(int(spf[0]), int(spf[1]), int(spf[2])) + datetime.timedelta(hours=spf[3]) + datetime.timedelta(minutes=spf[4])\n",
    "        endDate = datetime.datetime.strptime(str(spf[5])+str(spf[6]).zfill(2)+str(spf[7]).zfill(2)\n",
    "            +str(spf[8]).zfill(2)+str(spf[9]).zfill(2),\"%Y%m%d%H%M\")\n",
    "        #datetime.date(int(spf[5]), int(spf[6]), int(spf[7])) + datetime.timedelta(hours=spf[8]) + datetime.timedelta(minutes=spf[9])\n",
    "        \n",
    "        if sDate >= startDate and sDate <= endDate and int(spf[10]) == int(sSensor):\n",
    "            mask.append(True)\n",
    "            i+=1\n",
    "        else:\n",
    "            mask.append(False)\n",
    "        \n",
    "    if i != 1: print('error ', i, Filename)\n",
    "\n",
    "    results = OutputVectors[mask,11:]\n",
    "\n",
    "    if i > 1: \n",
    "        print('Found Two ', Filename)\n",
    "        results = results[0,:]\n",
    "    #np.array(results)\n",
    "\n",
    "    return results\n",
    "\n",
    "def makeFrames(input): #,sequ,frameLength):\n",
    "    frames=[] #np.array([],dtype=object,)\n",
    "    segmentGap = int((np.shape(input)[1]-FrameLength)/numberFrames)\n",
    "    #print(segmentGap,sequ, frameLength)\n",
    "    for i in range(numberFrames):\n",
    "        start = i * segmentGap\n",
    "        imageMatrix = input[:,start:start+FrameLength]\n",
    "        np.matrix(imageMatrix)\n",
    "        imageMatrix = imageMatrix.T\n",
    "\n",
    "        w0=int(np.shape(imageMatrix)[0]/2)\n",
    "        a0=np.linspace(0,w0-1,w0,dtype=int)*2\n",
    "        imageMatrix = np.delete(imageMatrix,a0,axis=0)\n",
    "\n",
    "        w1=int(np.shape(imageMatrix)[1]/2)\n",
    "        a1=np.linspace(0,w1-1,w1,dtype=int)*2\n",
    "        imageMatrix = np.delete(imageMatrix,a1,axis=1)\n",
    "\n",
    "        frames.append(imageMatrix)\n",
    "    \n",
    "    return frames\n",
    "\n",
    "def ParseData(FPwMD):\n",
    "\n",
    "    try:\n",
    "        frames = np.asarray(makeFrames(FPwMD[2]))\n",
    "\n",
    "        Results = truthVector(FPwMD[0])\n",
    "\n",
    "        return frames, Results\n",
    "    except:\n",
    "        print('Oh Fuck',FPwMD[0])\n",
    "        pass\n",
    "\n",
    "if np.size(files) % GroupSize == 0:\n",
    "    loops = int(np.size(files)/GroupSize)\n",
    "else:\n",
    "    loops = int(float(np.size(files))/float(GroupSize))+1\n",
    "#tmep for testing\n",
    "#loops=1\n",
    "\n",
    "AllFingers = [] #np.asarray(dtype=object)\n",
    "\n",
    "#for i in range(loops):\n",
    "AllAccels = Parallel(n_jobs=num_cores)(delayed(getAcceleration)(file) for file in files)\n",
    "Flattened = []\n",
    "for j in range(np.shape(AllAccels)[0]):\n",
    "    if AllAccels[j][0] == False:\n",
    "        print(j,AllAccels[j][1])\n",
    "    else: \n",
    "        Flattened.append(AllAccels[j])\n",
    "print('Have Data')\n",
    "\n",
    "\n",
    "\n",
    "Fingers =  Parallel(n_jobs=num_cores)(delayed(makePrints)(datas) for datas in Flattened)\n",
    "if np.size(AllFingers) == 0:\n",
    "    AllFingers = Fingers\n",
    "else:\n",
    "    AllFingers = np.concatenate((AllFingers, Fingers), axis = 0)\n",
    "print('Have fingerprints')\n",
    "#AllRidges = Parallel(n_jobs=num_cores)(delayed(getResults)(datas) for datas in AllFingers)\n",
    "#print('Have ridgecounts')\n",
    "#Events=[]\n",
    "#Events = Parallel(n_jobs=num_cores)(delayed(CountAboveThreshold)(datas) for datas in AllRidges)\n",
    "#\n",
    "#Events = np.matrix(Events)\n",
    "#df = pd.DataFrame(data=Events)\n",
    "#df.to_csv(rootfolder +'Random Check' + str(i) + '.csv', sep=',', index = False, header=False,quotechar='\"')\n",
    "#print(str(i+1)+' of '+str(loops))\n",
    "\n",
    "Data = Parallel(n_jobs=num_cores)(delayed(ParseData)(file) for file in AllFingers)\n",
    "\n",
    "\n",
    "np.shape(Data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (0) into shape (4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/local/scr/dchendrickson01/TMPDIR/ipykernel_34492/2592788056.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdatum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mDataSet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mResultsSet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (0) into shape (4)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "DataSet = [] \n",
    "ResultsSet = np.zeros((np.shape(Data)[0],np.shape(Data[0][1])[1]))\n",
    "i=0\n",
    "for datum in Data:\n",
    "    DataSet.append(datum[0])\n",
    "    ResultsSet[i]=np.asarray(datum[1][:]).flatten()\n",
    "    i+=1\n",
    "\n",
    "DataSet = np.asarray(DataSet)\n",
    "\n",
    "print('Data Parsed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(DataSet, ResultsSet, test_size=0.20, shuffle=True, random_state=0)\n",
    "\n",
    "\n",
    "np.shape(DataSet)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(ConvLSTM2D(filters = 8, \n",
    "            kernel_size = (5, 5), \n",
    "            return_sequences = False, \n",
    "            data_format = \"channels_last\", \n",
    "            input_shape = (numberFrames, np.shape(X_train)[2], np.shape(X_train)[3], 1)\n",
    "            )\n",
    "        )\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(np.shape(y_train)[1], activation = \"softmax\"))\n",
    " \n",
    "model.summary()\n",
    " \n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])\n",
    " \n",
    "earlystop = EarlyStopping(patience=7)   \n",
    "callbacks = [earlystop]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit(x = X_train, y = y_train, epochs=TrainEpochs, batch_size = 8 , shuffle=False, validation_split=0.2, callbacks=callbacks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error  0 60kPoints-220403-1022-s2.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.save(SaveModelFolder)\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig(rootfolder + 'ModelAccuracy.png')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig(rootfolder + 'ModelLoss.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11c16a051206f53cf7fe024f12cacb318023d916d0a5509b7bf3391ee4b4163a"
  },
  "kernelspec": {
   "display_name": "testenv2",
   "language": "python",
   "name": "testenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
