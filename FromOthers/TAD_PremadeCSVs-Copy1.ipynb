{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Timeseries anomaly detection using an Autoencoder\n",
    "\n",
    "**Author:** [pavithrasv](https://github.com/pavithrasv)<br>\n",
    "**Date created:** 2020/05/31<br>\n",
    "**Last modified:** 2020/05/31<br>\n",
    "**Description:** Detect anomalies in a timeseries using an Autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/keras-team/keras-io/blob/master/examples/timeseries/ipynb/timeseries_anomaly_detection.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This script demonstrates how you can use a reconstruction convolutional\n",
    "autoencoder model to detect anomalies in timeseries data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-11 08:50:25.283706: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-11 08:50:27.631702: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras import layers, models\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sciclone/home/dchendrickson01/.local/lib/python3.11/site-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Load the data\n",
    "\n",
    "We will use the [Numenta Anomaly Benchmark(NAB)](\n",
    "https://www.kaggle.com/boltzmannbrain/nab) dataset. It provides artificial\n",
    "timeseries data containing labeled anomalous periods of behavior. Data are\n",
    "ordered, timestamped, single-valued metrics.\n",
    "\n",
    "We will use the `art_daily_small_noise.csv` file for training and the\n",
    "`art_daily_jumpsup.csv` file for testing. The simplicity of this dataset\n",
    "allows us to demonstrate anomaly detection effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Build a model\n",
    "\n",
    "We will build a convolutional reconstruction autoencoder model. The model will\n",
    "take input of shape `(batch_size, sequence_length, num_features)` and return\n",
    "output of the same shape. In this case, `sequence_length` is 288 and\n",
    "`num_features` is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Get Data  sequences\n",
    "Create sequences combining `TIME_STEPS` contiguous data values from the\n",
    "training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 μs, sys: 3 μs, total: 5 μs\n",
      "Wall time: 10.3 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TIME_STEPS = 1000\n",
    "Dims = 3\n",
    "\n",
    "Folder = '/scratch/1000Sm/'\n",
    "Folder = '/scratch/1000Input/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "file_list = [\n",
    "    os.path.join(Folder,file)\n",
    "    for file in os.listdir(Folder) if file.endswith('Data.csv') #and file.startswith('2')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset = tf.data.Dataset.list_files(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv(file_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    df.columns = ['rx','ry','rz']\n",
    "    features = df[['rx','ry','rz']].values.tolist()\n",
    "    \n",
    "    label = pd.read_csv(file_path[:-8]+'Outs.csv', header=None)\n",
    "    label.columns = ['sx']\n",
    "    labels = np.asarray(label.sx)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self, files, batch_size=batchSize, shuffle=True, **kwargs):\n",
    "        super().__init__(**kwargs)  # Call the parent class constructor\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.files = files\n",
    "        self.on_epoch_end()\n",
    "      \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.files) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        Feat, Labe = [], []\n",
    "        \n",
    "        for idx in indexes:\n",
    "            f = parse_csv(self.files[idx])\n",
    "            Feat.append(f[0])\n",
    "            Labe.append(f[1])\n",
    "\n",
    "        Feat = tf.convert_to_tensor(Feat, dtype=tf.float32)\n",
    "        Labe = tf.convert_to_tensor(Labe, dtype=tf.float32)\n",
    "        return Feat, Labe\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.files))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = CustomDataset(file_list, batch_size=batchSize, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "Please note that we are using `x_train` as both the input and the target\n",
    "since this is a reconstruction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential(\n",
    "    [\n",
    "        # Input layer\n",
    "        layers.Input(shape=(1000, 3)),\n",
    "        \n",
    "        # Increased number of filters and added layers\n",
    "        layers.Conv1D(filters=128, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        layers.Conv1D(filters=64, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        layers.Conv1D(filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        layers.Dropout(rate=0.3),\n",
    "        \n",
    "        # Transpose layers with increased filters\n",
    "        layers.Conv1DTranspose(filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"),\n",
    "        layers.Conv1DTranspose(filters=64, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        layers.Conv1DTranspose(filters=128, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        layers.Dropout(rate=0.3),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Conv1DTranspose(filters=1, kernel_size=7, padding=\"same\"),\n",
    "        \n",
    "        # Added dense layers with more neurons\n",
    "        layers.Dense(256, activation=\"relu\"),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(1)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModelCheckpoint(Callback):\n",
    "    def __init__(self, filepath, save_freq):\n",
    "        super(CustomModelCheckpoint, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.save_freq = save_freq\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.save_freq == 0:\n",
    "            self.model.save(self.filepath.format(epoch=epoch + 1), save_format='keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = CustomModelCheckpoint(\n",
    "    filepath='/scratch/models/TAD_08110900_checkpoint_{epoch:02d}.h5',\n",
    "    save_freq=2  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x=training_generator,\n",
    "    epochs=10,\n",
    "    batch_size=batchSize,\n",
    "    #validation_split=0.1,\n",
    "    callbacks=[checkpoint_callback,\n",
    "        keras.callbacks.EarlyStopping(monitor=\"loss\", patience=5, mode=\"min\")\n",
    "    ],\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's plot training and validation loss to see how the training went."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "#plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdfasdfasdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Detecting anomalies\n",
    "\n",
    "We will detect anomalies by determining how well our model can reconstruct\n",
    "the input data.\n",
    "\n",
    "\n",
    "1.   Find MAE loss on training samples.\n",
    "2.   Find max MAE loss value. This is the worst our model has performed trying\n",
    "to reconstruct a sample. We will make this the `threshold` for anomaly\n",
    "detection.\n",
    "3.   If the reconstruction loss for a sample is greater than this `threshold`\n",
    "value then we can infer that the model is seeing a pattern that it isn't\n",
    "familiar with. We will label this sample as an `anomaly`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Get train MAE loss.\n",
    "x_train_pred = model.predict(x_train)\n",
    "train_mae_loss = np.mean(np.abs(x_train_pred - x_train), axis=1)\n",
    "\n",
    "plt.hist(train_mae_loss, bins=50)\n",
    "plt.xlabel(\"Train MAE loss\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()\n",
    "\n",
    "# Get reconstruction loss threshold.\n",
    "threshold = np.max(train_mae_loss)\n",
    "print(\"Reconstruction error threshold: \", threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Compare recontruction\n",
    "\n",
    "Just for fun, let's see how our model has recontructed the first sample.\n",
    "This is the 288 timesteps from day 1 of our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Checking how the first sequence is learnt\n",
    "plt.plot(x_train[0])\n",
    "plt.plot(x_train_pred[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "df_test_value = (df_daily_jumpsup - training_mean) / training_std\n",
    "fig, ax = plt.subplots()\n",
    "df_test_value.plot(legend=False, ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# Create sequences from test values.\n",
    "x_test = create_sequences(df_test_value.values)\n",
    "print(\"Test input shape: \", x_test.shape)\n",
    "\n",
    "# Get test MAE loss.\n",
    "x_test_pred = model.predict(x_test)\n",
    "test_mae_loss = np.mean(np.abs(x_test_pred - x_test), axis=1)\n",
    "test_mae_loss = test_mae_loss.reshape((-1))\n",
    "\n",
    "plt.hist(test_mae_loss, bins=50)\n",
    "plt.xlabel(\"test MAE loss\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()\n",
    "\n",
    "# Detect all the samples which are anomalies.\n",
    "anomalies = test_mae_loss > threshold\n",
    "print(\"Number of anomaly samples: \", np.sum(anomalies))\n",
    "print(\"Indices of anomaly samples: \", np.where(anomalies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Plot anomalies\n",
    "\n",
    "We now know the samples of the data which are anomalies. With this, we will\n",
    "find the corresponding `timestamps` from the original test data. We will be\n",
    "using the following method to do that:\n",
    "\n",
    "Let's say time_steps = 3 and we have 10 training values. Our `x_train` will\n",
    "look like this:\n",
    "\n",
    "- 0, 1, 2\n",
    "- 1, 2, 3\n",
    "- 2, 3, 4\n",
    "- 3, 4, 5\n",
    "- 4, 5, 6\n",
    "- 5, 6, 7\n",
    "- 6, 7, 8\n",
    "- 7, 8, 9\n",
    "\n",
    "All except the initial and the final time_steps-1 data values, will appear in\n",
    "`time_steps` number of samples. So, if we know that the samples\n",
    "[(3, 4, 5), (4, 5, 6), (5, 6, 7)] are anomalies, we can say that the data point\n",
    "5 is an anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# data i is an anomaly if samples [(i - timesteps + 1) to (i)] are anomalies\n",
    "anomalous_data_indices = []\n",
    "for data_idx in range(TIME_STEPS - 1, len(df_test_value) - TIME_STEPS + 1):\n",
    "    if np.all(anomalies[data_idx - TIME_STEPS + 1 : data_idx]):\n",
    "        anomalous_data_indices.append(data_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's overlay the anomalies on the original test data plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "df_subset = df_daily_jumpsup.iloc[anomalous_data_indices]\n",
    "fig, ax = plt.subplots()\n",
    "df_daily_jumpsup.plot(legend=False, ax=ax)\n",
    "df_subset.plot(legend=False, ax=ax, color=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "timeseries_anomaly_detection",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
