{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Header used on the projects\n",
    "\n",
    "#first the major packages used for math and graphing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "import scipy.special as sp\n",
    "\n",
    "#Custome graph format style sheet\n",
    "plt.style.use('Prospectus.mplstyle')\n",
    "\n",
    "#If being run by a seperate file, use the seperate file's graph format and saving paramaeters\n",
    "#otherwise set what is needed\n",
    "if not 'Saving' in locals():\n",
    "    Saving = False\n",
    "if not 'Titles' in locals():\n",
    "    Titles = True\n",
    "if not 'Ledgends' in locals():\n",
    "    Ledgends = True\n",
    "if not 'FFormat' in locals():\n",
    "    FFormat = '.eps'\n",
    "if not 'location' in locals():\n",
    "    #save location.  First one is for running on home PC, second for running on the work laptop.  May need to make a global change\n",
    "    location = 'E:\\\\Documents\\\\Dan\\\\Code\\\\FigsAndPlots\\\\FigsAndPlotsDocument\\\\Figures\\\\'\n",
    "    #location = 'C:\\\\Users\\\\dhendrickson\\\\Documents\\\\github\\\\FigsAndPlots\\\\FigsAndPlotsDocument\\\\Figures\\\\'\n",
    "\n",
    "my_cmap = plt.get_cmap('gray')\n",
    "#Standard cycle for collors and line styles\n",
    "default_cycler = (cycler('color', ['0.00', '0.40', '0.60', '0.70']) + cycler(linestyle=['-', '--', ':', '-.']))\n",
    "plt.rc('axes', prop_cycle=default_cycler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project Specific packages:\n",
    "import zipfile\n",
    "#import DWFT as fp\n",
    "import os as os\n",
    "import pandas as pd\n",
    "import random\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import tictoc as tt\n",
    "\n",
    "import pywt\n",
    "from pywt._extensions._pywt import (DiscreteContinuousWavelet, ContinuousWavelet,\n",
    "                                Wavelet, _check_dtype)\n",
    "from pywt._functions import integrate_wavelet, scale2frequency\n",
    "from time import time as ti\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    " \n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras_metrics as km\n",
    "  \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataSet = np.genfromtxt(open(\"E:\\\\Documents\\\\Dan\\\\PhD\\\\Play\\\\ASC\\\\60kPoints-210709-1026.csv\",'r'), delimiter=',',skip_header=4)\n",
    "#DataSet = np.genfromtxt(open(\"C:\\\\Users\\\\dhendrickson\\\\Pone Drive\\\\OneDrive - The Port of Virginia\\\\Shared with Everyone\\\\60kAccel-210713-1700.csv\",'r'), delimiter=',',skip_header=4)\n",
    "#folder = \"C:\\\\Users\\\\hendrickson\\\\Desktop\\\\temp\\\\\"\n",
    "#folder = \"g:\\\\\"\n",
    "#folder = \"g:\\\\Excel Versions\\\\\"\n",
    "rootfolder = \"C:\\\\Data\\\\\"\n",
    "folder=rootfolder+\"SmallCopy\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = 500\n",
    "img_height , img_width = scales, 100\n",
    "FrameLength = img_width\n",
    "numberFrames = 600\n",
    "DoSomeFiles = True\n",
    "NumberOfFiles = 5\n",
    "SmoothType = 1  # 0 = none, 1 = rolling average, 2 = rolling StdDev\n",
    "SmoothDistance=10\n",
    "TrainEpochs = 1\n",
    "num_cores = multiprocessing.cpu_count() -1\n",
    "SensorPositonFile = rootfolder + 'SensorStatsSmall.csv'\n",
    "SaveModelFolder = rootfolder + 'SavedModel\\\\'\n",
    "\n",
    "files = os.listdir(folder)\n",
    "if DoSomeFiles: files = random.sample(files,NumberOfFiles)\n",
    "\n",
    "GroupSize = 25\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cwt_fixed(data, scales, wavelet, sampling_period=1.):\n",
    "    \"\"\"\n",
    "    COPIED AND FIXED FROM pywt.cwt TO BE ABLE TO USE WAVELET FAMILIES SUCH\n",
    "    AS COIF AND DB\n",
    "\n",
    "    COPIED From Spenser Kirn\n",
    "    \n",
    "    All wavelet work except bior family, rbio family, haar, and db1.\n",
    "    \n",
    "    cwt(data, scales, wavelet)\n",
    "\n",
    "    One dimensional Continuous Wavelet Transform.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : array_like\n",
    "        Input signal\n",
    "    scales : array_like\n",
    "        scales to use\n",
    "    wavelet : Wavelet object or name\n",
    "        Wavelet to use\n",
    "    sampling_period : float\n",
    "        Sampling period for frequencies output (optional)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    coefs : array_like\n",
    "        Continous wavelet transform of the input signal for the given scales\n",
    "        and wavelet\n",
    "    frequencies : array_like\n",
    "        if the unit of sampling period are seconds and given, than frequencies\n",
    "        are in hertz. Otherwise Sampling period of 1 is assumed.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Size of coefficients arrays depends on the length of the input array and\n",
    "    the length of given scales.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pywt\n",
    "    >>> import numpy as np\n",
    "    >>> import matplotlib.pyplot as plt\n",
    "    >>> x = np.arange(512)\n",
    "    >>> y = np.sin(2*np.pi*x/32)\n",
    "    >>> coef, freqs=pywt.cwt(y,np.arange(1,129),'gaus1')\n",
    "    >>> plt.matshow(coef) # doctest: +SKIP\n",
    "    >>> plt.show() # doctest: +SKIP\n",
    "    ----------\n",
    "    >>> import pywt\n",
    "    >>> import numpy as np\n",
    "    >>> import matplotlib.pyplot as plt\n",
    "    >>> t = np.linspace(-1, 1, 200, endpoint=False)\n",
    "    >>> sig  = np.cos(2 * np.pi * 7 * t) + np.real(np.exp(-7*(t-0.4)**2)*np.exp(1j*2*np.pi*2*(t-0.4)))\n",
    "    >>> widths = np.arange(1, 31)\n",
    "    >>> cwtmatr, freqs = pywt.cwt(sig, widths, 'mexh')\n",
    "    >>> plt.imshow(cwtmatr, extent=[-1, 1, 1, 31], cmap='PRGn', aspect='auto',\n",
    "    ...            vmax=abs(cwtmatr).max(), vmin=-abs(cwtmatr).max())  # doctest: +SKIP\n",
    "    >>> plt.show() # doctest: +SKIP\n",
    "    \"\"\"\n",
    "\n",
    "    # accept array_like input; make a copy to ensure a contiguous array\n",
    "    dt = _check_dtype(data)\n",
    "    data = np.array(data, dtype=dt)\n",
    "    if not isinstance(wavelet, (ContinuousWavelet, Wavelet)):\n",
    "        wavelet = DiscreteContinuousWavelet(wavelet)\n",
    "    if np.isscalar(scales):\n",
    "        scales = np.array([scales])\n",
    "    if data.ndim == 1:\n",
    "        try:\n",
    "            if wavelet.complex_cwt:\n",
    "                out = np.zeros((np.size(scales), data.size), dtype=complex)\n",
    "            else:\n",
    "                out = np.zeros((np.size(scales), data.size))\n",
    "        except AttributeError:\n",
    "            out = np.zeros((np.size(scales), data.size))\n",
    "        for i in np.arange(np.size(scales)):\n",
    "            precision = 10\n",
    "            int_psi, x = integrate_wavelet(wavelet, precision=precision)\n",
    "            step = x[1] - x[0]\n",
    "            j = np.floor(\n",
    "                np.arange(scales[i] * (x[-1] - x[0]) + 1) / (scales[i] * step))\n",
    "            if np.max(j) >= np.size(int_psi):\n",
    "                j = np.delete(j, np.where((j >= np.size(int_psi)))[0])\n",
    "            coef = - np.sqrt(scales[i]) * np.diff(\n",
    "                np.convolve(data, int_psi[j.astype(int)][::-1]))\n",
    "            d = (coef.size - data.size) / 2.\n",
    "            out[i, :] = coef[int(np.floor(d)):int(-np.ceil(d))]\n",
    "        frequencies = scale2frequency(wavelet, scales, precision)\n",
    "        if np.isscalar(frequencies):\n",
    "            frequencies = np.array([frequencies])\n",
    "        for i in np.arange(len(frequencies)):\n",
    "            frequencies[i] /= sampling_period\n",
    "        return out, frequencies\n",
    "    else:\n",
    "        raise ValueError(\"Only dim == 1 supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getThumbprint(data, wvt, ns=scales, numslices=5, slicethickness=0.12, \n",
    "                  valleysorpeaks='both', normconstant=1, plot=True):\n",
    "    '''\n",
    "    STarted with Spenser Kirn's code, modifed by DCH\n",
    "    Updated version of the DWFT function above that allows plotting of just\n",
    "    valleys or just peaks or both. To plot just valleys set valleysorpeaks='valleys'\n",
    "    to plot just peaks set valleysorpeaks='peaks' or 'both' to plot both.\n",
    "    '''\n",
    "    # First take the wavelet transform and then normalize to one\n",
    "    cfX, freqs = cwt_fixed(data, np.arange(1,ns+1), wvt)\n",
    "    cfX = np.true_divide(cfX, abs(cfX).max()*normconstant)\n",
    "    \n",
    "    fp = np.zeros((len(data), ns), dtype=int)\n",
    "    \n",
    "    # Create the list of locations between -1 and 1 to preform slices. Valley\n",
    "    # slices will all be below 0 and peak slices will all be above 0.\n",
    "    if valleysorpeaks == 'both':\n",
    "        slicelocations1 = np.arange(-1 ,0.0/numslices, 1.0/numslices)\n",
    "        slicelocations2 = np.arange(1.0/numslices, 1+1.0/numslices, 1.0/numslices)\n",
    "        slicelocations = np.array(np.append(slicelocations1,slicelocations2))\n",
    "        \n",
    "    for loc in slicelocations:\n",
    "        for y in range(0, ns):\n",
    "            for x in range(0, len(data)):\n",
    "                if cfX[y, x]>=(loc-(slicethickness/2)) and cfX[y,x]<= (loc+(slicethickness/2)):\n",
    "                    fp[x,y] = 1\n",
    "                    \n",
    "    fp = np.transpose(fp[:,:ns])\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RidgeCount(fingerprint):\n",
    "    '''\n",
    "    From Spencer Kirn\n",
    "    Count the number of times the fingerprint changes from 0 to 1 or 1 to 0 in \n",
    "    consective rows. Gives a vector representation of the DWFT\n",
    "    '''\n",
    "    diff = np.zeros((fingerprint.shape))\n",
    "    \n",
    "    for i, row in enumerate(fingerprint):\n",
    "        if i==0:\n",
    "            prev = row\n",
    "        else:\n",
    "            # First row (i=0) of diff will always be 0s because it does not\n",
    "            # matter what values are present. \n",
    "            # First find where the rows differ\n",
    "            diff_vec = abs(row-prev)\n",
    "            # Then set those differences to 1 to be added later\n",
    "            diff_vec[diff_vec != 0] = 1\n",
    "            diff[i, :] = diff_vec\n",
    "            \n",
    "            prev = row\n",
    "            \n",
    "    ridgeCount = diff.sum(axis=0)\n",
    "    \n",
    "    return ridgeCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Smoothing(RawData): #, SmoothType = 1, SmoothDistance=15):\n",
    "\n",
    "    if SmoothType == 0:\n",
    "        SmoothedData = RawData\n",
    "    elif SmoothType ==1:\n",
    "        SmoothedData = RawData\n",
    "        if np.shape(np.shape(RawData))== 2:\n",
    "            for i in range(SmoothDistance-1):\n",
    "                for j in range(3):\n",
    "                    SmoothedData[j,i+1]=np.average(RawData[j,0:i+1])\n",
    "            for i in range(np.shape(RawData)[0]-SmoothDistance):\n",
    "                for j in range(3):\n",
    "                    SmoothedData[j,i+SmoothDistance]=np.average(RawData[j,i:i+SmoothDistance])\n",
    "        elif np.shape(np.shape(RawData))== 1:\n",
    "            for i in range(SmoothDistance-1):\n",
    "                SmoothedData[i+1]=np.average(RawData[0:i+1])\n",
    "            for i in range(np.shape(RawData)[0]-SmoothDistance):\n",
    "                SmoothedData[i+SmoothDistance]=np.average(RawData[i:i+SmoothDistance])\n",
    "\n",
    "    return SmoothedData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRAcceleration(Data):\n",
    "    rVals = []\n",
    "    for i in range(np.shape(Data)[0]):\n",
    "        rVals.append(np.sqrt(Data[i,0]**2+Data[i,1]**2+Data[i,2]**2))\n",
    "    return rVals\n",
    "\n",
    "def getAcceleration(FileName):\n",
    "    try:\n",
    "        DataSet = np.genfromtxt(open(folder+FileName,'r'), delimiter=',',skip_header=0)\n",
    "        rData = getRAcceleration(DataSet[:,2:5])\n",
    "        rSmoothed = Smoothing(rData)\n",
    "        #return [[FileName,'x',DataSet[:,2]],[FileName,'y',DataSet[:,3]],[FileName,'z',DataSet[:,4]],[FileName,'r',rData]]\n",
    "        return [FileName, 'r', rSmoothed]\n",
    "    except:\n",
    "        return [False,FileName,False]\n",
    "\n",
    "def makePrints(DataArray):\n",
    "    try:\n",
    "        FingerPrint = getThumbprint(DataArray[2],'gaus2')\n",
    "        return [DataArray[0],DataArray[1],FingerPrint]\n",
    "    except:\n",
    "        return [DataArray[0], 'Fail', np.zeros(60000,500)]\n",
    "\n",
    "def getResults(FPnMd):\n",
    "    Ridges = RidgeCount(FPnMd[2][:,500:59500])\n",
    "    return [FPnMd[0],FPnMd[1],Ridges]\n",
    "\n",
    "def CountAboveThreshold(Ridges, Threshold = 10):\n",
    "    Cnum = np.count_nonzero(Ridges[2] >= Threshold)\n",
    "    return [Ridges[0],Ridges[1],Cnum]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def truthVector(Filename):\n",
    "    # Parses the filename, and compares it against the record of sensor position on cranes\n",
    "    # inputs: filename\n",
    "    # outputs: truth vector\n",
    "\n",
    "\n",
    "    #Parsing the file name.  Assuming it is in the standard format\n",
    "    sSensor = Filename[23]\n",
    "    sDate = datetime.datetime.strptime('20'+Filename[10:21],\"%Y%m%d-%H%M\")\n",
    "\n",
    "    mask = []\n",
    "\n",
    "    i=0\n",
    "    #loops through the known sensor movements, and creates a filter mask\n",
    "    for spf in OutputVectors:\n",
    "        \n",
    "        startDate = datetime.datetime.strptime(str(spf[0])+str(spf[1]).zfill(2)+str(spf[2]).zfill(2)\n",
    "            +str(spf[3]).zfill(2)+str(spf[4]).zfill(2),\"%Y%m%d%H%M\")\n",
    "        #datetime.date(int(spf[0]), int(spf[1]), int(spf[2])) + datetime.timedelta(hours=spf[3]) + datetime.timedelta(minutes=spf[4])\n",
    "        endDate = datetime.datetime.strptime(str(spf[5])+str(spf[6]).zfill(2)+str(spf[7]).zfill(2)\n",
    "            +str(spf[8]).zfill(2)+str(spf[9]).zfill(2),\"%Y%m%d%H%M\")\n",
    "        #datetime.date(int(spf[5]), int(spf[6]), int(spf[7])) + datetime.timedelta(hours=spf[8]) + datetime.timedelta(minutes=spf[9])\n",
    "        \n",
    "        if sDate >= startDate and sDate <= endDate and int(spf[10]) == int(sSensor):\n",
    "            mask.append(True)\n",
    "            i+=1\n",
    "        else:\n",
    "            mask.append(False)\n",
    "        \n",
    "    if i != 1: print('error ', i, Filename)\n",
    "\n",
    "    results = OutputVectors[mask,11:]\n",
    "\n",
    "    if i > 1: \n",
    "        print('Found Two ', Filename)\n",
    "        results = results[0,:]\n",
    "    #np.array(results)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def makeFrames(input): #,sequ,frameLength):\n",
    "    frames=[] #np.array([],dtype=object,)\n",
    "    segmentGap = int((np.shape(input)[1]-FrameLength)/numberFrames)\n",
    "    #print(segmentGap,sequ, frameLength)\n",
    "    for i in range(numberFrames):\n",
    "        start = i * segmentGap\n",
    "        imageMatrix = input[:,start:start+FrameLength]\n",
    "        np.matrix(imageMatrix)\n",
    "        imageMatrix = imageMatrix.T\n",
    "        frames.append(imageMatrix)\n",
    "    \n",
    "    return frames\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ParseData(FPwMD):\n",
    "\n",
    "    frames = np.asarray(makeFrames(FPwMD[2]))\n",
    "\n",
    "    Results = truthVector(FPwMD[0])\n",
    "\n",
    "    return frames, Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danhe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2007: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  result = asarray(a).shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have Data 0 1\n",
      "Have fingerprints 0 1\n"
     ]
    }
   ],
   "source": [
    "if np.size(files) % GroupSize ==0:\n",
    "    loops = int(np.size(files)/GroupSize)\n",
    "else:\n",
    "    loops = int(float(np.size(files))/float(GroupSize))+1\n",
    "#tmep for testing\n",
    "#loops=1\n",
    "\n",
    "for i in range(loops):\n",
    "    AllAccels = Parallel(n_jobs=num_cores)(delayed(getAcceleration)(file) for file in files[i*GroupSize:((i+1)*GroupSize)])\n",
    "    Flattened = []\n",
    "    for j in range(np.shape(AllAccels)[0]):\n",
    "        if AllAccels[j][0] == False:\n",
    "            print(j,AllAccels[j][1])\n",
    "        else: \n",
    "            Flattened.append(AllAccels[j])\n",
    "    print('Have Data',i,loops)\n",
    "    AllFingers =  Parallel(n_jobs=num_cores)(delayed(makePrints)(datas) for datas in Flattened)\n",
    "    print('Have fingerprints',i,loops)\n",
    "    #AllRidges = Parallel(n_jobs=num_cores)(delayed(getResults)(datas) for datas in AllFingers)\n",
    "    #print('Have ridgecounts')\n",
    "    #Events=[]\n",
    "    #Events = Parallel(n_jobs=num_cores)(delayed(CountAboveThreshold)(datas) for datas in AllRidges)\n",
    "    #\n",
    "    #Events = np.matrix(Events)\n",
    "    #df = pd.DataFrame(data=Events)\n",
    "    #df.to_csv(rootfolder +'Random Check' + str(i) + '.csv', sep=',', index = False, header=False,quotechar='\"')\n",
    "    #print(str(i+1)+' of '+str(loops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutputVectors = np.genfromtxt(open(SensorPositonFile,'r'), delimiter=',',skip_header=1,dtype=int, missing_values=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = Parallel(n_jobs=num_cores)(delayed(ParseData)(file) for file in AllFingers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Parsed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DataSet = [] \n",
    "ResultsSet = np.zeros((np.shape(Data)[0],np.shape(Data[0][1])[1]))\n",
    "i=0\n",
    "for datum in Data:\n",
    "    DataSet.append(datum[0])\n",
    "    ResultsSet[i]=datum[1][0]\n",
    "    i+=1\n",
    "\n",
    "DataSet = np.asarray(DataSet)\n",
    "\n",
    "print('Data Parsed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv_lstm2d (ConvLSTM2D)    (None, 98, 498, 64)       150016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 98, 498, 64)       0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3123456)           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               799604992 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 799,756,036\n",
      "Trainable params: 799,756,036\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#ResultsSet = ResultsSet[0:np.shape(DataSet)[0],:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(DataSet, ResultsSet, test_size=0.20, shuffle=True, random_state=0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(ConvLSTM2D(filters = 64, \n",
    "            kernel_size = (3, 3), \n",
    "            return_sequences = False, \n",
    "            data_format = \"channels_last\", \n",
    "            input_shape = (numberFrames, img_width, img_height, 1)\n",
    "            )\n",
    "        )\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(np.shape(y_train)[1], activation = \"softmax\"))\n",
    " \n",
    "model.summary()\n",
    " \n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])\n",
    " \n",
    "earlystop = EarlyStopping(patience=7)   \n",
    "callbacks = [earlystop]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Canceled future for execute_request message before replies were done",
     "output_type": "error",
     "traceback": [
      "Error: Canceled future for execute_request message before replies were done",
      "at t.KernelShellFutureHandler.dispose (c:\\Users\\danhe\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:1204175)",
      "at c:\\Users\\danhe\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:1223227",
      "at Map.forEach (<anonymous>)",
      "at v._clearKernelState (c:\\Users\\danhe\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:1223212)",
      "at v.dispose (c:\\Users\\danhe\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:1216694)",
      "at c:\\Users\\danhe\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:533674",
      "at t.swallowExceptions (c:\\Users\\danhe\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:913059)",
      "at dispose (c:\\Users\\danhe\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:533652)",
      "at t.RawSession.dispose (c:\\Users\\danhe\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:537330)",
      "at processTicksAndRejections (node:internal/process/task_queues:96:5)"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(x = X_train, y = y_train, epochs=TrainEpochs, batch_size = 8 , shuffle=False, validation_split=0.2, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.version)\n",
    "print(len(tf.config.list_physical_devices('GPU'))>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save(SaveModelFolder)\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig(rootfolder + 'ModelAccuracy.png')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig(rootfolder + 'ModelLoss.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
