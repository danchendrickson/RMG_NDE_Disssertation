{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Header used on the projects\n",
    "\n",
    "#first the major packages used for math and graphing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "import scipy.special as sp\n",
    "#import pandas as pd\n",
    "\n",
    "#Custome graph format style sheet\n",
    "#plt.style.use('Prospectus.mplstyle')\n",
    "\n",
    "#If being run by a seperate file, use the seperate file's graph format and saving paramaeters\n",
    "#otherwise set what is needed\n",
    "if not 'Saving' in locals():\n",
    "    Saving = True\n",
    "if not 'Titles' in locals():\n",
    "    Titles = True\n",
    "if not 'Ledgends' in locals():\n",
    "    Ledgends = True\n",
    "if not 'FFormat' in locals():\n",
    "    FFormat = '.png'\n",
    "Computer = \"SciClone\"   # \"SciClone\"    \"WinLap\"  \"LinLap\"   \"Desktop\"  \"PortLap\"\n",
    "\n",
    "#Standard cycle for collors and line styles\n",
    "default_cycler = (cycler('color', ['0.00', '0.40', '0.60', '0.70']) + cycler(linestyle=['-', '--', ':', '-.']))\n",
    "plt.rc('axes', prop_cycle=default_cycler)\n",
    "my_cmap = plt.get_cmap('gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import pywt\n",
    "import os as os\n",
    "import statistics as st\n",
    "import os as os\n",
    "import random\n",
    "import time\n",
    "import platform\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "tf.config.set_visible_devices([], 'GPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HostName = platform.node()\n",
    "dataSize = 'big'\n",
    "\n",
    "if HostName == \"Server\":\n",
    "    Computer = \"Desktop\"   \n",
    "elif HostName[-6:] == 'wm.edu':\n",
    "    Computer = \"SciClone\"\n",
    "elif HostName == \"SchoolLaptop\":\n",
    "    Computer = \"LinLap\"\n",
    "elif HostName == \"WTC-TAB-512\":\n",
    "    Computer = \"PortLap\"\n",
    "else:\n",
    "    Computer = \"WinLap\"\n",
    "\n",
    "if Computer == \"SciClone\":\n",
    "    location = '/sciclone/home20/dchendrickson01/'\n",
    "elif Computer == \"WinLap\":\n",
    "    location = 'C:\\\\Data\\\\'\n",
    "elif Computer == \"Desktop\":\n",
    "    location = \"E:\\\\Backups\\\\Dan\\\\CraneData\\\\\"\n",
    "elif Computer == \"LinLap\":\n",
    "    location = '/home/dan/Output/'\n",
    "\n",
    "if Computer ==  \"SciClone\":\n",
    "    rootfolder = '/sciclone/home20/dchendrickson01/'\n",
    "    if dataSize == 'big':\n",
    "        folder = '/sciclone/scr10/dchendrickson01/CraneData/'\n",
    "        imFolder ='/sciclone/scr10/dchendrickson01/BigPrints/'\n",
    "    else:\n",
    "        folder = '/sciclone/data10/dchendrickson01/SmallCopy/'\n",
    "elif Computer == \"Desktop\":\n",
    "    rootfolder = location\n",
    "    if dataSize == 'big':\n",
    "        folder = 'G:\\\\CraneData\\\\'\n",
    "    else:\n",
    "        folder = rootfolder + \"SmallCopy\\\\\"\n",
    "elif Computer ==\"WinLap\":\n",
    "    rootfolder = location\n",
    "    folder = rootfolder + \"SmallCopy\\\\\"   \n",
    "elif Computer == \"LinLap\":\n",
    "    rootfolder = '/home/dan/Data/'\n",
    "    folder = rootfolder + 'SmallCopy/'\n",
    "    \n",
    "#Pre Calculations\n",
    "num_cores = multiprocessing.cpu_count()-1\n",
    "\n",
    "\n",
    "num_classes = 2\n",
    "input_shape = (500, 10000, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have Files\n"
     ]
    }
   ],
   "source": [
    "                                 \n",
    "Good_files =[os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(imFolder+'Good/')) for f in fn]\n",
    "Bad_files =[os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(imFolder+'Bad/')) for f in fn]\n",
    "\n",
    "print('Have Files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Images = []\n",
    "Truth = []\n",
    "  \n",
    "def addImage(file):\n",
    "    pathname, extension = os.path.splitext(file)\n",
    "    filetree = pathname.split('/')\n",
    "    foldername = filetree[-2]\n",
    "    if foldername == 'Good':\n",
    "        truth = [0]\n",
    "        image = cv2.imread(file, flags=cv2.IMREAD_COLOR)\n",
    "        return truth,image\n",
    "    elif foldername == 'Bad':\n",
    "        truth = [1]\n",
    "        image = cv2.imread(file, flags=cv2.IMREAD_COLOR)\n",
    "        return truth,image\n",
    "    else:\n",
    "        return None\n",
    "j=0\n",
    "\n",
    "tg = Parallel(n_jobs=num_cores)(delayed(addImage)(file) for file in Good_files[:1000])\n",
    "tb = Parallel(n_jobs=num_cores)(delayed(addImage)(file) for file in Bad_files[:1000])\n",
    "\n",
    "\n",
    "Truth = np.concatenate((tg[0], tb[0]), axis=0)\n",
    "Images = np.concatenate((tg[1], tb[1]), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for file in Good_files[:1000]:\n",
    "    t ,i  = addImage(file) \n",
    "    Images.append(np.asarray(i))\n",
    "    Truth.append(np.asarray(t))\n",
    "    j+=1\n",
    "    if j%1000==0:print(j)\n",
    "j=0\n",
    "for file in Bad_files[:1000]:\n",
    "    t ,i  = addImage(file) \n",
    "    Images.append(np.asarray(i))\n",
    "    Truth.append(np.asarray(t))\n",
    "    j+=1\n",
    "    if j%1000==0:print(j)\n",
    "print('sorted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Images=np.asarray(Images)\n",
    "Truth=np.asarray(Truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train, x_test,y_train,y_test = train_test_split(Images,Truth, test_size=0.15, shuffle=True, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (3,) - y_train shape: (3,)\n",
      "x_test shape: (1,) - y_test shape: (1,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n## Configure the hyperparameters\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"x_train shape: {np.shape(x_train)} - y_train shape: {np.shape(y_train)}\")\n",
    "print(f\"x_test shape: {np.shape(x_test)} - y_test shape: {np.shape(y_test)}\")\n",
    "\n",
    "\"\"\"\n",
    "## Configure the hyperparameters\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 256\n",
    "num_epochs = 100\n",
    "image_l = 10000  # We'll resize input images to this size\n",
    "image_w = 500\n",
    "patch_size = 10  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_l // patch_size) * (image_w // patch_size)\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Use data augmentation\n",
    "\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)\n",
    "\n",
    "\n",
    "\n",
    "## Implement multilayer perceptron (MLP)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Implement patch creation as a layer\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        patches=np.asarray(patches)\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      8\u001b[0m image \u001b[38;5;241m=\u001b[39m x_train[np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mrange\u001b[39m(x_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))]\n\u001b[0;32m----> 9\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m resized_image \u001b[38;5;241m=\u001b[39m [image] \u001b[38;5;66;03m#tf.image.resize(\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'astype'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    \n",
    "\"\"\"\n",
    "Let's display patches for a sample image\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
    "plt.imshow(image.astype(\"uint8\"))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "resized_image = [image] #tf.image.resize(\n",
    "#    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
    "#)\n",
    "\n",
    "patches = Patches(patch_size)(resized_image)\n",
    "print(f\"Image size: {image_l} X {image_w}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "print(f\"Patches per image: {patches.shape[1]}\")\n",
    "print(f\"Elements per patch: {patches.shape[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n = int(image_l / patch_size) #int(np.sqrt(patches.shape[1]))\n",
    "m = int(image_w / patch_size)\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, m, i + 1)\n",
    "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
    "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## Implement the patch encoding layer\\nThe `PatchEncoder` layer will linearly transform a patch by projecting it into a\\nvector of size `projection_dim`. In addition, it adds a learnable position\\nembedding to the projected vector.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Implement the patch encoding layer\n",
    "The `PatchEncoder` layer will linearly transform a patch by projecting it into a\n",
    "vector of size `projection_dim`. In addition, it adds a learnable position\n",
    "embedding to the projected vector.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Build the ViT model\n",
    "The ViT model consists of multiple Transformer blocks,\n",
    "which use the `layers.MultiHeadAttention` layer as a self-attention mechanism\n",
    "applied to the sequence of patches. The Transformer blocks produce a\n",
    "`[batch_size, num_patches, projection_dim]` tensor, which is processed via an\n",
    "classifier head with softmax to produce the final class probabilities output.\n",
    "Unlike the technique described in the [paper](https://arxiv.org/abs/2010.11929),\n",
    "which prepends a learnable embedding to the sequence of encoded patches to serve\n",
    "as the image representation, all the outputs of the final Transformer block are\n",
    "reshaped with `layers.Flatten()` and used as the image\n",
    "representation input to the classifier head.\n",
    "Note that the `layers.GlobalAveragePooling1D` layer\n",
    "could also be used instead to aggregate the outputs of the Transformer block,\n",
    "especially when the number of patches and the projection dimensions are large.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=(image_w,image_l))\n",
    "    # Augment data.\n",
    "    augmented = inputs #data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Compile, train, and evaluate the mode\n",
    "\"\"\"\n",
    "\n",
    "def run_experiment(model):\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate = learning_rate,\n",
    "                                momentum=0.0, \n",
    "                                nesterov=False, \n",
    "                                name='SGD')\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"patches\" (type Patches).\n\nin user code:\n\n    File \"/local/scr/dchendrickson01/TMPDIR/ipykernel_175450/3543310222.py\", line 43, in call  *\n        patches = tf.image.extract_patches(\n\n    ValueError: Shape must be rank 4 but is rank 3 for '{{node patches/ExtractImagePatches}} = ExtractImagePatches[T=DT_FLOAT, ksizes=[1, 10, 10, 1], padding=\"VALID\", rates=[1, 1, 1, 1], strides=[1, 10, 10, 1]](Placeholder)' with input shapes: [?,500,10000].\n\n\nCall arguments received:\n  • images=tf.Tensor(shape=(None, 500, 10000), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vit_classifier \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_vit_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m history \u001b[38;5;241m=\u001b[39m run_experiment(vit_classifier)\n",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36mcreate_vit_classifier\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m augmented \u001b[38;5;241m=\u001b[39m inputs \u001b[38;5;66;03m#data_augmentation(inputs)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Create patches.\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m patches \u001b[38;5;241m=\u001b[39m \u001b[43mPatches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43maugmented\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Encode patches.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m encoded_patches \u001b[38;5;241m=\u001b[39m PatchEncoder(num_patches, projection_dim)(patches)\n",
      "File \u001b[0;32m~/.conda/envs/tfcgpu/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.conda/envs/tfcgpu/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:692\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 692\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m    693\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"patches\" (type Patches).\n\nin user code:\n\n    File \"/local/scr/dchendrickson01/TMPDIR/ipykernel_175450/3543310222.py\", line 43, in call  *\n        patches = tf.image.extract_patches(\n\n    ValueError: Shape must be rank 4 but is rank 3 for '{{node patches/ExtractImagePatches}} = ExtractImagePatches[T=DT_FLOAT, ksizes=[1, 10, 10, 1], padding=\"VALID\", rates=[1, 1, 1, 1], strides=[1, 10, 10, 1]](Placeholder)' with input shapes: [?,500,10000].\n\n\nCall arguments received:\n  • images=tf.Tensor(shape=(None, 500, 10000), dtype=float32)"
     ]
    }
   ],
   "source": [
    "vit_classifier = create_vit_classifier()\n",
    "history = run_experiment(vit_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(imFolder+'Model/')\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig(imFolder + 'ModelAccuracy.png')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig(imFolder + 'ModelLoss.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c67ee391b46d44d4606b34baa2c48797dd5e9b304d9504fa8fe1f4850e129826"
  },
  "kernelspec": {
   "display_name": "tfcgpu",
   "language": "python",
   "name": "tfcgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
