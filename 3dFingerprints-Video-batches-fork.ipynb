{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/bin/python\n",
    "\n",
    "#\n",
    "#  Works in CPU Mode not GPU\n",
    "#\n",
    "\n",
    "#Standard Header used on the projects\n",
    "# %%\n",
    "Computer = \"SciClone\" # \"SciClone\"    \"WinLap\"  \"LinLap\"   \"Desktop\" \n",
    "\n",
    "\n",
    "#first the major packages used for math and graphing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "\n",
    "import os as os\n",
    "import random\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import pywt\n",
    "from pywt._extensions._pywt import (DiscreteContinuousWavelet, ContinuousWavelet,\n",
    "                                Wavelet, _check_dtype)\n",
    "from pywt._functions import integrate_wavelet, scale2frequency\n",
    "from time import time as ti\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential, Model \n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import  EarlyStopping\n",
    " \n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import tensorflow.keras_metrics as km\n",
    "  \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "\n",
    "if Computer == \"SciClone\":\n",
    "    location = '/sciclone/home20/dchendrickson01/'\n",
    "elif Computer == \"WinLap\":\n",
    "    location = 'C:\\\\Data\\\\'\n",
    "elif Computer == \"Desktop\":\n",
    "    location = \"E:\\\\Backups\\\\Dan\\\\CraneData\\\\\"\n",
    "elif Computer == \"LinLap\":\n",
    "    location = '/home/dan/Output/'\n",
    "    \n",
    "\n",
    "if Computer ==  \"SciClone\":\n",
    "    rootfolder = '/sciclone/home20/dchendrickson01/'\n",
    "    folder = '/sciclone/data10/dchendrickson01/SmallCopy/'\n",
    "elif Computer == \"Desktop\":\n",
    "    rootfolder = location\n",
    "    folder = rootfolder + \"SmallCopy\\\\\"\n",
    "elif Computer ==\"WinLap\":\n",
    "    rootfolder = location\n",
    "    folder = rootfolder + \"SmallCopy\\\\\"   \n",
    "elif Computer == \"LinLap\":\n",
    "    rootfolder = '/home/dan/Data/'\n",
    "    folder = rootfolder + 'SmallCopy/'\n",
    "    \n",
    "\n",
    "scales = 500\n",
    "#img_height , img_width = scales, 200\n",
    "FrameLength = 600\n",
    "numberFrames = 600\n",
    "DoSomeFiles = True\n",
    "#NumberOfFiles = 45\n",
    "SmoothType = 1  # 0 = none, 1 = rolling average, 2 = rolling StdDev\n",
    "SmoothDistance=25\n",
    "TrainEpochs = 1\n",
    "WaveletToUse = 'db3'\n",
    "num_cores = multiprocessing.cpu_count() -1\n",
    "SensorPositonFile = rootfolder + 'SensorStatsSmall.csv'\n",
    "if Computer == \"SciClone\" or Computer == \"LinLap\":\n",
    "    SaveModelFolder = rootfolder + 'SavedModel/'\n",
    "else:\n",
    "    SaveModelFolder = rootfolder + 'SavedModel\\\\'\n",
    "\n",
    "files = os.listdir(folder)\n",
    "#if DoSomeFiles: files = random.sample(files,NumberOfFiles)\n",
    "\n",
    "GroupSize = 25\n",
    "OutputVectors = np.genfromtxt(open(SensorPositonFile,'r'), delimiter=',',skip_header=1,dtype=int, missing_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CoreFunctions as cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cwt_fixed(data, scales, wavelet, sampling_period=1., spacer = 5):\n",
    "    \"\"\"\n",
    "    COPIED AND FIXED FROM pywt.cwt TO BE ABLE TO USE WAVELET FAMILIES SUCH\n",
    "    AS COIF AND DB\n",
    "\n",
    "    COPIED From Spenser Kirn\n",
    "    \n",
    "    All wavelet work except bior family, rbio family, haar, and db1.\n",
    "    \n",
    "    cwt(data, scales, wavelet)\n",
    "\n",
    "    One dimensional Continuous Wavelet Transform.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : array_like\n",
    "        Input signal\n",
    "    scales : array_like\n",
    "        scales to use\n",
    "    wavelet : Wavelet object or name\n",
    "        Wavelet to use\n",
    "    sampling_period : float\n",
    "        Sampling period for frequencies output (optional)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    coefs : array_like\n",
    "        Continous wavelet transform of the input signal for the given scales\n",
    "        and wavelet\n",
    "    frequencies : array_like\n",
    "        if the unit of sampling period are seconds and given, than frequencies\n",
    "        are in hertz. Otherwise Sampling period of 1 is assumed.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Size of coefficients arrays depends on the length of the input array and\n",
    "    the length of given scales.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pywt\n",
    "    >>> import numpy as np\n",
    "    >>> import matplotlib.pyplot as plt\n",
    "    >>> x = np.arange(512)\n",
    "    >>> y = np.sin(2*np.pi*x/32)\n",
    "    >>> coef, freqs=pywt.cwt(y,np.arange(1,129),'gaus1')\n",
    "    >>> plt.matshow(coef) # doctest: +SKIP\n",
    "    >>> plt.show() # doctest: +SKIP\n",
    "    ----------\n",
    "    >>> import pywt\n",
    "    >>> import numpy as np\n",
    "    >>> import matplotlib.pyplot as plt\n",
    "    >>> t = np.linspace(-1, 1, 200, endpoint=False)\n",
    "    >>> sig  = np.cos(2 * np.pi * 7 * t) + np.real(np.exp(-7*(t-0.4)**2)*np.exp(1j*2*np.pi*2*(t-0.4)))\n",
    "    >>> widths = np.arange(1, 31)\n",
    "    >>> cwtmatr, freqs = pywt.cwt(sig, widths, 'mexh')\n",
    "    >>> plt.imshow(cwtmatr, extent=[-1, 1, 1, 31], cmap='PRGn', aspect='auto',\n",
    "    ...            vmax=abs(cwtmatr).max(), vmin=-abs(cwtmatr).max())  # doctest: +SKIP\n",
    "    >>> plt.show() # doctest: +SKIP\n",
    "    \"\"\"\n",
    "\n",
    "    # accept array_like input; make a copy to ensure a contiguous array\n",
    "    dt = _check_dtype(data)\n",
    "    data = np.array(data, dtype=dt)\n",
    "    if not isinstance(wavelet, (ContinuousWavelet, Wavelet)):\n",
    "        wavelet = DiscreteContinuousWavelet(wavelet)\n",
    "    if np.isscalar(scales):\n",
    "        scales = np.array([scales])*spacer\n",
    "    if data.ndim == 1:\n",
    "        try:\n",
    "            if wavelet.complex_cwt:\n",
    "                out = np.zeros((np.size(scales), data.size), dtype=complex)\n",
    "            else:\n",
    "                out = np.zeros((np.size(scales), data.size))\n",
    "        except AttributeError:\n",
    "            out = np.zeros((np.size(scales), data.size))\n",
    "        for i in np.arange(np.size(scales)):\n",
    "            precision = 10\n",
    "            int_psi, x = integrate_wavelet(wavelet, precision=precision)\n",
    "            step = x[1] - x[0]\n",
    "            j = np.floor(\n",
    "                np.arange(scales[i] * (x[-1] - x[0]) + 1) / (scales[i] * step))\n",
    "            if np.max(j) >= np.size(int_psi):\n",
    "                j = np.delete(j, np.where((j >= np.size(int_psi)))[0])\n",
    "            coef = - np.sqrt(scales[i]) * np.diff(\n",
    "                np.convolve(data, int_psi[j.astype(int)][::-1]))\n",
    "            d = (coef.size - data.size) / 2.\n",
    "            out[i, :] = coef[int(np.floor(d)):int(-np.ceil(d))]\n",
    "        frequencies = scale2frequency(wavelet, scales, precision)\n",
    "        if np.isscalar(frequencies):\n",
    "            frequencies = np.array([frequencies])\n",
    "        for i in np.arange(len(frequencies)):\n",
    "            frequencies[i] /= sampling_period\n",
    "        return out, frequencies\n",
    "    else:\n",
    "        raise ValueError(\"Only dim == 1 supported\")\n",
    "\n",
    "def getThumbprint(data, wvt=WaveletToUse, ns=scales, numslices=5, slicethickness=0.12, \n",
    "                  valleysorpeaks='both', normconstant=1, plot=True):\n",
    "    '''\n",
    "    STarted with Spenser Kirn's code, modifed by DCH\n",
    "    Updated version of the DWFT function above that allows plotting of just\n",
    "    valleys or just peaks or both. To plot just valleys set valleysorpeaks='valleys'\n",
    "    to plot just peaks set valleysorpeaks='peaks' or 'both' to plot both.\n",
    "    '''\n",
    "    # First take the wavelet transform and then normalize to one\n",
    "    cfX, freqs = cwt_fixed(data, np.arange(1,ns+1), wvt)\n",
    "    cfX = np.true_divide(cfX, abs(cfX).max()*normconstant)\n",
    "    '''\n",
    "    fp = np.zeros((len(data), ns), dtype=int)\n",
    "    \n",
    "    # Create the list of locations between -1 and 1 to preform slices. Valley\n",
    "    # slices will all be below 0 and peak slices will all be above 0.\n",
    "    if valleysorpeaks == 'both':\n",
    "        slicelocations1 = np.arange(-1 ,0.0/numslices, 1.0/numslices)\n",
    "        slicelocations2 = np.arange(1.0/numslices, 1+1.0/numslices, 1.0/numslices)\n",
    "        slicelocations = np.array(np.append(slicelocations1,slicelocations2))\n",
    "        \n",
    "    for loc in slicelocations:\n",
    "        for y in range(0, ns):\n",
    "            for x in range(0, len(data)):\n",
    "                if cfX[y, x]>=(loc-(slicethickness/2)) and cfX[y,x]<= (loc+(slicethickness/2)):\n",
    "                    fp[x,y] = 1\n",
    "                    \n",
    "    fp = np.transpose(fp[:,:ns])\n",
    "    return fp\n",
    "    '''\n",
    "    return cfX\n",
    "\n",
    "def RidgeCount(fingerprint):\n",
    "    '''\n",
    "    From Spencer Kirn\n",
    "    Count the number of times the fingerprint changes from 0 to 1 or 1 to 0 in \n",
    "    consective rows. Gives a vector representation of the DWFT\n",
    "    '''\n",
    "    diff = np.zeros((fingerprint.shape))\n",
    "    \n",
    "    for i, row in enumerate(fingerprint):\n",
    "        if i==0:\n",
    "            prev = row\n",
    "        else:\n",
    "            # First row (i=0) of diff will always be 0s because it does not\n",
    "            # matter what values are present. \n",
    "            # First find where the rows differ\n",
    "            diff_vec = abs(row-prev)\n",
    "            # Then set those differences to 1 to be added later\n",
    "            diff_vec[diff_vec != 0] = 1\n",
    "            diff[i, :] = diff_vec\n",
    "            \n",
    "            prev = row\n",
    "            \n",
    "    ridgeCount = diff.sum(axis=0)\n",
    "    \n",
    "    return ridgeCount\n",
    "\n",
    "def Smoothing(RawData, SmoothType = 3, SmoothDistance=15):\n",
    "\n",
    "    if SmoothType == 0:\n",
    "        SmoothedData = RawData\n",
    "    elif SmoothType ==1:\n",
    "        SmoothedData = RawData\n",
    "        if np.shape(np.shape(RawData))== 2:\n",
    "            for i in range(SmoothDistance-1):\n",
    "                for j in range(3):\n",
    "                    SmoothedData[j,i+1]=np.average(RawData[j,0:i+1])\n",
    "            for i in range(np.shape(RawData)[0]-SmoothDistance):\n",
    "                for j in range(3):\n",
    "                    SmoothedData[j,i+SmoothDistance]=np.average(RawData[j,i:i+SmoothDistance])\n",
    "        elif np.shape(np.shape(RawData))== 1:\n",
    "            for i in range(SmoothDistance-1):\n",
    "                SmoothedData[i+1]=np.average(RawData[0:i+1])\n",
    "            for i in range(np.shape(RawData)[0]-SmoothDistance):\n",
    "                SmoothedData[i+SmoothDistance]=np.average(RawData[i:i+SmoothDistance])\n",
    "    elif SmoothType == 3:\n",
    "        SmoothedData = cf.KalmanFilterDenoise(RawData)\n",
    "\n",
    "    return SmoothedData\n",
    "\n",
    "def getRAcceleration(Data):\n",
    "    rVals = []\n",
    "    for i in range(np.shape(Data)[0]):\n",
    "        rVals.append(np.sqrt(Data[i,0]**2+Data[i,1]**2+Data[i,2]**2))\n",
    "    return rVals\n",
    "\n",
    "def getAcceleration(FileName):\n",
    "    try:\n",
    "        DataSet = np.genfromtxt(open(folder+FileName,'r'), delimiter=',',skip_header=0)\n",
    "        #rData = getRAcceleration(DataSet[:,2:5])\n",
    "        #rSmoothed = Smoothing(rData)\n",
    "        xSmoothed = Smoothing(DataSet[:,2])\n",
    "        ySmoothed = Smoothing(DataSet[:,3])\n",
    "        zSmoothed = Smoothing(DataSet[:,4])\n",
    "        #return [[FileName,'x',DataSet[:,2]],[FileName,'y',DataSet[:,3]],[FileName,'z',DataSet[:,4]],[FileName,'r',rData]]\n",
    "        return [FileName, 'r', [xSmoothed, ySmoothed, zSmoothed]]\n",
    "    except:\n",
    "        return [False,FileName,False]\n",
    "\n",
    "def makePrints(DataArray):\n",
    "    try:\n",
    "        FingerPrint = getThumbprint(DataArray[2],'gaus2')\n",
    "        return [DataArray[0],DataArray[1],FingerPrint.T]\n",
    "    except:\n",
    "        return [DataArray[0], 'Fail', np.zeros(60000,500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeMatrixPrints(DataMatrix):\n",
    "\n",
    "    xPrint = getThumbprint(np.asarray(DataMatrix[0]).flatten())\n",
    "    yPrint = getThumbprint(np.asarray(DataMatrix[1]).flatten())\n",
    "    zPrint = getThumbprint(np.asarray(DataMatrix[2]).flatten())\n",
    "\n",
    "    PrintMatrix = np.dstack((xPrint,yPrint,zPrint))\n",
    "    \n",
    "    return [PrintMatrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResults(FPnMd):\n",
    "    Ridges = RidgeCount(FPnMd[2][:,500:59500])\n",
    "    return [FPnMd[0],FPnMd[1],Ridges]\n",
    "\n",
    "def CountAboveThreshold(Ridges, Threshold = 10):\n",
    "    Cnum = np.count_nonzero(Ridges[2] >= Threshold)\n",
    "    return [Ridges[0],Ridges[1],Cnum]\n",
    "\n",
    "def truthVector(Filename):\n",
    "    # Parses the filename, and compares it against the record of sensor position on cranes\n",
    "    # inputs: filename\n",
    "    # outputs: truth vector\n",
    "\n",
    "\n",
    "    #Parsing the file name.  Assuming it is in the standard format\n",
    "    sSensor = Filename[23]\n",
    "    sDate = datetime.datetime.strptime('20'+Filename[10:21],\"%Y%m%d-%H%M\")\n",
    "\n",
    "    mask = []\n",
    "\n",
    "    i=0\n",
    "    #loops through the known sensor movements, and creates a filter mask\n",
    "    for spf in OutputVectors:\n",
    "        \n",
    "        startDate = datetime.datetime.strptime(str(spf[0])+str(spf[1]).zfill(2)+str(spf[2]).zfill(2)\n",
    "            +str(spf[3]).zfill(2)+str(spf[4]).zfill(2),\"%Y%m%d%H%M\")\n",
    "        #datetime.date(int(spf[0]), int(spf[1]), int(spf[2])) + datetime.timedelta(hours=spf[3]) + datetime.timedelta(minutes=spf[4])\n",
    "        endDate = datetime.datetime.strptime(str(spf[5])+str(spf[6]).zfill(2)+str(spf[7]).zfill(2)\n",
    "            +str(spf[8]).zfill(2)+str(spf[9]).zfill(2),\"%Y%m%d%H%M\")\n",
    "        #datetime.date(int(spf[5]), int(spf[6]), int(spf[7])) + datetime.timedelta(hours=spf[8]) + datetime.timedelta(minutes=spf[9])\n",
    "        \n",
    "        if sDate >= startDate and sDate <= endDate and int(spf[10]) == int(sSensor):\n",
    "            mask.append(True)\n",
    "            i+=1\n",
    "        else:\n",
    "            mask.append(False)\n",
    "        \n",
    "    if i != 1: print('error ', i, Filename)\n",
    "\n",
    "    results = OutputVectors[mask,11:]\n",
    "\n",
    "    if i > 1: \n",
    "        print('Found Two ', Filename)\n",
    "        results = results[0,:]\n",
    "    #np.array(results)\n",
    "\n",
    "    return results\n",
    "\n",
    "def makeFrames(input): #,sequ,frameLength):\n",
    "    frames=[] #np.array([],dtype=object,)\n",
    "    segmentGap = int((np.shape(input)[1]-FrameLength)/numberFrames)\n",
    "    #print(segmentGap,sequ, frameLength)\n",
    "    for i in range(numberFrames):\n",
    "        start = i * segmentGap\n",
    "        imageMatrix = input[:,start:start+FrameLength]\n",
    "        np.matrix(imageMatrix)\n",
    "        imageMatrix = imageMatrix.T\n",
    "\n",
    "        w0=int(np.shape(imageMatrix)[0]/2)\n",
    "        a0=np.linspace(0,w0-1,w0,dtype=int)*2\n",
    "        imageMatrix = np.delete(imageMatrix,a0,axis=0)\n",
    "\n",
    "        w1=int(np.shape(imageMatrix)[1]/2)\n",
    "        a1=np.linspace(0,w1-1,w1,dtype=int)*2\n",
    "        imageMatrix = np.delete(imageMatrix,a1,axis=1)\n",
    "\n",
    "        #for j in range(np.shape(imageMatrix)[0]):\n",
    "        #    for k in range(np.shape(imageMatrix)[1]):\n",
    "        #        imageMatrix[j,k] = [imageMatrix[j,k]]\n",
    "\n",
    "        frames.append(np.asarray(imageMatrix))\n",
    "    \n",
    "    return frames\n",
    "\n",
    "def make3dFrames(input, SizeReduction = 2): #,sequ,frameLength):\n",
    "    frames=[] #np.array([],dtype=object,)\n",
    "    segmentGap = int((np.shape(input)[1]-FrameLength)/numberFrames)\n",
    "    #print(segmentGap,sequ, frameLength)\n",
    "    for i in range(numberFrames):\n",
    "        start = i * segmentGap\n",
    "        imageMatrix = input[:,start:start+FrameLength,:]\n",
    "        #np.matrix(imageMatrix)\n",
    "        #imageMatrix = imageMatrix.T\n",
    "\n",
    "        w0=int(np.shape(imageMatrix)[0]/SizeReduction)\n",
    "        a0=np.linspace(0,w0-1,w0,dtype=int)*SizeReduction\n",
    "        imageMatrix = np.delete(imageMatrix,a0,axis=0)\n",
    "\n",
    "        w1=int(np.shape(imageMatrix)[1]/SizeReduction)\n",
    "        a1=np.linspace(0,w1-1,w1,dtype=int)*SizeReduction\n",
    "        imageMatrix = np.delete(imageMatrix,a1,axis=1)\n",
    "\n",
    "        \n",
    "        w0=int(np.shape(imageMatrix)[0]/SizeReduction)\n",
    "        a0=np.linspace(0,w0-1,w0,dtype=int)*SizeReduction\n",
    "        imageMatrix = np.delete(imageMatrix,a0,axis=0)\n",
    "\n",
    "        w1=int(np.shape(imageMatrix)[1]/SizeReduction)\n",
    "        a1=np.linspace(0,w1-1,w1,dtype=int)*SizeReduction\n",
    "        imageMatrix = np.delete(imageMatrix,a1,axis=1)\n",
    "        #for j in range(np.shape(imageMatrix)[0]):\n",
    "        #    for k in range(np.shape(imageMatrix)[1]):\n",
    "        #        imageMatrix[j,k] = [imageMatrix[j,k]]\n",
    "\n",
    "        frames.append(np.asarray(imageMatrix,dtype=int))\n",
    "    \n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ParseData(FPwMD):\n",
    "\n",
    "    frames = np.asarray(make3dFrames(FPwMD))\n",
    "\n",
    "    #Results = truthVector(FPwMD[0])\n",
    "\n",
    "    return frames #, Results\n",
    "\n",
    "\n",
    "def KalmanGroup(DataMatrix):\n",
    "    waveKalmaned = np.asarray([],dtype=object)\n",
    "    waveKalmaned = Parallel(n_jobs=num_cores)(delayed(cf.KalmanFilterDenoise)(np.asarray(data).flatten()) for data in DataMatrix)\n",
    "    waveKalmaned = np.matrix(waveKalmaned)\n",
    "    length = np.shape(waveKalmaned)[0]\n",
    "    justifier = np.ones((length, np.shape(waveKalmaned)[1]))\n",
    "    average = np.zeros(length)\n",
    "    for i in range(length):\n",
    "        average[i]= np.average(waveKalmaned[i][:])\n",
    "    justifier = justifier.T * average.T\n",
    "    waveKalmaned = waveKalmaned - justifier.T\n",
    "    \n",
    "    return waveKalmaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakingTrainingSet(files):\n",
    "    AllFingers = []\n",
    "\n",
    "    AllAccels = Parallel(n_jobs=num_cores)(delayed(cf.getAcceleration)(file) for file in files)\n",
    "    Flattened = []\n",
    "    for j in range(np.shape(AllAccels)[0]):\n",
    "        if AllAccels[j][0] == False:\n",
    "            print(j,AllAccels[j][1])\n",
    "        else: \n",
    "            Flattened.append(AllAccels[j])\n",
    "    print('Have Data')\n",
    "\n",
    "    MetaData = []  #np.asarray([],dtype=object)\n",
    "    DataOnlyMatrix = np.asarray([],dtype=object)\n",
    "    for j in range(np.shape(AllAccels)[0]):\n",
    "        if AllAccels[j][0] == False :\n",
    "            if AllAccels[j][1][4:9] =='Accel':\n",
    "                print(j,AllAccels[j][1])\n",
    "        else: \n",
    "            for k in range(3):\n",
    "                MetaData.append([AllAccels[j][k][0], AllAccels[j][k][1], AllAccels[j][k][3], AllAccels[j][k][4]])\n",
    "                if np.size(DataOnlyMatrix) == 0:\n",
    "                        DataOnlyMatrix =np.matrix(AllAccels[j][k][2])\n",
    "                else:\n",
    "                        DataOnlyMatrix = np.concatenate((DataOnlyMatrix,np.matrix(AllAccels[j][k][2])),axis=0)\n",
    "\n",
    "    MetaData = np.matrix(MetaData)\n",
    "\n",
    "    AllAccels = KalmanGroup(DataOnlyMatrix)\n",
    "\n",
    "    maxes = np.amax(AllAccels[:,500:], axis = 1)\n",
    "    mins = np.amin(AllAccels[:,500:], axis = 1)\n",
    "\n",
    "    Keep = np.zeros(mins.size)\n",
    "    for i in range(mins.size):\n",
    "        if i % 3 == 0:\n",
    "            if maxes[i] > 0.01 and mins[i] < -0.01:\n",
    "                Keep[i]=1\n",
    "                Keep[i+1]=1\n",
    "                Keep[i+2]=1\n",
    "                #print(i)\n",
    "\n",
    "\n",
    "    Keep = np.array(Keep, dtype='bool')\n",
    "\n",
    "    AllAccels = AllAccels[Keep,:]\n",
    "    MetaData = MetaData[Keep,:]\n",
    "\n",
    "    MotionsLeft = int(np.shape(AllAccels)[0]/3.0)\n",
    "\n",
    "    AllFingers =  Parallel(n_jobs=num_cores)(delayed(makeMatrixPrints)([AllAccels[i*3],AllAccels[i*3+1],AllAccels[i*3+2]]) for i in range(MotionsLeft))\n",
    "\n",
    "    print('Have fingerprints')\n",
    "\n",
    "    DataSet = Parallel(n_jobs=num_cores)(delayed(ParseData)(file[0]) for file in AllFingers)\n",
    "    DataSet = np.asarray(DataSet)\n",
    "\n",
    "    ResultsSet = []\n",
    "    for i in range(MotionsLeft):\n",
    "        ResultsSet.append(np.asarray(cf.truthVector(MetaData[i*3,3])).flatten())\n",
    "\n",
    "    print('Data Parsed')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(DataSet, ResultsSet, test_size=0.20, shuffle=True, random_state=0)\n",
    "    y_train = np.matrix(y_train)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2007: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  result = asarray(a).shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have fingerprints\n",
      "Data Parsed\n"
     ]
    }
   ],
   "source": [
    "fCount = len(files)\n",
    "GroupsLeft = int(fCount/GroupSize) + 1\n",
    "\n",
    "SplitRatio = 1/(GroupsLeft)\n",
    "\n",
    "RemainingFiles, GroupFiles, x,y = train_test_split(files, range(len(files)), test_size=SplitRatio, shuffle=True, random_state=0)\n",
    "\n",
    "GroupsLeft -=1\n",
    "\n",
    "X_train, X_test, y_train, y_test = MakingTrainingSet(GroupFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(ConvLSTM2D(filters = 16, \n",
    "            kernel_size = (5, 5), \n",
    "            return_sequences = False, \n",
    "            data_format = \"channels_last\", \n",
    "            input_shape = (numberFrames, np.shape(X_train[0][0])[0], np.shape(X_train[0][0])[1],3)\n",
    "            )\n",
    "        )\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(np.shape(y_train)[1], activation = \"softmax\"))\n",
    " \n",
    "model.summary()\n",
    "\n",
    " \n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "earlystop = EarlyStopping(patience=7)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [earlystop]\n",
    "\n",
    "#trainable_var = tf.trainable_variables()\n",
    "#sess = tf.Session()\n",
    "#saver = ft.train.Saver()\n",
    "#sess.run(tf.global_variables_initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 81s 81s/step - loss: 1.3860 - accuracy: 0.3333 - val_loss: 1.3306 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x = X_train, y = y_train, epochs=1, batch_size = 8 , shuffle=False, validation_split=0.25, callbacks=callbacks)\n",
    "#saver.save(sess,'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have fingerprints\n",
      "Data Parsed\n",
      "2/2 [==============================] - 109s 22s/step - loss: 1.4000 - accuracy: 0.1111 - val_loss: 1.1799 - val_accuracy: 0.3333\n",
      "Have Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have fingerprints\n",
      "Data Parsed\n",
      "2/2 [==============================] - 107s 20s/step - loss: 1.5257 - accuracy: 0.4444 - val_loss: 1.4179 - val_accuracy: 0.0000e+00\n",
      "Have Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have fingerprints\n",
      "Data Parsed\n",
      "2/2 [==============================] - 120s 36s/step - loss: 1.3572 - accuracy: 0.1000 - val_loss: 1.3472 - val_accuracy: 0.5000\n",
      "Have Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have fingerprints\n",
      "Data Parsed\n",
      "1/1 [==============================] - 83s 83s/step - loss: 1.2227 - accuracy: 0.7143 - val_loss: 1.0547 - val_accuracy: 1.0000\n",
      "Have Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have fingerprints\n",
      "Data Parsed\n",
      "1/1 [==============================] - 83s 83s/step - loss: 1.1590 - accuracy: 0.5714 - val_loss: 1.2084 - val_accuracy: 0.6667\n",
      "Have Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have fingerprints\n",
      "Data Parsed\n",
      "1/1 [==============================] - 69s 69s/step - loss: 1.2899 - accuracy: 0.3333 - val_loss: 0.8782 - val_accuracy: 1.0000\n",
      "Have Data\n",
      "Have fingerprints\n",
      "Data Parsed\n",
      "2/2 [==============================] - 106s 22s/step - loss: 1.1772 - accuracy: 0.4444 - val_loss: 1.3667 - val_accuracy: 0.0000e+00\n",
      "Have Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/lib/python3.9/site-packages/scipy/optimize/_minpack_py.py:833: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have fingerprints\n",
      "Data Parsed\n",
      "2/2 [==============================] - 107s 23s/step - loss: 1.3300 - accuracy: 0.2222 - val_loss: 1.2081 - val_accuracy: 1.0000\n",
      "Have Data\n",
      "Have fingerprints\n",
      "Data Parsed\n",
      "1/1 [==============================] - 82s 82s/step - loss: 1.3384 - accuracy: 0.2857 - val_loss: 1.2847 - val_accuracy: 0.3333\n",
      "Have Data\n",
      "Have fingerprints\n",
      "Data Parsed\n",
      "2/2 [==============================] - 106s 21s/step - loss: 1.3315 - accuracy: 0.5556 - val_loss: 1.0875 - val_accuracy: 0.6667\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "test_size=1.0 should be either positive and smaller than the number of samples 24 or a float in the (0, 1) range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m GroupsLeft \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      2\u001b[0m     SplitRatio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(GroupsLeft)\n\u001b[0;32m----> 4\u001b[0m     RemainingFiles, GroupFiles, x,y \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRemainingFiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mRemainingFiles\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSplitRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     GroupsLeft \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      8\u001b[0m     X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m MakingTrainingSet(GroupFiles)\n",
      "File \u001b[0;32m~/.conda/envs/tfcgpu/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2433\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2430\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[1;32m   2432\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 2433\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[1;32m   2435\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m   2438\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/tfcgpu/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2056\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2048\u001b[0m train_size_type \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(train_size)\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind\n\u001b[1;32m   2050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2051\u001b[0m     test_size_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2052\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (test_size \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m n_samples \u001b[38;5;129;01mor\u001b[39;00m test_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   2053\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m test_size_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2054\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (test_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m test_size \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2055\u001b[0m ):\n\u001b[0;32m-> 2056\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2057\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_size=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m should be either positive and smaller\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2058\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m or a float in the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(0, 1) range\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(test_size, n_samples)\n\u001b[1;32m   2060\u001b[0m     )\n\u001b[1;32m   2062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2063\u001b[0m     train_size_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2064\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (train_size \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m n_samples \u001b[38;5;129;01mor\u001b[39;00m train_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   2065\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m train_size_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2066\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (train_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m train_size \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2067\u001b[0m ):\n\u001b[1;32m   2068\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2069\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_size=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m should be either positive and smaller\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2070\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m or a float in the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2071\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(0, 1) range\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(train_size, n_samples)\n\u001b[1;32m   2072\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: test_size=1.0 should be either positive and smaller than the number of samples 24 or a float in the (0, 1) range"
     ]
    }
   ],
   "source": [
    "while GroupsLeft > 0:\n",
    "    SplitRatio = 1/(GroupsLeft)\n",
    "\n",
    "    RemainingFiles, GroupFiles, x,y = train_test_split(RemainingFiles, range(len(RemainingFiles)), test_size=SplitRatio, shuffle=True, random_state=0)\n",
    "\n",
    "    GroupsLeft -=1\n",
    "\n",
    "    X_train, X_test, y_train, y_test = MakingTrainingSet(GroupFiles)\n",
    "    #saver.restore('model.ckpt')\n",
    "    \n",
    "    history = model.fit(x = X_train, y = y_train, epochs=1, batch_size = 8 , shuffle=False, validation_split=0.25, callbacks=callbacks)\n",
    "    \n",
    "    #saver.save(sess,'model.ckpt')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 600, 125, 150, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save(SaveModelFolder)\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig(rootfolder + 'ModelAccuracy.png')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig(rootfolder + 'ModelLoss.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "tfcgpu",
   "language": "python",
   "name": "tfcgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
