{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/sciclone/home20/dchendrickson01/.conda/envs/tfcgpu/bin/python\n",
    "\n",
    "\n",
    "#Standard Header used on the projects\n",
    "# %%\n",
    "\n",
    "dataSize = 'big'  # 'small'\n",
    "\n",
    "#first the major packages used for math and graphing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "\n",
    "import os as os\n",
    "import random\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import pywt\n",
    "from pywt._extensions._pywt import (DiscreteContinuousWavelet, ContinuousWavelet,\n",
    "                                Wavelet, _check_dtype)\n",
    "from pywt._functions import integrate_wavelet, scale2frequency\n",
    "from time import time as ti\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential, Model \n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import  EarlyStopping\n",
    " \n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import tensorflow.keras_metrics as km\n",
    "  \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "import platform\n",
    "\n",
    "HostName = platform.node()\n",
    "\n",
    "if HostName == \"Server\":\n",
    "    Computer = \"Desktop\"   \n",
    "elif HostName[-6:] == 'wm.edu':\n",
    "    Computer = \"SciClone\"\n",
    "elif HostName == \"SchoolLaptop\":\n",
    "    Computer = \"LinLap\"\n",
    "elif HostName == \"WTC-TAB-512\":\n",
    "    Computer = \"PortLap\"\n",
    "else:\n",
    "    Computer = \"WinLap\"\n",
    "\n",
    "if Computer == \"SciClone\":\n",
    "    location = '/sciclone/home20/dchendrickson01/'\n",
    "elif Computer == \"WinLap\":\n",
    "    location = 'C:\\\\Data\\\\'\n",
    "elif Computer == \"Desktop\":\n",
    "    location = \"E:\\\\Backups\\\\Dan\\\\CraneData\\\\\"\n",
    "elif Computer == \"LinLap\":\n",
    "    location = '/home/dan/Output/'\n",
    "    \n",
    "\n",
    "if Computer ==  \"SciClone\":\n",
    "    rootfolder = '/sciclone/home20/dchendrickson01/'\n",
    "    if dataSize == 'big':\n",
    "        folder = '/sciclone/scr10/dchendrickson01/CraneData/'\n",
    "    else:\n",
    "        folder = '/sciclone/data10/dchendrickson01/SmallCopy/'\n",
    "elif Computer == \"Desktop\":\n",
    "    rootfolder = location\n",
    "    if dataSize == 'big':\n",
    "        folder = 'G:\\\\CraneData\\\\'\n",
    "    else:\n",
    "        folder = rootfolder + \"SmallCopy\\\\\"\n",
    "elif Computer ==\"WinLap\":\n",
    "    rootfolder = location\n",
    "    folder = rootfolder + \"SmallCopy\\\\\"   \n",
    "elif Computer == \"LinLap\":\n",
    "    rootfolder = '/home/dan/Data/'\n",
    "    folder = rootfolder + 'SmallCopy/'\n",
    "    \n",
    "\n",
    "scales = 500\n",
    "#img_height , img_width = scales, 200\n",
    "DoSomeFiles = False\n",
    "\n",
    "SmoothType = 3  # 0 = none, 1 = rolling average, 2 = rolling StdDev\n",
    "TrainEpochs = 3\n",
    "WaveletToUse = 'beta'\n",
    "\n",
    "num_cores = multiprocessing.cpu_count() -1\n",
    "NumberOfFiles = num_cores - 2\n",
    "GroupSize = NumberOfFiles\n",
    "\n",
    "SensorPositonFile = rootfolder + 'SensorStatsSmall.csv'\n",
    "\n",
    "if Computer == \"SciClone\" or Computer == \"LinLap\":\n",
    "    SaveModelFolder = rootfolder + 'SavedModel2/'\n",
    "else:\n",
    "    SaveModelFolder = rootfolder + 'SavedModel\\\\'\n",
    "\n",
    "files = os.listdir(folder)\n",
    "if DoSomeFiles: files = random.sample(files,NumberOfFiles)\n",
    "\n",
    "OutputVectors = np.genfromtxt(open(SensorPositonFile,'r'), delimiter=',',skip_header=1,dtype=int, missing_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CoreFunctions as cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resizeImage(FP):\n",
    "    res = cv2.resize(FP, dsize=(int(np.shape(FP)[0]/2), int(np.shape(FP)[1]/6)), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture capt\n",
    "\n",
    "def MakingTrainingSet(files):\n",
    "    AllAccels = Parallel(n_jobs=num_cores)(delayed(cf.getAcceleration)(file) for file in files)\n",
    "    Flattened = []\n",
    "    for j in range(np.shape(AllAccels)[0]):\n",
    "        if AllAccels[j][0] == False:\n",
    "            print(j,AllAccels[j][1])\n",
    "        else: \n",
    "            Flattened.append(AllAccels[j])\n",
    "    print('Have Data')\n",
    "\n",
    "    MetaData = []  #np.asarray([],dtype=object)\n",
    "    DataOnlyMatrix = np.asarray([],dtype=object)\n",
    "    for j in range(np.shape(AllAccels)[0]):\n",
    "        if AllAccels[j][0] == False :\n",
    "            if AllAccels[j][1][4:9] =='Accel':\n",
    "                print(j,AllAccels[j][1])\n",
    "        else: \n",
    "            for k in range(3):\n",
    "                MetaData.append([AllAccels[j][k][0], AllAccels[j][k][1], AllAccels[j][k][3], AllAccels[j][k][4]])\n",
    "                if np.size(DataOnlyMatrix) == 0:\n",
    "                        DataOnlyMatrix =np.matrix(AllAccels[j][k][2])\n",
    "                else:\n",
    "                        DataOnlyMatrix = np.concatenate((DataOnlyMatrix,np.matrix(AllAccels[j][k][2])),axis=0)\n",
    "\n",
    "    MetaData = np.matrix(MetaData)\n",
    "\n",
    "    AllAccels = cf.KalmanGroup(DataOnlyMatrix)\n",
    "\n",
    "    del DataOnlyMatrix\n",
    "\n",
    "    maxes = np.amax(AllAccels[:,500:], axis = 1)\n",
    "    mins = np.amin(AllAccels[:,500:], axis = 1)\n",
    "\n",
    "    Keep = np.zeros(mins.size)\n",
    "    for i in range(mins.size):\n",
    "        if i % 3 == 0:\n",
    "            if maxes[i] > 0.01 and mins[i] < -0.01:\n",
    "                Keep[i]=1\n",
    "                Keep[i+1]=1\n",
    "                Keep[i+2]=1\n",
    "                #print(i)\n",
    "\n",
    "\n",
    "    Keep = np.array(Keep, dtype='bool')\n",
    "\n",
    "    AllAccels = AllAccels[Keep,:]\n",
    "    MetaData = MetaData[Keep,:]\n",
    "\n",
    "    MotionsLeft = int(np.shape(AllAccels)[0]/3.0)\n",
    "\n",
    "    AllFingers =  Parallel(n_jobs=num_cores)(delayed(cf.makeMatrixImages)([AllAccels[i*3],AllAccels[i*3+1],AllAccels[i*3+2]]) for i in range(MotionsLeft))\n",
    "    del AllAccels\n",
    "\n",
    "    print('Have fingerprints')\n",
    "\n",
    "    ResultsSet = []\n",
    "    for i in range(MotionsLeft):\n",
    "        ResultsSet.append(np.asarray(cf.truthVector(MetaData[i*3,3])).flatten())\n",
    "    \n",
    "    del MetaData\n",
    "\n",
    "    print('Data Parsed')\n",
    "\n",
    "    SmallFingers =  Parallel(n_jobs=num_cores)(delayed(resizeImage)(FP) for FP in AllFingers)\n",
    "    del AllFingers\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(SmallFingers, ResultsSet, test_size=0.20, shuffle=True, random_state=0)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GroupSize = NumberOfFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have Data\n",
      "Have fingerprints\n",
      "Data Parsed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "fCount = len(files)\n",
    "GroupsLeft = int(fCount/GroupSize) + 1\n",
    "\n",
    "SplitRatio = 1/(GroupsLeft)\n",
    "\n",
    "RemainingFiles, GroupFiles, x,y = train_test_split(files, range(len(files)), test_size=SplitRatio, shuffle=True, random_state=0)\n",
    "\n",
    "GroupsLeft -=1\n",
    "\n",
    "X_train, X_test, y_train, y_test = MakingTrainingSet(GroupFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width = np.shape(X_train)[1]\n",
    "img_height = np.shape(X_train)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling (Rescaling)       (None, 10000, 250, 3)     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 10000, 250, 32)    896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 5000, 125, 32)    0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 5000, 125, 32)     9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 2500, 62, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 2500, 62, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 1250, 31, 64)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2480000)           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               317440128 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 317,469,284\n",
      "Trainable params: 317,469,284\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Rescaling(1./1, input_shape=(img_width, img_height, 3)),\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(np.shape(y_train)[1])\n",
    "    ])\n",
    " \n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])\n",
    " \n",
    "earlystop = EarlyStopping(patience=7)   \n",
    "callbacks = [earlystop]\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.matrix(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1/1 [==============================] - 3s 3s/step - loss: 2.6286 - accuracy: 0.0000e+00 - val_loss: 2.3278 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.9168 - accuracy: 0.5000 - val_loss: 2.3357 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.9132 - accuracy: 0.5000 - val_loss: 2.3437 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x = X_train, y = y_train, epochs=3, batch_size = 8 , shuffle=False, validation_split=0.25, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: E:\\Backups\\Dan\\CraneData\\SavedModel\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(SaveModelFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have Data\n",
      "Have fingerprints\n",
      "Data Parsed\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.8321 - accuracy: 0.0000e+00 - val_loss: 16.1181 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: E:\\Backups\\Dan\\CraneData\\SavedModel\\assets\n",
      "2\n",
      "Have Data\n",
      "Have fingerprints\n",
      "Data Parsed\n",
      "1/1 [==============================] - 3s 3s/step - loss: 2.3060 - accuracy: 0.0000e+00 - val_loss: 2.2271 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: E:\\Backups\\Dan\\CraneData\\SavedModel\\assets\n",
      "3\n",
      "Have Data\n",
      "Have fingerprints\n",
      "Data Parsed\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.6385 - accuracy: 0.0000e+00 - val_loss: 2.2342 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: E:\\Backups\\Dan\\CraneData\\SavedModel\\assets\n",
      "4\n",
      "Have Data\n",
      "Have fingerprints\n",
      "Data Parsed\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.6148 - accuracy: 0.0000e+00 - val_loss: 2.2239 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: E:\\Backups\\Dan\\CraneData\\SavedModel\\assets\n",
      "5\n",
      "Have Data\n",
      "Have fingerprints\n",
      "Data Parsed\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Training data contains 1 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.25`. Either provide more data, or a different value for the `validation_split` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28772/3992507952.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSaveModelFolder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mi\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Dan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Dan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mtrain_validation_split\u001b[1;34m(arrays, validation_split)\u001b[0m\n\u001b[0;32m   1498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1499\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0msplit_at\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0msplit_at\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1500\u001b[1;33m     raise ValueError(\n\u001b[0m\u001b[0;32m   1501\u001b[0m         \u001b[1;34m\"Training data contains {batch_dim} samples, which is not sufficient \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1502\u001b[0m         \u001b[1;34m\"to split it into a validation and training set as specified by \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Training data contains 1 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.25`. Either provide more data, or a different value for the `validation_split` argument."
     ]
    }
   ],
   "source": [
    "\n",
    "i = 1\n",
    "while GroupsLeft > 0:\n",
    "    SplitRatio = 1/(GroupsLeft)\n",
    "\n",
    "    RemainingFiles, GroupFiles, x,y = train_test_split(RemainingFiles, range(len(RemainingFiles)), test_size=SplitRatio, shuffle=True, random_state=0)\n",
    "\n",
    "    GroupsLeft -=1\n",
    "\n",
    "    X_train, X_test, y_train, y_test = MakingTrainingSet(GroupFiles)\n",
    "    #saver.restore('model.ckpt')\n",
    "    if np.shape(X_train)[0] >= 4:\n",
    "        y_train = np.matrix(y_train)\n",
    "        X_train = np.asarray(X_train)\n",
    "        \n",
    "        history = model.fit(x = X_train, y = y_train, epochs=1, batch_size = 8 , shuffle=False, validation_split=0.25, callbacks=callbacks)\n",
    "        if i%5 == 0:\n",
    "            model.save(SaveModelFolder)\n",
    "        i+=1\n",
    "        print(i)\n",
    "        #saver.save(sess,'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig(rootfolder + 'ModelAccuracy.png')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig(rootfolder + 'ModelLoss.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "11c16a051206f53cf7fe024f12cacb318023d916d0a5509b7bf3391ee4b4163a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
