{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import keras\n",
    "#from keras import layers\n",
    "#from matplotlib import pyplot as plt\n",
    "#import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from cycler import cycler\n",
    "import scipy.special as sp\n",
    "import os as os\n",
    "#import pywt as py\n",
    "#import statistics as st\n",
    "import os as os\n",
    "#import random\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "#import platform\n",
    "from time import time as ti\n",
    "from skimage.restoration import denoise_wavelet\n",
    "#import tensorflow as tf\n",
    "import pickle\n",
    "#import CoreFunctions as cf\n",
    "\n",
    "import sys\n",
    "\n",
    "DataFolder = '/scratch/Recordings2/'\n",
    "SaveFolder = '/scratch/750inputs/'\n",
    "DataFolder = '/sciclone/scr10/dchendrickson01/Recordings2/'\n",
    "SaveFolder = '/sciclone/scr10/dchendrickson01/AllMoveMatsPickles/'\n",
    "\n",
    "StartPoint = 0\n",
    "Groups = 180\n",
    "#StartPoint = int(sys.argv[1])\n",
    "#Groups = 186\n",
    "\n",
    "verbose = True\n",
    "small = False\n",
    "noise = verbose\n",
    "\n",
    "TIME_STEPS = 750\n",
    "Skips = 50\n",
    "\n",
    "tic = ti()\n",
    "start = tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def RollingStdDev(RawData, SmoothData, RollSize = 25):\n",
    "    StdDevs = []\n",
    "    for i in range(RollSize):\n",
    "        Diffs = RawData[0:i+1]-SmoothData[0:i+1]\n",
    "        Sqs = Diffs * Diffs\n",
    "        Var = sum(Sqs) / (i+1)\n",
    "        StdDev = np.sqrt(Var)\n",
    "        StdDevs.append(StdDev)\n",
    "    for i in range(len(RawData)-RollSize-1):\n",
    "        j = i + RollSize\n",
    "        Diffs = RawData[i:j]-SmoothData[i:j]\n",
    "        Sqs = Diffs * Diffs\n",
    "        Var = sum(Sqs) / RollSize\n",
    "        StdDev = np.sqrt(Var)\n",
    "        StdDevs.append(StdDev)  \n",
    "    \n",
    "    return StdDevs\n",
    "\n",
    "def RollingStdDevFaster(RawData, SmoothData, RollSize = 25):\n",
    "\n",
    "    Diffs = RawData - SmoothData\n",
    "    \n",
    "    del RawData, SmoothData\n",
    "    \n",
    "    Sqs = Diffs * Diffs\n",
    "\n",
    "    del Diffs\n",
    "    \n",
    "    Sqs = Sqs.tolist() \n",
    "    \n",
    "    Sqs.extend(np.zeros(RollSize))\n",
    "    \n",
    "    mSqs = np.matrix(Sqs)\n",
    "    \n",
    "    for i in range(RollSize):\n",
    "        Sqs.insert(0, Sqs.pop())\n",
    "        mSqs = np.concatenate((np.matrix(Sqs),mSqs))\n",
    "    \n",
    "    sVect = mSqs.sum(axis=0)\n",
    "    eVect = (mSqs!=0).sum(axis=0)\n",
    "    \n",
    "    del mSqs, Sqs\n",
    "    \n",
    "    VarVect = sVect / eVect\n",
    "    \n",
    "    StdDevs = np.sqrt(VarVect)\n",
    "    \n",
    "    return StdDevs[:-RollSize]\n",
    "\n",
    "\n",
    "def RollingSum(Data, Length = 100):\n",
    "    RollSumStdDev = []\n",
    "    for i in range(Length):\n",
    "        RollSumStdDev.append(sum(Data[0:i+1]))\n",
    "    for i in range(len(Data) - Length):\n",
    "        RollSumStdDev.append(sum(Data[i:i+Length]))\n",
    "    return RollSumStdDev\n",
    "\n",
    "def SquelchPattern(DataSet, StallRange = 5000, SquelchLevel = 0.02, verbose = False):\n",
    "    \n",
    "    SquelchSignal = np.ones(len(DataSet))\n",
    "    if verbose:\n",
    "        print(len(SquelchSignal))\n",
    "        \n",
    "    for i in range(len(DataSet)-2*StallRange):\n",
    "        if np.average(DataSet[i:i+StallRange]) < SquelchLevel:\n",
    "            SquelchSignal[i+StallRange]=0\n",
    "\n",
    "    return SquelchSignal\n",
    "\n",
    "def SquelchPatternFast(DataSet, StallRange = 5000, SquelchLevel = 0.02):\n",
    "    SquelchSignal = np.ones(len(DataSet))\n",
    "\n",
    "    #DataSet = DataSet.tolist() \n",
    "    \n",
    "    DataSet.extend(np.zeros(StallRange))\n",
    "    \n",
    "    DSM = np.matrix(DataSet)\n",
    "    \n",
    "    for i in range(StallRange):\n",
    "        DataSet.insert(0, DataSet.pop())\n",
    "        DSM = np.concatenate((np.matrix(DataSet),DSM))\n",
    "    \n",
    "    DsmAvs = np.average(DSM,axis=0)\n",
    "    \n",
    "    DsmAvs[DsmAvs < SquelchLevel] = 0\n",
    "    DsmAvs[DsmAvs >= SquelchLevel] = 1\n",
    "\n",
    "    return SquelchSignal\n",
    "\n",
    "def getVelocity(Acceleration, Timestamps = 0.003, Squelch = [], corrected = 0):\n",
    "    velocity = np.zeros(len(Acceleration))\n",
    "    \n",
    "    Acceleration -= np.average(Acceleration)\n",
    "    \n",
    "    if len(Timestamps) == 1:\n",
    "        dTime = np.ones(len(Acceleration),dtype=float) * Timestamps\n",
    "    elif len(Timestamps) == len(Acceleration):\n",
    "        dTime = np.zeros(len(Timestamps), dtype=float)\n",
    "        dTime[0]=1\n",
    "        for i in range(len(Timestamps)-1):\n",
    "            j = i+1\n",
    "            if float(Timestamps[j]) > float(Timestamps[i]):\n",
    "                dTime[j]=float(Timestamps[j])-float(Timestamps[i])\n",
    "            else:\n",
    "                dTime[j]=float(Timestamps[j])-float(Timestamps[i])+10000.0\n",
    "        dTime /= 10000.0\n",
    "\n",
    "    velocity[0] = Acceleration[0] * (dTime[0])\n",
    "\n",
    "    for i in range(len(Acceleration)-1):\n",
    "        j = i + 1\n",
    "        if corrected ==2:\n",
    "            if Squelch[j]==0:\n",
    "                velocity[j]=0\n",
    "            else:\n",
    "                velocity[j] = velocity[i] + Acceleration[j] * dTime[j]                \n",
    "        else:\n",
    "            velocity[j] = velocity[i] + Acceleration[j] * dTime[j]\n",
    "\n",
    "    if corrected == 1:\n",
    "        PointVairance = velocity[-1:] / len(velocity)\n",
    "        for i in range(len(velocity)):\n",
    "            velocity[i] -=  PointVairance * i\n",
    "    \n",
    "    velocity *= 9.81\n",
    "\n",
    "    return velocity\n",
    "\n",
    "def MakeDTs(Seconds, Miliseconds):\n",
    "    dts = np.zeros(len(Miliseconds), dtype=float)\n",
    "    dts[0]=1\n",
    "    for i in range(len(MiliSeconds)-1):\n",
    "        j = i+1\n",
    "        if Seconds[j]==Seconds[i]:\n",
    "            dts[j]=Miliseconds[j]-Miliseconds[i]\n",
    "        else:\n",
    "            dts[j]=Miliseconds[j]-Miliseconds[i]+1000\n",
    "    dts /= 10000\n",
    "    return dts\n",
    "\n",
    "\n",
    "def split_list_by_ones(original_list, ones_list):\n",
    "    # Created with Bing AI support\n",
    "    #  1st request: \"python split list into chunks based on value\"\n",
    "    #  2nd request: \"I want to split the list based on the values in a second list.  Second list is all 1s and 0s.  I want all 0s removed, and each set of consequtive ones as its own item\"\n",
    "    #  3rd request: \"That is close.  Here is an example of the two lists, and what I would want returned: original_list = [1, 2, 3, 8, 7, 4, 5, 6, 4, 7, 8, 9]\n",
    "    #                ones_list =     [1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1]\n",
    "    #                return: [[1, 2, 3, 8], [4, 5, 6], [8,9]]\"\n",
    "    #\n",
    "    #This is the function that was created and seems to work on the short lists, goin to use fo rlong lists\n",
    "    \n",
    "    result_sublists = []\n",
    "    sublist = []\n",
    "\n",
    "    for val, is_one in zip(original_list, ones_list):\n",
    "        if is_one:\n",
    "            sublist.append(val)\n",
    "        elif sublist:\n",
    "            result_sublists.append(sublist)\n",
    "            sublist = []\n",
    "\n",
    "    # Add the last sublist (if any)\n",
    "    if sublist:\n",
    "        result_sublists.append(sublist)\n",
    "\n",
    "    return result_sublists\n",
    "\n",
    "# Generated training sequences for use in the model.\n",
    "def create_sequences(values, time_steps=TIME_STEPS, skips = Skips):\n",
    "    output = []\n",
    "    for i in range(int((len(values) - time_steps + skips)/skips)):\n",
    "        output.append(values[i*skips : (i*skips + time_steps)])\n",
    "    return np.stack(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def runFile(file, verbose = False, small = False, index=0, start=ti()):\n",
    "    noise = verbose\n",
    "    if file[-4:] == '.csv':    \n",
    "        dataset = pd.read_csv(DataFolder+file, delimiter =\",\", header=None, engine='python',on_bad_lines='skip')\n",
    "        if noise:\n",
    "            print(\"File Read\", ti()-start)\n",
    "        dataset = dataset.rename(columns={0:\"Day\"})\n",
    "        dataset = dataset.rename(columns={1:\"Second\"})\n",
    "        dataset = dataset.rename(columns={2:\"FracSec\"})\n",
    "        dataset = dataset.rename(columns={3:\"p\"})\n",
    "        dataset = dataset.rename(columns={4:\"h\"})\n",
    "        dataset = dataset.rename(columns={5:\"v\"})\n",
    "        dataset = dataset.rename(columns={6:\"Sensor\"})\n",
    "\n",
    "        dataset['Second'].replace('',0)\n",
    "        dataset['FracSec'].replace('',0)\n",
    "        dataset.replace([np.nan, np.inf, -np.inf],0,inplace=True)\n",
    "        \n",
    "        dataset[['Day','Second']] = dataset[['Day','Second']].apply(lambda x: x.astype(int).astype(str).str.zfill(6))\n",
    "        dataset[['FracSec']] = dataset[['FracSec']].apply(lambda x: x.astype(int).astype(str).str.zfill(4))\n",
    "\n",
    "        dataset[\"timestamp\"] = pd.to_datetime(dataset.Day+dataset.Second+dataset.FracSec,format='%y%m%d%H%M%S%f')\n",
    "        dataset[\"timestamps\"] = dataset[\"timestamp\"]\n",
    "\n",
    "        dataset[\"p\"] = dataset.p - np.average(dataset.p)\n",
    "        dataset[\"h\"] = dataset.h - np.average(dataset.h)\n",
    "        dataset[\"v\"] = dataset.v - np.average(dataset.v)\n",
    "        dataset[\"r\"] = np.sqrt(dataset.p**2 + dataset.h**2 + dataset.v**2)\n",
    "\n",
    "        dataset.index = dataset.timestamp\n",
    "\n",
    "        dataset[\"SmoothP\"] = denoise_wavelet(dataset.p, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        dataset[\"SmoothH\"] = denoise_wavelet(dataset.h, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        dataset[\"SmoothV\"] = denoise_wavelet(dataset.v, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        dataset[\"SmoothR\"] = denoise_wavelet(dataset.r, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "\n",
    "        if noise:\n",
    "            print(\"Data Cleaned\", ti()-start, len(dataset.p))\n",
    "\n",
    "        RawData = dataset.v\n",
    "        SmoothData = dataset.SmoothV\n",
    "        RollSize = 25\n",
    "\n",
    "        Diffs = RawData - SmoothData\n",
    "\n",
    "        Sqs = Diffs * Diffs\n",
    "\n",
    "        Sqs = Sqs.tolist() \n",
    "\n",
    "        Sqs.extend(np.zeros(RollSize))\n",
    "\n",
    "        mSqs = np.matrix(Sqs)\n",
    "\n",
    "        for i in range(RollSize):\n",
    "            Sqs.insert(0, Sqs.pop())\n",
    "            mSqs = np.concatenate((np.matrix(Sqs),mSqs))\n",
    "\n",
    "        sVect = mSqs.sum(axis=0)\n",
    "        eVect = (mSqs!=0).sum(axis=0)\n",
    "\n",
    "        VarVect = sVect / eVect\n",
    "\n",
    "        StdDevs = np.sqrt(VarVect)\n",
    "\n",
    "        StdDevsZ = np.asarray(StdDevs)\n",
    "\n",
    "        StdDevsZ=np.append(StdDevsZ,[0])\n",
    "\n",
    "        StdDevsZ = np.asarray(StdDevsZ.T[:len(dataset.p)])\n",
    "\n",
    "        if noise:\n",
    "            print(\"Size StdDevsZ\", ti()-start, np.shape(StdDevsZ))\n",
    "\n",
    "        #StdDevsZ = np.nan_to_num(StdDevsZ)\n",
    "\n",
    "        #StdDevsZ[StdDevsZ == np.inf] = 0\n",
    "        #StdDevsZ[StdDevsZ == -np.inf] = 0\n",
    "\n",
    "        if noise:\n",
    "            print(\"cleaned\", ti()-start, np.shape(StdDevsZ))\n",
    "\n",
    "        SmoothDevZ = denoise_wavelet(StdDevsZ, method='VisuShrink', mode='soft', wavelet='sym2', rescale_sigma='True')\n",
    "\n",
    "        if noise:\n",
    "            print(\"denoise 1\", ti()-start, np.shape(StdDevsZ))\n",
    "\n",
    "        #SmoothDevZa = cf.Smoothing(StdDevsZ, 3, wvt='sym2', dets_to_remove=2, levels=3)\n",
    "        #SmoothDevZ = np.ravel(SmoothDevZ[0,:])\n",
    "\n",
    "        #SmoothDevZ = SmoothDevZ.tolist()\n",
    "\n",
    "        if noise:\n",
    "            print(\"denoise 2\", ti()-start, np.shape(SmoothDevZ))\n",
    "\n",
    "        #ataset[\"SmoothDevZ\"] = SmoothDevZ\n",
    "\n",
    "        SmoothDevZ[np.isnan(SmoothDevZ)]=0\n",
    "        \n",
    "        Max = np.max(SmoothDevZ)\n",
    "\n",
    "        \n",
    "        \n",
    "        if noise:\n",
    "            print(\"Max\", ti()-start, np.shape(Max), Max)\n",
    "\n",
    "        buckets = int(Max / 0.005) + 1\n",
    "        bins = np.linspace(0,buckets*0.005,buckets+1)\n",
    "        counts, bins = np.histogram(SmoothDevZ,bins=bins)\n",
    "\n",
    "        CummCount = 0\n",
    "        HalfWay = 0\n",
    "        for i in range(len(counts)):\n",
    "            CummCount += counts[i]\n",
    "            if CummCount / len(SmoothDevZ) >= 0.5:\n",
    "                if HalfWay == 0:\n",
    "                    HalfWay = i\n",
    "\n",
    "        SquelchLevel = bins[HalfWay] \n",
    "        if noise:\n",
    "            print(\"SmoothDevz size\", np.shape(SmoothDevZ))\n",
    "\n",
    "        dataset[\"IsMoving\"] = SquelchPattern(SmoothDevZ, 4000, SquelchLevel, verbose=noise)\n",
    "\n",
    "        if noise:\n",
    "            print(\"Squelch Made\", ti()-start)\n",
    "        #dataset[\"velocity\"] = getVelocity(dataset.p, dataset.FracSec, dataset.IsMoving, 2)\n",
    "        #if noise:\n",
    "        #    print(\"Velocity Calculated.  File done: \",file)\n",
    "\n",
    "        #df_pr = split_list_by_ones(dataset.p, dataset.IsMoving)\n",
    "        #df_hr = split_list_by_ones(dataset.h, dataset.IsMoving)\n",
    "        #df_vr = split_list_by_ones(dataset.v, dataset.IsMoving)\n",
    "        #df_rrr = split_list_by_ones(dataset.r, dataset.IsMoving)\n",
    "        df_ps = split_list_by_ones(dataset.SmoothP, dataset.IsMoving)\n",
    "        df_hs = split_list_by_ones(dataset.SmoothH, dataset.IsMoving)\n",
    "        df_vs = split_list_by_ones(dataset.SmoothV, dataset.IsMoving)\n",
    "        df_rs = split_list_by_ones(dataset.SmoothR, dataset.IsMoving)\n",
    "\n",
    "        MatsSmooth = []\n",
    "        for i in range(len(df_ps)):\n",
    "            MatsSmooth.append(np.vstack((df_ps[i],df_hs[i],df_vs[i],df_rs[i])))\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Split by ones\", ti()-start)\n",
    "\n",
    "\n",
    "        '''df_p=[0]\n",
    "        df_h=[0]\n",
    "        df_v=[0]\n",
    "        df_r=[0]\n",
    "        df_rp=[0]\n",
    "        df_rh=[0]\n",
    "        df_rv=[0]\n",
    "        df_rr=[0]\n",
    "        for i in range(len(df_ps)):\n",
    "            df_p += df_ps[i]\n",
    "            df_h += df_hs[i]\n",
    "            df_v += df_vs[i]\n",
    "            df_r += df_rs[i]\n",
    "            df_rp += df_pr[i]\n",
    "            df_rh += df_hr[i]\n",
    "            df_rv += df_vr[i]\n",
    "            df_rr += df_rrr[i]\n",
    "        '''\n",
    "        if verbose:\n",
    "            print('format changed', ti()-start, np.shape(df_p))\n",
    "\n",
    "        return MatsSmooth\n",
    "    else:\n",
    "        return ['fail','fail']\n",
    "        \n",
    "        #if verbose:\n",
    "        #    print('Data normalized', ti()-start)\n",
    "\n",
    "        #return df_p, df_h, df_v, df_r, df_rp, df_rh, df_rv, df_rr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "files= os.listdir(DataFolder) \n",
    "#Results = Parallel(n_jobs=8)(delayed(runFile)(file, False, False, index) for index, file in enumerate(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1074"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1074/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getData(offset, loops = 20, starts=ti()):\n",
    "    Mats=[]\n",
    "    counter = 0\n",
    "    for i in range(loops):\n",
    "        Mats = runFile(files[i+offset*loops], verbose = False, small = False, index=0, start=starts)\n",
    "        if len(Mats)> 20:\n",
    "            for run in Mats:\n",
    "                Mats.append([counter, run])\n",
    "                counter+=1\n",
    "        #print(i, int((ti()-starts)/60*100)/100, len(Mats))\n",
    "        if i % 19 == 0:\n",
    "            pickle.dump(Mats,open(SaveFolder+'data'+str(offset).zfill(2)+'-'+str(counter).zfill(5)+'.p','wb'))\n",
    "            Mats = []\n",
    "\n",
    "    if len(Mats)>1:\n",
    "        pickle.dump(Mats,open(SaveFolder+'data'+str(offset).zfill(2)+'-'+str(counter).zfill(5)+'.p','wb'))\n",
    "            \n",
    "    \n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TerminatedWorkerError",
     "evalue": "A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nThe exit codes of the workers are {SIGKILL(-9)}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m AllDatas \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgetData\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mti\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1692\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1693\u001b[0m \n\u001b[1;32m   1694\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1697\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1699\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1700\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1702\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1734\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1730\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1734\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:736\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    730\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:754\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 754\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nThe exit codes of the workers are {SIGKILL(-9)}"
     ]
    }
   ],
   "source": [
    "AllDatas = Parallel(n_jobs=6)(delayed(getData)(i,3,ti()) for i in range(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdfasdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mats=[]\n",
    "for Data in AllDatas:\n",
    "    for Mat in Data:\n",
    "        for run in Mat:\n",
    "            Mats.append(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del AllDatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Mats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Mats[156])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths=[]\n",
    "rejects=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Originals = []\n",
    "Smoothed = []\n",
    "\n",
    "small = 12000\n",
    "large = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for move in Mats:\n",
    "    leng = np.shape(move)[1]\n",
    "    if (leng < large and leng > small):\n",
    "        lengths.append(leng)\n",
    "    else:\n",
    "        rejects.append(leng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(lengths, bins=20)\n",
    "plt.xlabel(\"lengths\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lengths) / (len(lengths)+len(rejects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "groups = [2250-3000,\n",
    "          3001-5000, \n",
    "          5001-7500, \n",
    "          7501-10000,\n",
    "          10001-15000,\n",
    "          15001-20000,\n",
    "          20001-30000,\n",
    "          30001-50000,\n",
    "          50001-75000,\n",
    "          75001-100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, move in enumerate(Mats):\n",
    "    moveSize = np.shape(move)[1]\n",
    "    if (moveSize < large and moveSize > small):\n",
    "        extra = large - moveSize\n",
    "        sPad = 0 #int(extra/2)\n",
    "        ePad = extra # int(extra/2)+(extra%2)\n",
    "        Smoothed.append(np.vstack((np.pad(move[0,:], (sPad, ePad), mode='constant', constant_values=np.average(move[0,:])),\n",
    "                                   np.pad(move[1,:], (sPad, ePad), mode='constant', constant_values=np.average(move[1,:])),\n",
    "                                   np.pad(move[2,:], (sPad, ePad), mode='constant', constant_values=np.average(move[2,:])),\n",
    "                                   np.pad(move[3,:], (sPad, ePad), mode='constant', constant_values=np.average(move[3,:])))\n",
    "                                  ).T\n",
    "                        )\n",
    "        '''\n",
    "        Originals.append(np.vstack((np.pad(l_rp[i][j], (sPad, ePad), mode='constant', constant_values=np.average(l_rp[i][j])),\n",
    "                                   np.pad(l_rh[i][j], (sPad, ePad), mode='constant', constant_values=np.average(l_rh[i][j])),\n",
    "                                   np.pad(l_rv[i][j], (sPad, ePad), mode='constant', constant_values=np.average(l_rv[i][j])),\n",
    "                                   np.pad(l_rr[i][j], (sPad, ePad), mode='constant', constant_values=np.average(l_rr[i][j])))\n",
    "                                  ).T\n",
    "                        )\n",
    "        '''\n",
    "\n",
    "    else:\n",
    "        rejects.append(moveSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 112\n",
    "fig = plt.figure()\n",
    "plt.plot(Smoothed[i][:,0])\n",
    "plt.plot(Smoothed[i][:,1])\n",
    "plt.plot(Smoothed[i][:,2])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 112\n",
    "fig = plt.figure()\n",
    "plt.plot(Smoothed[i][:,3])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential(\n",
    "    [\n",
    "        # Input layer\n",
    "        layers.Input(shape=(large,3)),\n",
    "        \n",
    "        # Increased number of filters and added layers\n",
    "        layers.Conv1D(filters=512, kernel_size=17, padding=\"same\", strides=2, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        layers.Conv1D(filters=256, kernel_size=11, padding=\"same\", strides=2, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        layers.Conv1D(filters=128, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        layers.Dropout(rate=0.3),\n",
    "        \n",
    "        # Transpose layers with increased filters\n",
    "        layers.Conv1DTranspose(filters=128, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"),\n",
    "        layers.Conv1DTranspose(filters=256, kernel_size=11, padding=\"same\", strides=2, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        layers.Conv1DTranspose(filters=512, kernel_size=17, padding=\"same\", strides=2, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        layers.Dropout(rate=0.3),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Conv1DTranspose(filters=1, kernel_size=5, padding=\"same\"),\n",
    "        \n",
    "        # Added dense layers with more neurons\n",
    "        layers.Dense(256, activation=\"relu\"),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(1)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModelCheckpoint(Callback):\n",
    "    def __init__(self, filepath, save_freq):\n",
    "        super(CustomModelCheckpoint, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.save_freq = save_freq\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.save_freq == 0:\n",
    "            self.model.save(self.filepath.format(epoch=epoch + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = CustomModelCheckpoint(\n",
    "    filepath='/sciclone/scr10/dchendrickson01/models/BabySteps_20-30sMovesbigger-0911b-{epoch:02d}.keras',\n",
    "    save_freq=1  \n",
    ")\n",
    "\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir='/sciclone/scr10/dchendrickson01/models/profiles/0909-16000/',\n",
    "                                            profile_batch='01, 256')\n",
    "\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor=\"loss\", patience=5, mode=\"min\")\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=\"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = tf.convert_to_tensor(Smoothed, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, outputs = tf.split(ss, [3, 1], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x=inputs,\n",
    "    y=outputs,\n",
    "    epochs=10,\n",
    "    batch_size=2,\n",
    "    #validation_split=0.1,\n",
    "    callbacks=[checkpoint_callback, es_callback],# tb_callback],        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x=inputs,\n",
    "    y=outputs,\n",
    "    epochs=10,\n",
    "    batch_size=2,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[checkpoint_callback, reduce_lr],# tb_callback],        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(large,3)),\n",
    "        layers.Conv1D(\n",
    "            filters=32,\n",
    "            kernel_size=7,\n",
    "            padding=\"same\",\n",
    "            strides=1,\n",
    "            activation=\"relu\",\n",
    "        ),\n",
    "        layers.Dropout(rate=0.2),\n",
    "        layers.Conv1D(\n",
    "            filters=16,\n",
    "            kernel_size=7,\n",
    "            padding=\"same\",\n",
    "            strides=1,\n",
    "            activation=\"relu\",\n",
    "        ),\n",
    "        layers.Conv1DTranspose(\n",
    "            filters=16,\n",
    "            kernel_size=7,\n",
    "            padding=\"same\",\n",
    "            strides=1,\n",
    "            activation=\"relu\",\n",
    "        ),\n",
    "        layers.Dropout(rate=0.2),\n",
    "        layers.Conv1DTranspose(\n",
    "            filters=32,\n",
    "            kernel_size=7,\n",
    "            padding=\"same\",\n",
    "            strides=1,\n",
    "            activation=\"relu\",\n",
    "        ),\n",
    "        layers.Conv1DTranspose(filters=1, kernel_size=7, padding=\"same\"),\n",
    "    ]\n",
    ")\n",
    "model2.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=\"mse\")\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback2 = CustomModelCheckpoint(\n",
    "    filepath='/sciclone/scr10/dchendrickson01/models/BabySteps_20-30sMoves-0908b-{epoch:02d}.keras',\n",
    "    save_freq=1  \n",
    ")\n",
    "\n",
    "tb_callback2 = tf.keras.callbacks.TensorBoard(log_dir='/sciclone/scr10/dchendrickson01/models/profiles/0908-16000/',\n",
    "                                            profile_batch='01, 256')\n",
    "\n",
    "es_callback2 = keras.callbacks.EarlyStopping(monitor=\"loss\", patience=5, mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model2.fit(\n",
    "    x=inputs,\n",
    "    y=outputs,\n",
    "    epochs=10,\n",
    "    batch_size=2,\n",
    "    #validation_split=0.1,\n",
    "    callbacks=[checkpoint_callback, es_callback, tb_callback],        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pred = model.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mae_loss = np.mean(np.abs(x_train_pred - outputs), axis=1)\n",
    "\n",
    "plt.hist(train_mae_loss, bins=50)\n",
    "plt.xlabel(\"Train MAE loss\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()\n",
    "\n",
    "# Get reconstruction loss threshold.\n",
    "threshold = np.max(train_mae_loss)\n",
    "print(\"Reconstruction error threshold: \", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pred2 = model2.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(x_train_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mae_loss2 = np.mean(np.abs(x_train_pred2 - outputs), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_mae_loss2, bins=50)\n",
    "plt.xlabel(\"Train MAE loss\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()\n",
    "\n",
    "# Get reconstruction loss threshold.\n",
    "threshold2 = np.max(train_mae_loss2)\n",
    "print(\"Reconstruction error threshold: \", threshold2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(x_train_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking how the first sequence is learnt\n",
    "fig = plt.figure(figsize=(10,3), dpi=1200)\n",
    "for i in range(3):\n",
    "    plt.plot(outputs[i*17])\n",
    "    plt.plot(x_train_pred[i*17],linewidth = 0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking how the first sequence is learnt\n",
    "for i in range(3):\n",
    "    plt.plot(outputs[i*17])\n",
    "    plt.plot(x_train_pred2[i*17])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tin = tf.convert_to_tensor([np.zeros((16000,3)),np.ones((16000,3))],dtype=tf.float32)\n",
    "test = model.predict(tin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(2):\n",
    "    fig = plt.figure(figsize=(10,3), dpi=1200)\n",
    "    #plt.plot(outputs[i*17])\n",
    "    plt.plot(test[i],linewidth = 0.5)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "timeseries_anomaly_detection",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
