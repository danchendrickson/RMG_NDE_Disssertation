{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3-2021.11/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import keras\n",
    "#from keras import layers\n",
    "#from matplotlib import pyplot as plt\n",
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#from cycler import cycler\n",
    "import scipy.special as sp\n",
    "import os as os\n",
    "#import pywt as py\n",
    "#import statistics as st\n",
    "import os as os\n",
    "#import random\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "#import platform\n",
    "from time import time as ti\n",
    "from skimage.restoration import denoise_wavelet\n",
    "#import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/sciclone/scr10/dchendrickson01/Recordings2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEPS = 500\n",
    "Skips = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Load the data\n",
    "\n",
    "We will use the [Numenta Anomaly Benchmark(NAB)](\n",
    "https://www.kaggle.com/boltzmannbrain/nab) dataset. It provides artificial\n",
    "timeseries data containing labeled anomalous periods of behavior. Data are\n",
    "ordered, timestamped, single-valued metrics.\n",
    "\n",
    "We will use the `art_daily_small_noise.csv` file for training and the\n",
    "`art_daily_jumpsup.csv` file for testing. The simplicity of this dataset\n",
    "allows us to demonstrate anomaly detection effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RollingStdDev(RawData, SmoothData, RollSize = 25):\n",
    "    StdDevs = []\n",
    "    for i in range(RollSize):\n",
    "        Diffs = RawData[0:i+1]-SmoothData[0:i+1]\n",
    "        Sqs = Diffs * Diffs\n",
    "        Var = sum(Sqs) / (i+1)\n",
    "        StdDev = np.sqrt(Var)\n",
    "        StdDevs.append(StdDev)\n",
    "    for i in range(len(RawData)-RollSize-1):\n",
    "        j = i + RollSize\n",
    "        Diffs = RawData[i:j]-SmoothData[i:j]\n",
    "        Sqs = Diffs * Diffs\n",
    "        Var = sum(Sqs) / RollSize\n",
    "        StdDev = np.sqrt(Var)\n",
    "        StdDevs.append(StdDev)  \n",
    "    \n",
    "    return StdDevs\n",
    "\n",
    "def RollingSum(Data, Length = 100):\n",
    "    RollSumStdDev = []\n",
    "    for i in range(Length):\n",
    "        RollSumStdDev.append(sum(Data[0:i+1]))\n",
    "    for i in range(len(Data) - Length):\n",
    "        RollSumStdDev.append(sum(Data[i:i+Length]))\n",
    "    return RollSumStdDev\n",
    "\n",
    "def SquelchPattern(DataSet, StallRange = 5000, SquelchLevel = 0.02):\n",
    "    SquelchSignal = np.ones(len(DataSet))\n",
    "\n",
    "    for i in range(len(DataSet)-2*StallRange):\n",
    "        if np.average(DataSet[i:i+StallRange]) < SquelchLevel:\n",
    "            SquelchSignal[i+StallRange]=0\n",
    "\n",
    "    return SquelchSignal\n",
    "\n",
    "def getVelocity(Acceleration, Timestamps = 0.003, Squelch = [], corrected = 0):\n",
    "    velocity = np.zeros(len(Acceleration))\n",
    "    \n",
    "    Acceleration -= np.average(Acceleration)\n",
    "    \n",
    "    if len(Timestamps) == 1:\n",
    "        dTime = np.ones(len(Acceleration),dtype=float) * Timestamps\n",
    "    elif len(Timestamps) == len(Acceleration):\n",
    "        dTime = np.zeros(len(Timestamps), dtype=float)\n",
    "        dTime[0]=1\n",
    "        for i in range(len(Timestamps)-1):\n",
    "            j = i+1\n",
    "            if float(Timestamps[j]) > float(Timestamps[i]):\n",
    "                dTime[j]=float(Timestamps[j])-float(Timestamps[i])\n",
    "            else:\n",
    "                dTime[j]=float(Timestamps[j])-float(Timestamps[i])+10000.0\n",
    "        dTime /= 10000.0\n",
    "\n",
    "    velocity[0] = Acceleration[0] * (dTime[0])\n",
    "\n",
    "    for i in range(len(Acceleration)-1):\n",
    "        j = i + 1\n",
    "        if corrected ==2:\n",
    "            if Squelch[j]==0:\n",
    "                velocity[j]=0\n",
    "            else:\n",
    "                velocity[j] = velocity[i] + Acceleration[j] * dTime[j]                \n",
    "        else:\n",
    "            velocity[j] = velocity[i] + Acceleration[j] * dTime[j]\n",
    "\n",
    "    if corrected == 1:\n",
    "        PointVairance = velocity[-1:] / len(velocity)\n",
    "        for i in range(len(velocity)):\n",
    "            velocity[i] -=  PointVairance * i\n",
    "    \n",
    "    velocity *= 9.81\n",
    "\n",
    "    return velocity\n",
    "\n",
    "def MakeDTs(Seconds, Miliseconds):\n",
    "    dts = np.zeros(len(Miliseconds), dtype=float)\n",
    "    dts[0]=1\n",
    "    for i in range(len(MiliSeconds)-1):\n",
    "        j = i+1\n",
    "        if Seconds[j]==Seconds[i]:\n",
    "            dts[j]=Miliseconds[j]-Miliseconds[i]\n",
    "        else:\n",
    "            dts[j]=Miliseconds[j]-Miliseconds[i]+1000\n",
    "    dts /= 10000\n",
    "    return dts\n",
    "\n",
    "\n",
    "def split_list_by_ones(original_list, ones_list):\n",
    "    # Created with Bing AI support\n",
    "    #  1st request: \"python split list into chunks based on value\"\n",
    "    #  2nd request: \"I want to split the list based on the values in a second list.  Second list is all 1s and 0s.  I want all 0s removed, and each set of consequtive ones as its own item\"\n",
    "    #  3rd request: \"That is close.  Here is an example of the two lists, and what I would want returned: original_list = [1, 2, 3, 8, 7, 4, 5, 6, 4, 7, 8, 9]\n",
    "    #                ones_list =     [1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1]\n",
    "    #                return: [[1, 2, 3, 8], [4, 5, 6], [8,9]]\"\n",
    "    #\n",
    "    #This is the function that was created and seems to work on the short lists, goin to use fo rlong lists\n",
    "    \n",
    "    result_sublists = []\n",
    "    sublist = []\n",
    "\n",
    "    for val, is_one in zip(original_list, ones_list):\n",
    "        if is_one:\n",
    "            sublist.append(val)\n",
    "        elif sublist:\n",
    "            result_sublists.append(sublist)\n",
    "            sublist = []\n",
    "\n",
    "    # Add the last sublist (if any)\n",
    "    if sublist:\n",
    "        result_sublists.append(sublist)\n",
    "\n",
    "    return result_sublists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeDataframe(file, noise=False):\n",
    "    dataset = pd.read_csv(folder+file, delimiter =\", \", header=None, engine='python',on_bad_lines='skip')\n",
    "    if noise:\n",
    "        print(\"File Read\")\n",
    "    dataset = dataset.rename(columns={0:\"Day\"})\n",
    "    dataset = dataset.rename(columns={1:\"Second\"})\n",
    "    dataset = dataset.rename(columns={2:\"FracSec\"})\n",
    "    dataset = dataset.rename(columns={3:\"p\"})\n",
    "    dataset = dataset.rename(columns={4:\"h\"})\n",
    "    dataset = dataset.rename(columns={5:\"v\"})\n",
    "    dataset = dataset.rename(columns={6:\"Sensor\"})\n",
    "\n",
    "    dataset[['Day','Second']] = dataset[['Day','Second']].apply(lambda x: x.astype(int).astype(str).str.zfill(6))\n",
    "    dataset[['FracSec']] = dataset[['FracSec']].apply(lambda x: x.astype(int).astype(str).str.zfill(4))\n",
    "\n",
    "    dataset[\"timestamp\"] = pd.to_datetime(dataset.Day+dataset.Second+dataset.FracSec,format='%y%m%d%H%M%S%f')\n",
    "    dataset[\"timestamps\"] = dataset[\"timestamp\"]\n",
    "    \n",
    "    dataset[\"p\"] = dataset.p - np.average(dataset.p)\n",
    "    dataset[\"h\"] = dataset.h - np.average(dataset.h)\n",
    "    dataset[\"v\"] = dataset.v - np.average(dataset.v)\n",
    "    #dataset[\"r\"] = np.sqrt(dataset.p**2 + dataset.h**2 + dataset.v**2)\n",
    "\n",
    "    dataset.index = dataset.timestamp\n",
    "\n",
    "    #dataset[\"smoothP\"] = denoise_wavelet(dataset.p, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "    #dataset[\"SmoothH\"] = denoise_wavelet(dataset.h, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "    dataset[\"SmoothV\"] = denoise_wavelet(dataset.v, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "\n",
    "    if noise:\n",
    "        print(\"Data Cleaned\")\n",
    "    \n",
    "    StdDevsZ = RollingStdDev(dataset.v, dataset.SmoothV)\n",
    "    StdDevsZ.append(0)\n",
    "    StdDevsZ = np.asarray(StdDevsZ)\n",
    "    SmoothDevZ = denoise_wavelet(StdDevsZ, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "\n",
    "    Max = np.max(SmoothDevZ)\n",
    "    buckets = int(Max / 0.005) + 1\n",
    "    bins = np.linspace(0,buckets*0.005,buckets+1)\n",
    "    counts, bins = np.histogram(SmoothDevZ,bins=bins)\n",
    "\n",
    "    CummCount = 0\n",
    "    HalfWay = 0\n",
    "    for i in range(len(counts)):\n",
    "        CummCount += counts[i]\n",
    "        if CummCount / len(SmoothDevZ) >= 0.5:\n",
    "            if HalfWay == 0:\n",
    "                HalfWay = i\n",
    "\n",
    "    SquelchLevel = bins[HalfWay] \n",
    "    dataset[\"IsMoving\"] = SquelchPattern(SmoothDevZ, 4000, SquelchLevel)\n",
    "    if noise:\n",
    "        print(\"Squelch Made\")\n",
    "    #dataset[\"velocity\"] = getVelocity(dataset.p, dataset.FracSec, dataset.IsMoving, 2)\n",
    "    #if noise:\n",
    "    #    print(\"Velocity Calculated.  File done: \",file)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated training sequences for use in the model.\n",
    "def create_sequences(values, time_steps=TIME_STEPS, skips = Skips):\n",
    "    output = []\n",
    "    for i in range(int((len(values) - time_steps + skips)/skips)):\n",
    "        output.append(values[i*skips : (i*skips + time_steps)])\n",
    "    return np.stack(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePickles(file, index,start=0):\n",
    "    if file[-4:] == '.csv':\n",
    "        df_small_noise = MakeDataframe(file,False)\n",
    "        df_ps = split_list_by_ones(df_small_noise.p, df_small_noise.IsMoving)\n",
    "        df_hs = split_list_by_ones(df_small_noise.h, df_small_noise.IsMoving)\n",
    "        df_vs = split_list_by_ones(df_small_noise.v, df_small_noise.IsMoving)\n",
    "\n",
    "        del df_small_noise\n",
    "\n",
    "        df_p=[0]\n",
    "        df_h=[0]\n",
    "        df_v=[0]\n",
    "        for i in range(len(df_ps)):\n",
    "            df_p += df_ps[i]\n",
    "            df_h += df_hs[i]\n",
    "            df_v += df_vs[i]\n",
    "\n",
    "        del df_ps, df_hs, df_vs\n",
    "\n",
    "        training_mean = np.average(df_p)\n",
    "        training_std = np.std(df_p)\n",
    "        df_training_value_p = (df_p - training_mean) / training_std\n",
    "\n",
    "        training_mean = np.average(df_h)\n",
    "        training_std = np.std(df_h)\n",
    "        df_training_value_h = (df_h - training_mean) / training_std\n",
    "\n",
    "        training_mean = np.average(df_v)\n",
    "        training_std = np.std(df_v)\n",
    "        df_training_value_v = (df_v - training_mean) / training_std\n",
    "\n",
    "        del df_p, df_h, df_v\n",
    "\n",
    "        x_train_p = create_sequences(df_training_value_p)\n",
    "        x_train_h = create_sequences(df_training_value_h)\n",
    "        x_train_v = create_sequences(df_training_value_v)\n",
    "\n",
    "        del df_training_value_p, df_training_value_h, df_training_value_v\n",
    "\n",
    "        x_train=[]\n",
    "        for i in range(len(x_train_p)):\n",
    "            #for i in range(1000000):\n",
    "            x_train.append(np.matrix([x_train_p[i],x_train_h[i],x_train_v[i]]).flatten())\n",
    "\n",
    "        del x_train_p, x_train_h, x_train_v\n",
    "\n",
    "        x_t1 = np.array(x_train)\n",
    "\n",
    "        del x_train\n",
    "\n",
    "        f = open(folder+'DayMovePickle/PickledPrep'+str(index).zfill(5)+'.p','wb')\n",
    "        pickle.dump(x_t1,f)\n",
    "        f.close()\n",
    "        if start != 0:\n",
    "            print('A file done in ', str(int((ti()-start)/600)*10))\n",
    "        if index % 100 == 0:\n",
    "            print('At least one process is at '+str(index))\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3-2021.11/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/local/anaconda3-2021.11/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/local/anaconda3-2021.11/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/local/anaconda3-2021.11/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/local/anaconda3-2021.11/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220919 recording1.csv\n",
      "220919 recording2.csv\n",
      "A file done in  60\n"
     ]
    }
   ],
   "source": [
    "tic = ti()\n",
    "Results = Parallel(n_jobs=5)(delayed(makePickles)(file, index,tic) for index, file in enumerate(files[:5]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "timeseries_anomaly_detection",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
