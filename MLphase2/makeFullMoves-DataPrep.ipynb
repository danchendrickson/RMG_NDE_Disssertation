{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import keras\n",
    "#from keras import layers\n",
    "#from matplotlib import pyplot as plt\n",
    "#import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from cycler import cycler\n",
    "import scipy.special as sp\n",
    "import os as os\n",
    "#import pywt as py\n",
    "#import statistics as st\n",
    "import os as os\n",
    "#import random\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "#import platform\n",
    "from time import time as ti\n",
    "from skimage.restoration import denoise_wavelet\n",
    "#import tensorflow as tf\n",
    "import pickle\n",
    "#import CoreFunctions as cf\n",
    "\n",
    "import sys\n",
    "\n",
    "DataFolder = '/scratch/Recordings2/'\n",
    "SaveFolder = '/scratch/750inputs/'\n",
    "DataFolder = '/sciclone/scr10/dchendrickson01/Recordings2/'\n",
    "SaveFolder = '/sciclone/scr10/dchendrickson01/Ninputs/'\n",
    "\n",
    "StartPoint = 0\n",
    "Groups = 180\n",
    "#StartPoint = int(sys.argv[1])\n",
    "#Groups = 186\n",
    "\n",
    "verbose = True\n",
    "small = False\n",
    "noise = verbose\n",
    "\n",
    "TIME_STEPS = 750\n",
    "Skips = 50\n",
    "\n",
    "tic = ti()\n",
    "start = tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def RollingStdDev(RawData, SmoothData, RollSize = 25):\n",
    "    StdDevs = []\n",
    "    for i in range(RollSize):\n",
    "        Diffs = RawData[0:i+1]-SmoothData[0:i+1]\n",
    "        Sqs = Diffs * Diffs\n",
    "        Var = sum(Sqs) / (i+1)\n",
    "        StdDev = np.sqrt(Var)\n",
    "        StdDevs.append(StdDev)\n",
    "    for i in range(len(RawData)-RollSize-1):\n",
    "        j = i + RollSize\n",
    "        Diffs = RawData[i:j]-SmoothData[i:j]\n",
    "        Sqs = Diffs * Diffs\n",
    "        Var = sum(Sqs) / RollSize\n",
    "        StdDev = np.sqrt(Var)\n",
    "        StdDevs.append(StdDev)  \n",
    "    \n",
    "    return StdDevs\n",
    "\n",
    "def RollingStdDevFaster(RawData, SmoothData, RollSize = 25):\n",
    "\n",
    "    Diffs = RawData - SmoothData\n",
    "    \n",
    "    del RawData, SmoothData\n",
    "    \n",
    "    Sqs = Diffs * Diffs\n",
    "\n",
    "    del Diffs\n",
    "    \n",
    "    Sqs = Sqs.tolist() \n",
    "    \n",
    "    Sqs.extend(np.zeros(RollSize))\n",
    "    \n",
    "    mSqs = np.matrix(Sqs)\n",
    "    \n",
    "    for i in range(RollSize):\n",
    "        Sqs.insert(0, Sqs.pop())\n",
    "        mSqs = np.concatenate((np.matrix(Sqs),mSqs))\n",
    "    \n",
    "    sVect = mSqs.sum(axis=0)\n",
    "    eVect = (mSqs!=0).sum(axis=0)\n",
    "    \n",
    "    del mSqs, Sqs\n",
    "    \n",
    "    VarVect = sVect / eVect\n",
    "    \n",
    "    StdDevs = np.sqrt(VarVect)\n",
    "    \n",
    "    return StdDevs[:-RollSize]\n",
    "\n",
    "\n",
    "def RollingSum(Data, Length = 100):\n",
    "    RollSumStdDev = []\n",
    "    for i in range(Length):\n",
    "        RollSumStdDev.append(sum(Data[0:i+1]))\n",
    "    for i in range(len(Data) - Length):\n",
    "        RollSumStdDev.append(sum(Data[i:i+Length]))\n",
    "    return RollSumStdDev\n",
    "\n",
    "def SquelchPattern(DataSet, StallRange = 5000, SquelchLevel = 0.02, verbose = False):\n",
    "    \n",
    "    SquelchSignal = np.ones(len(DataSet))\n",
    "    if verbose:\n",
    "        print(len(SquelchSignal))\n",
    "        \n",
    "    for i in range(len(DataSet)-2*StallRange):\n",
    "        if np.average(DataSet[i:i+StallRange]) < SquelchLevel:\n",
    "            SquelchSignal[i+StallRange]=0\n",
    "\n",
    "    return SquelchSignal\n",
    "\n",
    "def SquelchPatternFast(DataSet, StallRange = 5000, SquelchLevel = 0.02):\n",
    "    SquelchSignal = np.ones(len(DataSet))\n",
    "\n",
    "    #DataSet = DataSet.tolist() \n",
    "    \n",
    "    DataSet.extend(np.zeros(StallRange))\n",
    "    \n",
    "    DSM = np.matrix(DataSet)\n",
    "    \n",
    "    for i in range(StallRange):\n",
    "        DataSet.insert(0, DataSet.pop())\n",
    "        DSM = np.concatenate((np.matrix(DataSet),DSM))\n",
    "    \n",
    "    DsmAvs = np.average(DSM,axis=0)\n",
    "    \n",
    "    DsmAvs[DsmAvs < SquelchLevel] = 0\n",
    "    DsmAvs[DsmAvs >= SquelchLevel] = 1\n",
    "\n",
    "    return SquelchSignal\n",
    "\n",
    "def getVelocity(Acceleration, Timestamps = 0.003, Squelch = [], corrected = 0):\n",
    "    velocity = np.zeros(len(Acceleration))\n",
    "    \n",
    "    Acceleration -= np.average(Acceleration)\n",
    "    \n",
    "    if len(Timestamps) == 1:\n",
    "        dTime = np.ones(len(Acceleration),dtype=float) * Timestamps\n",
    "    elif len(Timestamps) == len(Acceleration):\n",
    "        dTime = np.zeros(len(Timestamps), dtype=float)\n",
    "        dTime[0]=1\n",
    "        for i in range(len(Timestamps)-1):\n",
    "            j = i+1\n",
    "            if float(Timestamps[j]) > float(Timestamps[i]):\n",
    "                dTime[j]=float(Timestamps[j])-float(Timestamps[i])\n",
    "            else:\n",
    "                dTime[j]=float(Timestamps[j])-float(Timestamps[i])+10000.0\n",
    "        dTime /= 10000.0\n",
    "\n",
    "    velocity[0] = Acceleration[0] * (dTime[0])\n",
    "\n",
    "    for i in range(len(Acceleration)-1):\n",
    "        j = i + 1\n",
    "        if corrected ==2:\n",
    "            if Squelch[j]==0:\n",
    "                velocity[j]=0\n",
    "            else:\n",
    "                velocity[j] = velocity[i] + Acceleration[j] * dTime[j]                \n",
    "        else:\n",
    "            velocity[j] = velocity[i] + Acceleration[j] * dTime[j]\n",
    "\n",
    "    if corrected == 1:\n",
    "        PointVairance = velocity[-1:] / len(velocity)\n",
    "        for i in range(len(velocity)):\n",
    "            velocity[i] -=  PointVairance * i\n",
    "    \n",
    "    velocity *= 9.81\n",
    "\n",
    "    return velocity\n",
    "\n",
    "def MakeDTs(Seconds, Miliseconds):\n",
    "    dts = np.zeros(len(Miliseconds), dtype=float)\n",
    "    dts[0]=1\n",
    "    for i in range(len(MiliSeconds)-1):\n",
    "        j = i+1\n",
    "        if Seconds[j]==Seconds[i]:\n",
    "            dts[j]=Miliseconds[j]-Miliseconds[i]\n",
    "        else:\n",
    "            dts[j]=Miliseconds[j]-Miliseconds[i]+1000\n",
    "    dts /= 10000\n",
    "    return dts\n",
    "\n",
    "\n",
    "def split_list_by_ones(original_list, ones_list):\n",
    "    # Created with Bing AI support\n",
    "    #  1st request: \"python split list into chunks based on value\"\n",
    "    #  2nd request: \"I want to split the list based on the values in a second list.  Second list is all 1s and 0s.  I want all 0s removed, and each set of consequtive ones as its own item\"\n",
    "    #  3rd request: \"That is close.  Here is an example of the two lists, and what I would want returned: original_list = [1, 2, 3, 8, 7, 4, 5, 6, 4, 7, 8, 9]\n",
    "    #                ones_list =     [1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1]\n",
    "    #                return: [[1, 2, 3, 8], [4, 5, 6], [8,9]]\"\n",
    "    #\n",
    "    #This is the function that was created and seems to work on the short lists, goin to use fo rlong lists\n",
    "    \n",
    "    result_sublists = []\n",
    "    sublist = []\n",
    "\n",
    "    for val, is_one in zip(original_list, ones_list):\n",
    "        if is_one:\n",
    "            sublist.append(val)\n",
    "        elif sublist:\n",
    "            result_sublists.append(sublist)\n",
    "            sublist = []\n",
    "\n",
    "    # Add the last sublist (if any)\n",
    "    if sublist:\n",
    "        result_sublists.append(sublist)\n",
    "\n",
    "    return result_sublists\n",
    "\n",
    "# Generated training sequences for use in the model.\n",
    "def create_sequences(values, time_steps=TIME_STEPS, skips = Skips):\n",
    "    output = []\n",
    "    for i in range(int((len(values) - time_steps + skips)/skips)):\n",
    "        output.append(values[i*skips : (i*skips + time_steps)])\n",
    "    return np.stack(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def runFile(file, verbose = False, small = False, index=0, start=ti()):\n",
    "    noise = verbose\n",
    "    if file[-4:] == '.csv':    \n",
    "        dataset = pd.read_csv(DataFolder+file, delimiter =\",\", header=None, engine='python',on_bad_lines='skip')\n",
    "        if noise:\n",
    "            print(\"File Read\", ti()-start)\n",
    "        dataset = dataset.rename(columns={0:\"Day\"})\n",
    "        dataset = dataset.rename(columns={1:\"Second\"})\n",
    "        dataset = dataset.rename(columns={2:\"FracSec\"})\n",
    "        dataset = dataset.rename(columns={3:\"p\"})\n",
    "        dataset = dataset.rename(columns={4:\"h\"})\n",
    "        dataset = dataset.rename(columns={5:\"v\"})\n",
    "        dataset = dataset.rename(columns={6:\"Sensor\"})\n",
    "\n",
    "        dataset['Second'].replace('',0)\n",
    "        dataset['FracSec'].replace('',0)\n",
    "        dataset.replace([np.nan, np.inf, -np.inf],0,inplace=True)\n",
    "        \n",
    "        dataset[['Day','Second']] = dataset[['Day','Second']].apply(lambda x: x.astype(int).astype(str).str.zfill(6))\n",
    "        dataset[['FracSec']] = dataset[['FracSec']].apply(lambda x: x.astype(int).astype(str).str.zfill(4))\n",
    "\n",
    "        dataset[\"timestamp\"] = pd.to_datetime(dataset.Day+dataset.Second+dataset.FracSec,format='%y%m%d%H%M%S%f')\n",
    "        dataset[\"timestamps\"] = dataset[\"timestamp\"]\n",
    "\n",
    "        dataset[\"p\"] = dataset.p - np.average(dataset.p)\n",
    "        dataset[\"h\"] = dataset.h - np.average(dataset.h)\n",
    "        dataset[\"v\"] = dataset.v - np.average(dataset.v)\n",
    "        dataset[\"r\"] = np.sqrt(dataset.p**2 + dataset.h**2 + dataset.v**2)\n",
    "\n",
    "        dataset.index = dataset.timestamp\n",
    "\n",
    "        dataset[\"SmoothP\"] = denoise_wavelet(dataset.p, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        dataset[\"SmoothH\"] = denoise_wavelet(dataset.h, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        dataset[\"SmoothV\"] = denoise_wavelet(dataset.v, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "        dataset[\"SmoothR\"] = denoise_wavelet(dataset.r, method='VisuShrink', mode='soft', wavelet_levels=3, wavelet='sym2', rescale_sigma='True')\n",
    "\n",
    "        if noise:\n",
    "            print(\"Data Cleaned\", ti()-start, len(dataset.p))\n",
    "\n",
    "        RawData = dataset.v\n",
    "        SmoothData = dataset.SmoothV\n",
    "        RollSize = 25\n",
    "\n",
    "        Diffs = RawData - SmoothData\n",
    "\n",
    "        Sqs = Diffs * Diffs\n",
    "\n",
    "        Sqs = Sqs.tolist() \n",
    "\n",
    "        Sqs.extend(np.zeros(RollSize))\n",
    "\n",
    "        mSqs = np.matrix(Sqs)\n",
    "\n",
    "        for i in range(RollSize):\n",
    "            Sqs.insert(0, Sqs.pop())\n",
    "            mSqs = np.concatenate((np.matrix(Sqs),mSqs))\n",
    "\n",
    "        sVect = mSqs.sum(axis=0)\n",
    "        eVect = (mSqs!=0).sum(axis=0)\n",
    "\n",
    "        VarVect = sVect / eVect\n",
    "\n",
    "        StdDevs = np.sqrt(VarVect)\n",
    "\n",
    "        StdDevsZ = np.asarray(StdDevs)\n",
    "\n",
    "        StdDevsZ=np.append(StdDevsZ,[0])\n",
    "\n",
    "        StdDevsZ = np.asarray(StdDevsZ.T[:len(dataset.p)])\n",
    "\n",
    "        if noise:\n",
    "            print(\"Size StdDevsZ\", ti()-start, np.shape(StdDevsZ))\n",
    "\n",
    "        #StdDevsZ = np.nan_to_num(StdDevsZ)\n",
    "\n",
    "        #StdDevsZ[StdDevsZ == np.inf] = 0\n",
    "        #StdDevsZ[StdDevsZ == -np.inf] = 0\n",
    "\n",
    "        if noise:\n",
    "            print(\"cleaned\", ti()-start, np.shape(StdDevsZ))\n",
    "\n",
    "        SmoothDevZ = denoise_wavelet(StdDevsZ, method='VisuShrink', mode='soft', wavelet='sym2', rescale_sigma='True')\n",
    "\n",
    "        if noise:\n",
    "            print(\"denoise 1\", ti()-start, np.shape(StdDevsZ))\n",
    "\n",
    "        #SmoothDevZa = cf.Smoothing(StdDevsZ, 3, wvt='sym2', dets_to_remove=2, levels=3)\n",
    "        #SmoothDevZ = np.ravel(SmoothDevZ[0,:])\n",
    "\n",
    "        #SmoothDevZ = SmoothDevZ.tolist()\n",
    "\n",
    "        if noise:\n",
    "            print(\"denoise 2\", ti()-start, np.shape(SmoothDevZ))\n",
    "\n",
    "        #ataset[\"SmoothDevZ\"] = SmoothDevZ\n",
    "\n",
    "        SmoothDevZ[np.isnan(SmoothDevZ)]=0\n",
    "        \n",
    "        Max = np.max(SmoothDevZ)\n",
    "\n",
    "        \n",
    "        \n",
    "        if noise:\n",
    "            print(\"Max\", ti()-start, np.shape(Max), Max)\n",
    "\n",
    "        buckets = int(Max / 0.005) + 1\n",
    "        bins = np.linspace(0,buckets*0.005,buckets+1)\n",
    "        counts, bins = np.histogram(SmoothDevZ,bins=bins)\n",
    "\n",
    "        CummCount = 0\n",
    "        HalfWay = 0\n",
    "        for i in range(len(counts)):\n",
    "            CummCount += counts[i]\n",
    "            if CummCount / len(SmoothDevZ) >= 0.5:\n",
    "                if HalfWay == 0:\n",
    "                    HalfWay = i\n",
    "\n",
    "        SquelchLevel = bins[HalfWay] \n",
    "        if noise:\n",
    "            print(\"SmoothDevz size\", np.shape(SmoothDevZ))\n",
    "\n",
    "        dataset[\"IsMoving\"] = SquelchPattern(SmoothDevZ, 4000, SquelchLevel, verbose=noise)\n",
    "\n",
    "        if noise:\n",
    "            print(\"Squelch Made\", ti()-start)\n",
    "        #dataset[\"velocity\"] = getVelocity(dataset.p, dataset.FracSec, dataset.IsMoving, 2)\n",
    "        #if noise:\n",
    "        #    print(\"Velocity Calculated.  File done: \",file)\n",
    "\n",
    "        #df_pr = split_list_by_ones(dataset.p, dataset.IsMoving)\n",
    "        #df_hr = split_list_by_ones(dataset.h, dataset.IsMoving)\n",
    "        #df_vr = split_list_by_ones(dataset.v, dataset.IsMoving)\n",
    "        #df_rrr = split_list_by_ones(dataset.r, dataset.IsMoving)\n",
    "        df_ps = split_list_by_ones(dataset.SmoothP, dataset.IsMoving)\n",
    "        df_hs = split_list_by_ones(dataset.SmoothH, dataset.IsMoving)\n",
    "        df_vs = split_list_by_ones(dataset.SmoothV, dataset.IsMoving)\n",
    "        df_rs = split_list_by_ones(dataset.SmoothR, dataset.IsMoving)\n",
    "\n",
    "        #MatsSmooth = []\n",
    "        #for i in range(len(df_ps)):\n",
    "        #    MatsSmooth.append(np.vstack((df_ps[i],df_hs[i],df_vs[i],df_rs[i])))\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Split by ones\", ti()-start)\n",
    "\n",
    "\n",
    "        '''df_p=[0]\n",
    "        df_h=[0]\n",
    "        df_v=[0]\n",
    "        df_r=[0]\n",
    "        df_rp=[0]\n",
    "        df_rh=[0]\n",
    "        df_rv=[0]\n",
    "        df_rr=[0]\n",
    "        for i in range(len(df_ps)):\n",
    "            df_p += df_ps[i]\n",
    "            df_h += df_hs[i]\n",
    "            df_v += df_vs[i]\n",
    "            df_r += df_rs[i]\n",
    "            df_rp += df_pr[i]\n",
    "            df_rh += df_hr[i]\n",
    "            df_rv += df_vr[i]\n",
    "            df_rr += df_rrr[i]\n",
    "        '''\n",
    "        if verbose:\n",
    "            print('format changed', ti()-start, len(df_ps),len(df_hs),len(df_vs),len(df_rs))\n",
    "\n",
    "        #if verbose:\n",
    "        #    print('Data normalized', ti()-start)\n",
    "\n",
    "        return  df_ps, df_hs, df_vs, df_rs\n",
    "    else:\n",
    "        return [0,0,0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "files= os.listdir(DataFolder) \n",
    "#Results = Parallel(n_jobs=8)(delayed(runFile)(file, False, False, index) for index, file in enumerate(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab_type": "code",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Read 223.4546377658844\n",
      "Data Cleaned 391.36684465408325 26010784\n",
      "Size StdDevsZ 452.5201120376587 (26010784,)\n",
      "cleaned 452.5204133987427 (26010784,)\n",
      "denoise 1 453.7830858230591 (26010784,)\n",
      "denoise 2 453.7832179069519 (26010784,)\n",
      "Max 453.82456493377686 () 0.1745304238059106\n",
      "SmoothDevz size (26010784,)\n",
      "26010784\n",
      "Squelch Made 700.4780135154724\n",
      "Split by ones 729.7307484149933\n",
      "format changed 729.7309563159943 2303 2303 2303 2303\n",
      "CPU times: user 11min 17s, sys: 55.1 s, total: 12min 12s\n",
      "Wall time: 12min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "random.shuffle(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(#len(df_p),len(df_h),len(df_v),len(df_r),\n",
    "      len(df_rp),len(df_rh),len(df_rv),len(df_rr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l_rp = []\n",
    "l_rh = []\n",
    "l_rv = []\n",
    "l_rr = []\n",
    "\n",
    "l_rp.append(df_rp)\n",
    "l_rh.append(df_rh)\n",
    "l_rv.append(df_rv)\n",
    "l_rr.append(df_rr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "starts=ti()\n",
    "for i in range(20):\n",
    "    df_rp, df_rh, df_rv, df_rr = runFile(files[i*10+2], verbose = False, small = False, index=0, start=starts)\n",
    "    l_rp.append(df_rp)\n",
    "    l_rh.append(df_rh)\n",
    "    l_rv.append(df_rv)\n",
    "    l_rr.append(df_rr)\n",
    "    print(i, int((ti()-starts)/60*100)/100, len(l_rp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "starts=ti()\n",
    "for i in range(20):\n",
    "    df_rp, df_rh, df_rv, df_rr = runFile(files[i*10+5], verbose = False, small = False, index=0, start=starts)\n",
    "    l_rp.append(df_rp)\n",
    "    l_rh.append(df_rh)\n",
    "    l_rv.append(df_rv)\n",
    "    l_rr.append(df_rr)\n",
    "    print(i, int((ti()-starts)/60*100)/100, len(l_rp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getData(offset, loops = 20, starts=ti()):\n",
    "    l_rp = []\n",
    "    l_rh = []\n",
    "    l_rv = []\n",
    "    l_rr = []\n",
    "    for i in range(loops):\n",
    "        df_rp, df_rh, df_rv, df_rr = runFile(files[i*3+offset], verbose = False, small = False, index=0, start=starts)\n",
    "        if len(df_rp)> 20:\n",
    "            l_rp.append(df_rp)\n",
    "            l_rh.append(df_rh)\n",
    "            l_rv.append(df_rv)\n",
    "            l_rr.append(df_rr)\n",
    "        print(i, int((ti()-starts)/60*100)/100, len(df_rp))\n",
    "    return l_rp,l_rh,l_rv,l_rr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l_rp,l_rh,l_rv,l_rr = Parallel(n_jobs=6)(delayed(getData)(i+1,20,ti()) for i in range(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mats=[]\n",
    "for Data in AllDatas:\n",
    "    for Mat in Data:\n",
    "        for run in Mat:\n",
    "            Mats.append(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del AllDatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths=[]\n",
    "rejects=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in l_rp:\n",
    "    #print(len(item))\n",
    "    for item in grou:\n",
    "        for move in item:\n",
    "            if (len(move) < 12500 and len(move) > 5000):\n",
    "                lengths.append(len(move))\n",
    "            else:\n",
    "                rejects.append(len(move))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(lengths, bins=20)\n",
    "plt.xlabel(\"lengths\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lengths) / (len(lengths)+len(rejects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "groups = [2250-3000,\n",
    "          3001-5000, \n",
    "          5001-7500, \n",
    "          7501-10000,\n",
    "          10001-15000,\n",
    "          15001-20000,\n",
    "          20001-30000,\n",
    "          30001-50000,\n",
    "          50001-75000,\n",
    "          75001-100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Originals = []\n",
    "Smoothed = []\n",
    "\n",
    "small = 5000\n",
    "large = 12500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(l_rp):\n",
    "    #print(len(item))\n",
    "    for j, move in enumerate(item):\n",
    "        moveSize = len(move)\n",
    "        if (moveSize < large and moveSize > small):\n",
    "            extra = large - moveSize\n",
    "            sPad = int(extra/2)\n",
    "            ePad = int(extra/2)+(extra%2)\n",
    "            '''Originals.append(np.vstack((np.pad(l_p[i][j], (sPad, ePad), mode='constant', constant_values=np.average(l_p[i][j])),\n",
    "                                        np.pad(l_h[i][j], (sPad, ePad), mode='constant', constant_values=np.average(l_h[i][j])),\n",
    "                                        np.pad(l_v[i][j], (sPad, ePad), mode='constant', constant_values=np.average(l_v[i][j])),\n",
    "                                        np.pad(l_r[i][j], (sPad, ePad), mode='constant', constant_values=np.average(l_r[i][j])))\n",
    "                                      )\n",
    "                            )\n",
    "            '''\n",
    "            Smoothed.append(np.vstack((np.pad(l_rp[i][j], (sPad, ePad), mode='constant', constant_values=np.average(l_rp[i][j])),\n",
    "                                       np.pad(l_rh[i][j], (sPad, ePad), mode='constant', constant_values=np.average(l_rh[i][j])),\n",
    "                                       np.pad(l_rv[i][j], (sPad, ePad), mode='constant', constant_values=np.average(l_rv[i][j])),\n",
    "                                       np.pad(l_rr[i][j], (sPad, ePad), mode='constant', constant_values=np.average(l_rr[i][j])))\n",
    "                                      )\n",
    "                            )\n",
    "            \n",
    "\n",
    "        else:\n",
    "            rejects.append(len(move))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Smoothed)\n",
    "#padded_array = np.pad(arr, (2, 2), mode='constant', constant_values=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Originals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4%2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 112\n",
    "fig = plt.figure()\n",
    "plt.plot(Smoothed[i][0,:])\n",
    "plt.plot(Smoothed[i][1,:])\n",
    "plt.plot(Smoothed[i][2,:])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 112\n",
    "fig = plt.figure()\n",
    "plt.plot(Smoothed[i][3,:])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential(\n",
    "    [\n",
    "        # Input layer\n",
    "        layers.Input(shape=(3, large)),\n",
    "        \n",
    "        # Increased number of filters and added layers\n",
    "        layers.Conv1D(filters=128, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        layers.Conv1D(filters=64, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        layers.Conv1D(filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        layers.Dropout(rate=0.3),\n",
    "        \n",
    "        # Transpose layers with increased filters\n",
    "        layers.Conv1DTranspose(filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"),\n",
    "        layers.Conv1DTranspose(filters=64, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        layers.Conv1DTranspose(filters=128, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        layers.Dropout(rate=0.3),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Conv1DTranspose(filters=1, kernel_size=7, padding=\"same\"),\n",
    "        \n",
    "        # Added dense layers with more neurons\n",
    "        layers.Dense(256, activation=\"relu\"),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(1)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModelCheckpoint(Callback):\n",
    "    def __init__(self, filepath, save_freq):\n",
    "        super(CustomModelCheckpoint, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.save_freq = save_freq\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.save_freq == 0:\n",
    "            self.model.save(self.filepath.format(epoch=epoch + 1), save_format='keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = CustomModelCheckpoint(\n",
    "    filepath='/sciclone/scr10/dchendrickson01/models/BabySteps_20-30sMoves-{epoch:02d}.keras',\n",
    "    save_freq=1  \n",
    ")\n",
    "\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir='/sciclone/scr10/dchendrickson01/models/profiles/0906-12500/',\n",
    "                                            profile_batch='01, 256')\n",
    "\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor=\"loss\", patience=5, mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = tf.convert_to_tensor(Smoothed, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = tf.convert_to_tensor(Smoothed[:][3], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, outputs = tf.split(ss, [3, 1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x=inputs,\n",
    "    y=outputs,\n",
    "    epochs=10,\n",
    "    batch_size=2,\n",
    "    #validation_split=0.1,\n",
    "    callbacks=[checkpoint_callback, es_callback, tb_callback],        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "123+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(3,large)),\n",
    "        layers.Conv1D(\n",
    "            filters=32,\n",
    "            kernel_size=7,\n",
    "            padding=\"same\",\n",
    "            strides=2,\n",
    "            activation=\"relu\",\n",
    "        ),\n",
    "        layers.Dropout(rate=0.2),\n",
    "        layers.Conv1D(\n",
    "            filters=16,\n",
    "            kernel_size=7,\n",
    "            padding=\"same\",\n",
    "            strides=2,\n",
    "            activation=\"relu\",\n",
    "        ),\n",
    "        layers.Conv1DTranspose(\n",
    "            filters=16,\n",
    "            kernel_size=7,\n",
    "            padding=\"same\",\n",
    "            strides=2,\n",
    "            activation=\"relu\",\n",
    "        ),\n",
    "        layers.Dropout(rate=0.2),\n",
    "        layers.Conv1DTranspose(\n",
    "            filters=32,\n",
    "            kernel_size=7,\n",
    "            padding=\"same\",\n",
    "            strides=2,\n",
    "            activation=\"relu\",\n",
    "        ),\n",
    "        layers.Conv1DTranspose(filters=1, kernel_size=7, padding=\"same\"),\n",
    "    ]\n",
    ")\n",
    "model2.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=\"mse\")\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model2.fit(\n",
    "    x=inputs,\n",
    "    y=outputs,\n",
    "    epochs=10,\n",
    "    batch_size=2,\n",
    "    #validation_split=0.1,\n",
    "    callbacks=[checkpoint_callback, es_callback, tb_callback],        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pred = model.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mae_loss = np.mean(np.abs(x_train_pred - outputs), axis=1)\n",
    "\n",
    "plt.hist(train_mae_loss, bins=50)\n",
    "plt.xlabel(\"Train MAE loss\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()\n",
    "\n",
    "# Get reconstruction loss threshold.\n",
    "threshold = np.max(train_mae_loss)\n",
    "print(\"Reconstruction error threshold: \", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pred2 = model2.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mae_loss2 = np.mean(np.abs(x_train_pred2 - outputs), axis=1)\n",
    "\n",
    "plt.hist(train_mae_loss2, bins=50)\n",
    "plt.xlabel(\"Train MAE loss\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()\n",
    "\n",
    "# Get reconstruction loss threshold.\n",
    "threshold2 = np.max(train_mae_loss2)\n",
    "print(\"Reconstruction error threshold: \", threshold2)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "timeseries_anomaly_detection",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
